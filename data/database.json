[
    {
        "metadata": {
            "title": "Mayhem at OpenAI + Our Interview With Sam Altman - The New York Times",
            "description": "Mayhem at OpenAI + Our Interview With Sam Altman  The New York Times",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMTEvMjAvcG9kY2FzdHMvbWF5aGVtLWF0LW9wZW5haS1vdXItaW50ZXJ2aWV3LXdpdGgtc2FtLWFsdG1hbi5odG1s0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "This transcript was created using speech recognition software. While it has been reviewed by human transcribers, it may contain errors. Please review the episode audio before quoting from this transcript and email transcripts@nytimes.com with any questions.\n\nkevin roose\n\nCasey, how was your weekend?\n\ncasey newton\n\n[CHUCKLES]: Well, there was no weekend. There was only work.\n\nkevin roose\n\nThat was a trick question.\n\ncasey newton\n\nThere will only ever be work. Yeah.\n\nkevin roose\n\n(LAUGHING) Yes.\n\ncasey newton\n\nWhat is happening, Kevin?\n\nkevin roose\n\n(LAUGHING) I don\u2019t know, man. I am on two hours of sleep. I\u2019ve been working all weekend, and I\u2019m increasingly certain that we are, in fact, living in a simulation.\n\ncasey newton\n\nI mean, it would be nice if we were living in a simulation, because that would suggest that there is at least some sort of plan for what might be about to happen next. But I think recent events would suggest that actually there is not.\n\nkevin roose\n\nYeah, I had a moment this morning where I woke up and I looked at my phone from my two-hour nap, and I was like, I\u2019m huffing fumes. This can\u2019t be real.\n\ncasey newton\n\nI mean, let\u2019s just say, like, over the course of a weekend, OpenAI as we know it ceased to exist. And by the time this podcast gets put in the air, I would believe anything you told me about the future of OpenAI, up to and including, it had been purchased by Etsy and it was becoming a maker of handcrafted coffee mugs.\n\nkevin roose\n\n[LAUGHS]: Honestly, it would not be the strangest thing that\u2019s happened this weekend.\n\ncasey newton\n\nNot remotely! It wouldn\u2019t be in the top five! [MUSIC PLAYING]\n\nkevin roose\n\nI\u2019m Kevin Roose, a tech columnist for \u201cThe New York Times.\u201d\n\ncasey newton\n\nI\u2019m Casey Newton from \u201cPlatformer.\u201d\n\nkevin roose\n\nAnd this is \u201cHard Fork.\u201d\n\ncasey newton\n\nThis week on the show, one of the wildest weekends in recent memory \u2014 we\u2019ll tell you everything that happened at OpenAI and what\u2019s going on with Sam Altman. And then, later in the show, we will present to you our interview with Sam Altman from last week. So before he was fired, we asked him about the future of AI, and we\u2019re going to share that conversation with you.\n\nkevin roose\n\nSo this episode is going to have two parts. The first part, we\u2019re going to talk about the news that happened at OpenAI over the weekend and run down all of the latest drama and talk about where we think things are headed from here. And then, we are going to play that Sam Altman interview that we discussed on our last emergency podcast, the one that we conducted last week and planned to run this week, but that has since become fascinating for very different reasons. So let\u2019s just run down what has happened so far, because there\u2019s been so much. It\u2019s, like, enough to fill one of those epic Russian novels or something. So on Friday \u2014\n\ncasey newton\n\nAnd the good news, by the way, is that the events are all very easy to understand. There\u2019s no way you\u2019ll mess up while trying to describe what happened over the past three days.\n\nkevin roose\n\n[LAUGHS]: Yeah, let\u2019s try this on a couple hours of sleep. OK. So Friday, when we recorded our last emergency podcast episode, Sam Altman had just been fired by the board of OpenAI. He was fired for what were essentially vague and unspecified reasons. The board put out a statement, sort of saying that he had not been candid with them, but they didn\u2019t say more about what exactly had led them to decide that he was no longer fit to run the company. So he\u2019s fired. It\u2019s this huge deal, huge shock to all of the people at OpenAI and in the tech industry. And then, it just keeps getting weirder. So Greg Brockman, the president and co-founder of OpenAI, announces that he, too, is quitting. Some other senior researchers resign as well. And then, Saturday rolls around, and we still don\u2019t really know what happened. Brad Lightcap, who is OpenAI\u2019s COO, sent out a memo to employees explaining that they know that Sam was not fired for any kind of malfeasance, right? This wasn\u2019t like a financial crime or anything related to a big data leak or anything. He says, quote, \u201cThis was a breakdown in communication between Sam and the board.\u201d\n\ncasey newton\n\nAnd let\u2019s say that by the time that Brad put that letter out, there had been reporting that at an all-hands, Ilya Sutskever, the chief scientist at the company and a member of the board, had told employees that getting rid of Sam was the only way to ensure that OpenAI could safely build AI, which led to a lot of speculation and commentary that this was an AI safety issue driven by Effective Altruists on the board. So it was very significant when we then get a letter from Brad saying explicitly, this was not an AI safety issue. And of course, that only served to make us even more confused. But lucky for us, further confusion with info.\n\nkevin roose\n\n(LAUGHING) Yeah, this was actually the clearest that things would be for the rest of the next 48 hours. So OpenAI \u2014 its executives are saying this isn\u2019t about safety or anything related to our practices. But what we know from reporting that I and my colleagues did over the weekend is that this actually was at least partially about AI safety and that one of the big fault lines between Sam Altman and the board was over the safety issue, was over whether he was moving too aggressively without taking the proper precautions.\n\ncasey newton\n\nYeah.\n\nkevin roose\n\nAfter this memo from the COO went out, there were reports that investors in OpenAI, including Sequoia Capital, Thrive Capital, and also Microsoft, which is the biggest investor in OpenAI, were exerting pressure on the board to reverse their decision and to reinstate Sam as CEO, and then for the entire board to resign. They had sort of a deadline for figuring some of this stuff out, which is 5 PM on Saturday. That came and went with no resolution. And then Sunday, a bunch of senior OpenAI people, including Sam Altman, who is, by the way, now no longer the CEO of this company, officially gather at the offices of OpenAI in San Francisco to try to work through this all.\n\ncasey newton\n\nThat\u2019s right. There is some reporting that all of a sudden, at least some people on the board are open to the idea of Sam returning, which was one of those moments that was both shocking and not at all surprising. Shocking, because they had just gotten rid of him. Not at all surprising, because I think by that point, it had started to dawn on the world, and on OpenAI in particular, on what it would mean for Altman to no longer be associated with this company where he had recruited most of the star talent.\n\nkevin roose\n\nTotally. And the employees of OpenAI were sort of making their feelings known as well. They did this sort of campaign on Saturday, where they were posting heart emojis in quote posts of Sam, sort of indicating that they stood by him and that they would follow him if he decided to leave and start another company or something like that.\n\ncasey newton\n\nYeah, it was something thing to behold. It was essentially a labor action aimed at the board. And what I will say was in this moment, you realize the degree to which the odds were weirdly stacked against the board. Because on one hand, the board has all of the power when it came to firing Sam. But beyond that, there is still a company to run. There is still technology to build. And so now, you had many employees of the company being very public in saying, hey, we do not have your back. We did not sign up for this, and you\u2019re in trouble.\n\nkevin roose\n\nYeah, and so on Sunday, there was a moment where it sort of looked like Sam Altman was going to return and sort of take his spot back as the CEO of this company. He posted a photo of himself in the OpenAI office wearing a guest badge, like one that you would give to a visitor to your office.\n\ncasey newton\n\nI will say, I have worn that exact badge at OpenAI headquarters before.\n\nkevin roose\n\n[LAUGHS]: Yeah. And so the caption on the photo was something like, this is the first and last time I\u2019ll ever wear one of these. So it kind of sounded like he was setting the scene for a return as well. And I would say there was just a feeling among especially the company\u2019s investors, but also a lot of employees and just people who work in the industry, that this wasn\u2019t going to stand, that there were too many mad employees, that the stakes of blowing this company up over this disagreement were too high. And if there really wasn\u2019t a smoking gun, if there was really nothing concrete that the board was going to hold up and say, this is why we fired Sam Altman, there was this sense that that just wasn\u2019t going to work, that there was no way that the board could actually go through with this firing.\n\ncasey newton\n\nYeah. And I think one way that the employees and the former executives were very effective was in using social media to create this picture of the overwhelming support that was behind them, right? So if you were an observer to this situation, you\u2019re only seeing one side of the story, right? Because the board is not out there posting. They haven\u2019t issued a statement that lays out any details about what Sam allegedly did. And so instead, you just have a bunch of people saying, like, hey, Sam was a great CEO. I love working for the guy. OpenAI is nothing without him. All these posts are getting massively reshared. It\u2019s easy to look at that and think, oh, yeah, he\u2019s probably going to be back in power by the end of the day.\n\nkevin roose\n\nTotally. So that was the scene as of Sunday afternoon. But then, Sunday evening Pacific Time, this new deadline, 5 PM Pacific Time, has been given for some kind of resolution. That also comes and goes, and there is no word from OpenAI\u2019s headquarters about what the heck is going on. It sort of feels like there\u2019s, like, a papal conclave and everyone is waiting for the white smoke to emerge from the chimney. And then, we get word that the board of directors of OpenAI has sent a note to employees announcing that Sam Altman will not return as CEO after all, and sort of standing by its decision. They still didn\u2019t give a firm reason or a specific reason why they pushed him out. But they said that, quote, \u201cPut simply, Sam\u2019s behavior and lack of transparency in his interactions with the board undermined the board\u2019s ability to effectively supervise the company in the manner it was mandated to do.\u201d And they announced that they have appointed a new interim CEO. Now, remember, this company already had an interim CEO, Mira Murati, the former chief technology officer of OpenAI who had been appointed on Friday. She also signaled her support for Sam and Greg, and reporting suggests that she actually tried to have them brought back. And because of that, the board decided to replace her as well. So Mira Murati\u2019s reign as the temporary CEO of OpenAI lasted about 48 hours before she was replaced by Emmett Shear, who is the former CEO of Twitch and who was the board\u2019s choice to take over on an interim basis.\n\ncasey newton\n\nThe board found an alternative man \u2014 or Altman \u2014 to lead the company.\n\nkevin roose\n\n[LAUGHS]: So that was already mind-blowing. This happened at night on Sunday. And I thought, well, clearly, things cannot get any crazier than this.\n\ncasey newton\n\nThat\u2019s when I went to bed, by the way. I was like, whatever\u2019s happening with these people can wait till the morning. And then, of course, I wake up, and an additional four years\u2019 worth of news has happened.\n\nkevin roose\n\nYes. So after this announcement about Sam Altman not returning and Emmett Shear being appointed as the interim CEO, there is a full-on staff revolt at OpenAI. The employees are outraged. They start threatening to quit. And then, just a couple of hours after this note from the board of directors comes out, Microsoft announces that it is hiring Sam Altman and Greg Brockman to lead an advanced research lab at the company.\n\ncasey newton\n\nAn advanced research lab, I assume, means that Satya has just given those two a fiefdom, and they will be given an unlimited budget to do whatever the heck they want. But of course, because Microsoft owns 49 percent of OpenAI, at this advanced research unit, Sam and Greg and all their old friends from OpenAI will have access to all of the APIs, everything that they were doing before. They will just get to pick up where they left off and build everything that they were going to do, but now, firmly under the auspices of a for-profit corporation and, by the way, one of the very biggest giants in the world.\n\nkevin roose\n\nYeah. So I think it\u2019s worth just pausing a beat on this move, because it is truly a wild twist in this saga. So just to explain why this is so crazy, so Microsoft is the biggest investor in OpenAI. They\u2019ve put $13 billion into the company. They\u2019re also sort of highly dependent on OpenAI, because they\u2019ve now built OpenAI\u2019s models into a bunch of their products that they are kind of betting the future of Microsoft on, in some sense. And this was a big bet for them that, over the course of a weekend, was threatening to fall apart, right? Sam Altman and Greg Brockman were the leaders of OpenAI. They were the people that Microsoft was most interested in having run the company. Microsoft did not like this new plan to have Emmett Shear take over as CEO. And \u2014\n\ncasey newton\n\nThey said it\u2019s \u201cshear\u201d madness, Kevin!\n\nkevin roose\n\n[LAUGHS]:: And so they did kind of the next best thing, which was to poach the leaders of OpenAI, the deposed leaders, and bring them into Microsoft, along with, presumably, many of their colleagues who will be leaving OpenAI in protest if the board sticks to this decision.\n\ncasey newton\n\nYeah, man. So this one threw me for a loop. Because if you have spoken with Sam or Greg or many of the people who work at OpenAI, you get the strong impression these people like working at a startup, OK? Working at OpenAI is, in many ways, the opposite of working at a company like Microsoft, which is this massive bureaucracy with so much process for doing anything. I think they really liked working at this nimble thing, at a new thing, being able to chart their own destiny. Keep in mind, OpenAI was about to become the sort of only big, new consumer technology company that we have seen in a long time in Silicon Valley. And so initially, it\u2019s like, OK, they\u2019re going to work at Microsoft? What the heck? Because Kevin, one thing you didn\u2019t mention \u2014 which is fine, because we did have to get through that timeline \u2014 but it\u2019s like, the instant that Sam was fired, reporting started leaking out he was starting a new company with Greg, right? So my assumption had been, these guys are going to go off back into startup land. They\u2019re going to raise an unlimited amount of money and do whatever they want. At the same time, you think about where they were in their mission when all of this happened on Friday. And they had a very clear roadmap, I think, for the next year. And if they would have to go out, raise money, build a new team, train a large language model, think about how much time it would take them just to get back to where they were before, right? They would probably lose a year, if not more, of development. So \u2014 and this is pure speculation, but my guess is that part of their calculus was, look, if we deal with the devil we know and we go to Microsoft, we get to play with all of our old toys, we will have an unlimited budget, and we can skip the fundraising and team-building stage and just get back to work. So I have to believe that was the calculus. But that said, it still was a very unexpected outcome, at least to me.\n\nkevin roose\n\nIt\u2019s a crazy outcome. And it means that Microsoft now has a hand in two, essentially, warring AI companies, right? They have what remains of OpenAI. And they have this long-term deal with OpenAI. And they also control, by the way, the computing power that OpenAI uses to run its models, which gives them some leverage there. So it is a fascinating position that Microsoft is now in and really makes them look even more dominant in AI than they already did.\n\ncasey newton\n\nThat\u2019s right. But listen. All of that said, everything that you just said is true, as we record. However, Kevin, by the end of the day, I would believe any of the following scenarios. Greg and Sam have quit Microsoft. Greg and Sam are starting their own company. Greg and Sam have returned to OpenAI. Greg and Sam have retired from public life. Greg and Sam have opened an Etsy store. This is all within the realm of possibility to me, OK? So if we\u2019re back doing another one of these emergency pods tomorrow, I just want to say that while I accept that everything that Kevin just said is true, I\u2019m only 5 percent confident that any of it lasts to the end of the week.\n\nkevin roose\n\nYes. We are still in the zone where anything can happen. In fact, there have been some things that have happened even since the Microsoft announcement. So super early on Monday morning, like 1 AM Pacific Time, when I was still up \u2014 but I guess you were asleep, because some of us aren\u2019t built for the grind set \u2014 Emmett Shear, the new interim CEO of OpenAI, put out a statement saying that he would basically be digging into what happened over the past weekend, speaking to employees and customers, and then trying to kind of restore stability at the company. And my read of this letter was that he was basically telling OpenAI employees, you know, please don\u2019t quit. I am not the doomer that you think I am, and you can continue to work here. Because one other thing that we should say about Emmett Shear is that while we don\u2019t a ton about his views on AI and AI progress, he has done some interviews where he\u2019s indicated that he is something of an AI pessimist, that he doesn\u2019t think that AI should be going so quickly ahead, that he wants to actually slow it down, which is a position that is at odds with what we know Sam Altman believes.\n\ncasey newton\n\nYeah. As soon as he was named, people found a recent interview he gave where he said that his P doom, his probability that AI will cause doom, was between 5 percent and 50 percent. But if you listen to that interview, it sure sounds like the P doom is closer to 50 than it is to 5, I would say. The other interesting thing in that statement is that Emmett said, before he took the job, he checked on the reasoning behind firing Sam. And he said, quote, \u201cThe board did not remove Sam over any specific disagreement on safety. Their reasoning was completely different from that.\u201d So once again, we have someone talking about the firing without telling us anything, and making it even more confusing.\n\nkevin roose\n\nTotally. But that is not even the end of the timeline. We are still going. Because after this 1 AM memo from Emmett Shear, OpenAI employees start collecting signatures on what amounts to an ultimatum, saying that they will quit if the board does not resign and replace Sam and \u2014 if the board does not resign and bring Sam Altman back as CEO. This letter starts going around OpenAI and eventually collects the signatures of the vast majority of the company\u2019s roughly 700 employees \u2014 almost all of its senior leadership and the rank-and-file \u2014 saying that if the board does not resign and bring back Sam Altman, they will go work for Microsoft or just leave OpenAI.\n\ncasey newton\n\nAnd do you know how much you have to hate your job to go work for Microsoft? These people are pissed, Kevin.\n\nkevin roose\n\n[LAUGHS]:: And then, as if it couldn\u2019t get any crazier, just Monday morning, Ilya Sutskever, the OpenAI co-founder and chief scientist and board member who started all of this, who led the coup against Sam Altman and rallied the board to force him out, posted on X, saying that he, quote, \u201cdeeply regrets his participation with the board.\u201d He said, quote, \u201cI never intended to harm OpenAI. I love everything we\u2019ve built together, and I will do everything I can to reunite the company.\u201d So that is it. That is the entire timeline of the weekend up to the point that we are recording this episode. Casey, are you OK? Do you need to lie down?\n\ncasey newton\n\nI \u2014 well, I do need to lie down. But you know, sometimes, Kevin, when you\u2019re watching a TV show or a movie and the central antagonist has a sudden change of heart that\u2019s completely unexplained, there\u2019s no obvious motivation, I always feel like, wow, the writers really copped out on this one. At least give us some sort of arc. That was the moment Ilya Sutskever had this moment where, as you say, after leading the charge to get rid of Sam for reasons that the board did not specify but that Ilya strongly hinted had something to do with AI safety, he now spins around and says, hey, it\u2019s time to get the band back together. I mean, just a tremendously undermining moment for the board, generally, and for Ilya in particular.\n\nkevin roose\n\nTotally. So right now, as things stand, there are a lot of different factions who have different feelings and emotions about what\u2019s going on. There\u2019s the people at OpenAI, the vast majority of whom are opposed to the board\u2019s actions here and are threatening to walk out if they\u2019re not reversed. There are the investors in OpenAI who are furious about how all of this is playing out. So a lot of people with a lot of high emotions and a lot of uncertainty, yelling at these \u2014 what used to be four and are now three OpenAI board members who have decided to just stand their ground and stick it out.\n\ncasey newton\n\nSo let\u2019s pause there. Because I think that while all of us agree that the board badly mishandled this situation, it is worth taking a beat on what this board\u2019s role is. When I listen back to the episode that we did on Friday, this is a place where I wish I had drilled down a little bit deeper. The mission of this board is to safely develop a superintelligence, absent any commercial motive. That is the goal of this board. This board was put together with the idea that if you have a big company \u2014 like, let\u2019s say, a Microsoft \u2014 that is in charge of a superintelligence, that \u2014 and what is that? Something that is smarter than us, right? Something that will outcompete us in natural selection. They didn\u2019t want that to be owned by a for-profit corporation, right? And something happened, where three of \u2014 at one point, at least four of the people on this board, and now, it\u2019s down to three, but three of those people thought, we are not achieving this mission. Sam did something, or he didn\u2019t do something, or he behaved in some way that made us feel like we cannot safely build a superintelligence, and so we need to find somebody else to run that company. And until we know why they felt that way, there is part of me that just feels like, we just can\u2019t fully process our feelings on this, right? Like, I think it was actually really depressing to see how quickly polarizing this became on social media as it sort of turned into Team Sam versus Team Safety. That\u2019s actually a really bad outcome for society, right? Because I think we do want \u2014 if we\u2019re going to build a superintelligence, I would like to see it built safely. I\u2019m not sure that it is a for-profit corporation that is going to do the best job with that, having watched for-profit corporations create a lot of social harm in my lifetime. Right? So I just want to say that, that I\u2019m sure, before the end of this podcast, we will continue to criticize the board for the way that it handled this. But at the same time, it\u2019s important to remember what their mission was and to assume that they had at least some reasons for doing what they did.\n\nkevin roose\n\nYeah. I mean, I was talking to people all day yesterday who thought that the money would win here, basically, that these investors and Microsoft \u2014 they were powerful enough, and they had enough stake in the outcome here that they would, by any means necessary, get Sam Altman and Greg Brockman back to OpenAI. And I was very surprised when that didn\u2019t happen. But maybe I shouldn\u2019t have been. Because as someone pointed out to me when I talked to them yesterday who was sort of involved with the situation said, the board has the ultimate leverage here, this structure, this convoluted governance structure, where there\u2019s a non-profit that controls a for-profit, and the non-profit can vote to fire the CEO at any time. Like, it was set up for this purpose. I mean, you can argue with how they executed it. And I would say they executed it very badly. But it was meant to give the board the power to shut this all down if they determined that what was happening at OpenAI was unsafe or was not going to lead to broadly beneficial AGI. And it sounds like that\u2019s what happened.\n\ncasey newton\n\nThat\u2019s right. Another piece that I would point to \u2014 my friend, Eric Newcomer, wrote a good column about this, just pointing out that Sam has had abrupt breaks with folks in the past, right? He had an abrupt break with Y Combinator, where he used to lead the startup incubator. He had an abrupt break with Elon Musk, who co-founded OpenAI with him. He had an abrupt break with the folks who left OpenAI to go start Anthropic for what they described as AI safety reasons, right? So there is a history there that suggests that \u2014 right now, a lot of people think that the board is crazy. But these are not the first people to say Sam Altman is not building AI safely.\n\nkevin roose\n\nRight. Here\u2019s the thing. Like, I still think there has to have been some inciting incident, right? This does not feel to me like it was kind of a slow accumulation of worry by Ilya Sutskever and the more safety-minded board members that just woke up one day and said, you know what? Like, it\u2019s just gotten a little too aggressive over there, so let\u2019s shut this thing down. I still think there had to have been some incident, something that Ilya Sutskever and the board saw, that made them think that they had to act now. So, so much is changing. We have to keep going back to this caveat of, like, we still don\u2019t know what is going to happen in the next hour, to say nothing of the next day or week or month. But that is the state of play right now. And I think this is \u2014 I mean, Casey, I don\u2019t know how you feel about this, but I would say this is the most fascinating and crazy story that I have ever covered in my career as a tech journalist. I cannot remember anything that made my head spin as much as this.\n\ncasey newton\n\nYeah, certainly, in terms of the number of unexplained and unexpected twists, it\u2019s hard for me to think of another story that comes close. But I think we should look forward a little bit and talk about what this might mean for OpenAI in particular. OpenAI was described to me over the weekend by a former employee as a money incinerator. ChatGPT does not make money. Training these models is incredibly expensive. The whole reason OpenAI became a for-profit company was because it cost so much money to build and maintain and run these models. When Sam was fired, it has been reported that he was out there raising money to put back into the incinerator. So think about the position that leaves the OpenAI board in. Let\u2019s say that they\u2019re able to staunch the bleeding and retain a couple hundred people who are closely associated with the mission, and the board thinks that these are the right people. Who is going to give them the money to continue their work after what has just happened? Right? Now, look, Emmett Shear is very well regarded in Silicon Valley. He was texting with sources last night, who were sort of very excited that he was the guy that they chose. And so no disrespect to him. But this board has shown that it is serious when it says it does not have a profit incentive. So unless it\u2019s going to go out there and start raising money from foundations and philanthropists and kindly billionaires, I do not see how they get the money to keep maintaining the status quo. And so in a very real sense, over the weekend, OpenAI truly may have died.\n\nkevin roose\n\nIt truly may have. I mean, you\u2019re right. Like, we are going to take a bunch of money and incinerate it. And by the way, \u201cwe\u2019re also going to move very slowly and not accelerate progress\u201d is not a compelling pitch to investors. And so I don\u2019t think that the sort of new OpenAI is going to have a good time when it goes out to raise its next round of funding or \u2014 by the way \u2014 and this is another factor that we haven\u2019t talked about \u2014 to close this tender offer, this round of secondary investment that was going to give OpenAI employees a chance to cash out some of their shares \u2014 that, I would say, is doomed.\n\ncasey newton\n\nYeah. And that \u2014 I\u2019m sure that motivated a lot of the signatures on the letter demanding that Sam and Greg come back, right? Because those people were about to get paid, and not anymore.\n\nkevin roose\n\nTotally. So that\u2019s some of what lies ahead for Microsoft and OpenAI, although anything could change. And that brings us to the interview that we had with Sam Altman last week. So last week, before any of this happened on Wednesday, which is two days before he was fired \u2014\n\ncasey newton\n\nIt was a simpler, more innocent time.\n\nkevin roose\n\n(LAUGHING) It\u2019s true. I actually do feel like that was about a year and a half ago. So we sat down with Sam Altman, and we asked him all kinds of questions, both about the year since ChatGPT was launched and what had happened since then, and also about the future and his thoughts about where I was headed and where OpenAI was headed. So then, all this news broke, and we thought, well, what do we do with this interview now? And we thought about, should we even run it? Should we chop it up and just play the most relevant bits? But we ultimately decided, like, we should just put the whole thing out.\n\ncasey newton\n\nPut it out there.\n\nkevin roose\n\nYeah. So I would just say to listeners, like, as you listen to this interview, you may be thinking, like, why are these guys asking about ChatGPT? Who cares about ChatGPT? We\u2019ve got bigger fish to fry here, people. But just keep in mind that when we recorded this, none of this drama had happened yet. The biggest news in Sam Altman\u2019s world was that the one-year anniversary of ChatGPT was coming up, and we wanted to ask him to reflect on that. So just keep in mind these are questions from Earth One, and we are now on Earth Two, and just bear that in mind as you listen. But I would say that the issues that we talked about with Sam \u2014 some of the things around the acceleration of progress at OpenAI and his view of the future and his optimism about what building powerful AI could do \u2014 those are some of the key issues that seem to have motivated this coup by the board. So I think it\u2019s still very relevant, even though the specific facts on the ground have changed so much since we recorded with him.\n\ncasey newton\n\nSo in this interview, you\u2019ll hear us talk about existential risk, AI safety. If that\u2019s a subject you haven\u2019t been paying much attention to, the fear here is that as these systems grow more powerful, and they are already growing exponentially more powerful year by year, at some point, they may become smarter than us. Their goals may disalign from ours. And so for folks who follow this stuff closely, there\u2019s a big debate on how seriously we should take that risk.\n\nkevin roose\n\nRight, and there\u2019s also a big debate in the tech world more broadly about whether AI and technology in general should be progressing faster or whether things are already going too fast and they should be slowed down. And so when we ask them about being an accelerationist, that\u2019s what we\u2019re talking about.\n\ncasey newton\n\nAnd I should say, I texted Sam this morning to see if there was anything that he wanted to say or add, and as we record, have not heard back from him yet.\n\nkevin roose\n\nWhen we come back, our interview from last week with Sam Altman. [MUSIC PLAYING] Sam Altman, welcome back to \u201cHard Fork.\u201d\n\nsam altman\n\nThank you.\n\ncasey newton\n\nSam, it has been just about a year since ChatGPT was released. And I wonder if you have been doing some reflecting over the past year and, kind of, where it has brought us in the development of AI.\n\nsam altman\n\nFrankly, it has been such a busy year, there has not been a ton of time for reflection.\n\nkevin roose\n\nWell, that\u2019s why we brought you in. We want you to reflect here.\n\nsam altman\n\nGreat, I can do it now. I mean, I definitely think this was the year, so far \u2014 there will be maybe more in the future \u2014 but the year, so far, where the general average tech person went from taking AI not that seriously to taking it pretty seriously.\n\ncasey newton\n\nYeah.\n\nsam altman\n\nAnd the recompiling of expectations, given that. So I think in some sense, that\u2019s like the most significant update of the year.\n\ncasey newton\n\nI would imagine that for you, a lot of the past year has been watching the world catch up to things that you have been thinking about for some time. Does it feel that way?\n\nsam altman\n\nYeah, it does. You know, we kind of always thought, on the inside of OpenAI, that it was strange that the rest of the world didn\u2019t take this more seriously. Like, it wasn\u2019t more excited about it.\n\ncasey newton\n\nI mean, I think if five years ago, you had explain what ChatGPT was going to be, I would have thought, wow, that \u2014 like, that sounds pretty cool. But \u2014 and presumably, I could have just looked into it more, and I would have smartened myself up. But I think until I actually used it, as is often the case, it was just hard to know what it was.\n\nsam altman\n\nYeah, I actually think we could have explained it, and it wouldn\u2019t have made that much of a difference. We tried. Like, people are busy with their lives. They don\u2019t have a lot of time to sit there and listen to some tech people prognosticate about something that may or may not happen. But you ship a product that people use, get real value out of, and then it\u2019s \u2014 and then, it\u2019s different.\n\ncasey newton\n\nYeah. I remember reading about the early days of the run-up to the launch of ChatGPT, and I think you all have said that you did not expect it to be a hit when it launched.\n\nsam altman\n\nNo, we thought it would be a hit. We didn\u2019t think it\u2019d be like this. We did it because we thought it was going to be a hit. We didn\u2019t think it was going to be, like, this big of a hit.\n\ncasey newton\n\nRight. As we\u2019re sitting here today, I believe it\u2019s the case that you can\u2019t actually sign up for ChatGPT Plus right now. Is that right?\n\nsam altman\n\nCorrect.\n\ncasey newton\n\nYeah. So what\u2019s that all about?\n\nsam altman\n\nWe have not enough capacity always. But at some point, it gets really bad. So over the last 10 days or so, we have done \u2014 we\u2019ve, like, done everything we can. We\u2019ve rolled out new optimizations. We\u2019ve disabled some features. And then, people just keep signing up. It keeps getting slower and slower. And there\u2019s, like, a limit at some point to what you can do there, and you can\u2019t \u2014 we just don\u2019t want to offer a bad quality of service. And so it gets slow enough that we just say, you know what? Until we can make more progress, either with more GPUs or more optimizations, we\u2019re going to put this on hold. Not a great place to be in, to be honest, but it was like the least of several bad options.\n\ncasey newton\n\nSure. And I feel like in the history of tech development, there often is a moment with really popular products where you just have to close signups for a little while, right?\n\nsam altman\n\nThe thing that\u2019s different about this than others is it\u2019s just \u2014 it\u2019s so much more compute-intensive than the world is used to for internet services. So you don\u2019t usually have to do this. Like, usually, by the time you\u2019re at this scale, you\u2019ve, like, solved your scaling bottlenecks.\n\nkevin roose\n\nYeah. One of the interesting things, for me, about covering all the AI changes over the past year is that it often feels like journalists and researchers and companies are discovering properties of these systems sort of at the same time, all together. I mean, I remember when we had you and Kevin Scott from Microsoft on the show earlier this year around the Bing relaunch. And you both said something to the effect of, well, to discover what these models are or what they\u2019re capable of, you kind of have to put them out into the world and have millions of people using them. Then, we saw all kinds of crazy but also inspiring things. You had Bing Sydney, but you also had people starting to use these things in their lives. So I guess I\u2019m curious what you feel like you have learned about language models and your language models specifically from putting them out into the world.\n\ncasey newton\n\nSo what we don\u2019t want to be surprised by is the capabilities of the model. That would be bad. And we\u2019re not \u2014 with GPT 4, for example, we took a long time between finishing the model and releasing it. Red team did heavily \u2014 really studied it, did all of the work, internally, externally. And there\u2019s \u2014 I\u2019d say there\u2019s, at least so far \u2014 and maybe now, it\u2019s been long enough that we would have \u2014 we have not been surprised by any capabilities the model had that we just didn\u2019t know about at all in a way that we were for GPT 3, frankly, sometimes. People found stuff. But what I think you can\u2019t do in the lab is understand how technology and society are going to co-evolve. So you can say, here\u2019s what the model can do and not do. But you can\u2019t say, like, and here\u2019s exactly how society is going to progress, given that. And that\u2019s where you just have to see what people are doing, how they\u2019re using it. And that \u2014 well, one thing is, they use it a lot. Like, that\u2019s one takeaway that we did not \u2014 clearly, we did not appropriately plan for. But more interesting than that is the way in which this is transforming people\u2019s productivity, personal lives, how they\u2019re learning, and how \u2014 like, one example that I think is instructive, because it was the first and the loudest, is what happened with ChatGPT and education. Days \u2014 at least weeks, but I think days after the release of ChatGPT, school districts were falling all over themselves to ban ChatGPT. And that didn\u2019t really surprise us. Like, that, we could have predicted and did predict. The thing that happened after that, quickly, was \u2014 like, weeks to months \u2014 was school districts and teachers saying, hey, actually, we made a mistake. And this is really important part of the future of education, and the benefits far outweigh the downside. And not only are we unbanning it, we\u2019re encouraging our teachers to make use of it in the classroom. We\u2019re encouraging our students to get really good at this tool, because it\u2019s going to be part of the way people live. And then, there was a big discussion about what the kind of path forward should be. And that is just not something that could have happened without releasing.\n\nkevin roose\n\nYeah.\n\nsam altman\n\nAnd part \u2014 can I say one more thing?\n\nkevin roose\n\nYeah.\n\nsam altman\n\nPart of the decision that we made with the ChatGPT release \u2014 the original plan had been to do the chat interface and GPT 4 together in March. And we really believe in this idea of iterative deployment. And we had realized that the chat interface plus GPT 4 was a lot. I don\u2019t think we realized quite how much \u2014\n\nkevin roose\n\nLike, too much for society to take in.\n\nsam altman\n\nSo we split it, and we put out \u2014 we put it out with GPT 3.5 first, which we thought was a much weaker model. It turned out to still be powerful enough for a lot of use cases. But I think that, in retrospect, was a really good decision and helped with that process of gradual adaptation for society.\n\nkevin roose\n\nLooking back, do you wish that you had done more to, I don\u2019t know, give people a manual to say, here\u2019s how you can use this at school or at work?\n\nsam altman\n\nTwo things. One, I wish we had done something intermediate between the release of 3.5 and the API and ChatGPT. Now, I don\u2019t know how well that would have worked, because I think there was just going to be some moment where it went, like, viral in the mind of society. And I don\u2019t know how incremental that could have been. That\u2019s sort of a \u201ceither it goes like this or it doesn\u2019t\u201d kind of thing. And I think \u2014 I have reflected on this question a lot. I think the world was going to have to have that moment. It was better sooner than later. It was good we did it when we did. Maybe we should have tried to push it even a little earlier. But it\u2019s a little chancey about when it hits, and I think only a consumer product could have done what happened there. Now, the second thing is, should we have released more of a how-to manual? And I honestly don\u2019t. No, I think we could have done some things there that would have been helpful, but I really believe that it\u2019s not optimal for tech companies to tell people, like, here is how to use this technology and here\u2019s how to do whatever. And the organic thing that happened there actually was pretty good.\n\ncasey newton\n\nYeah. I\u2019m curious about the thing that you just said about, we thought it was important to get this stuff into folks\u2019 hands sooner rather than later. Say more about why that is.\n\nsam altman\n\nMore time to adapt for our institutions and leaders to understand, for people to think about what the next version of the model should do, what they\u2019d like, what would be useful, what would not be useful, what would be really bad, how society and the economy need to co-evolve. Like, the thing that many people in the field or adjacent to the field have advocated or used to advocate for, which I always thought was super bad, was, this is so disruptive, such a big deal. It\u2019s got to be done in secret by the small group of us that can understand it. And then, we will fully build the AGI and push a button all at once when it\u2019s ready. And I think that\u2019d be quite, quite bad.\n\ncasey newton\n\nYeah, because it would just be way too much change too fast.\n\nsam altman\n\nYeah. Again, society and technology have to co-evolve, and people have to decide what\u2019s going to work for them and how they want to use it. And you can criticize OpenAI about many, many things, but we do try to really listen to people and adapt it in ways that make it better or more useful. And I think we\u2019re able to do that. But we wouldn\u2019t get it right without that feedback.\n\ncasey newton\n\nYeah.\n\nkevin roose\n\nI want to talk about AGI and the path to AGI later on. But first, I want to just define AGI and have you talk about where we are on the continuum. So \u2014\n\nsam altman\n\nI think it\u2019s a ridiculous and meaningless term.\n\nkevin roose\n\nYeah?\n\nsam altman\n\nSo I apologize that I keep using it. It\u2019s, like, deep in the muscle memory.\n\nkevin roose\n\nI mean, I just never know what people are talking about when they\u2019re talking \u2014\n\nsam altman\n\nNo one else does either. They mean, like, really smart AI.\n\nkevin roose\n\nYeah, so it stands for Artificial General Intelligence, and you could probably ask 100 different AI researchers, and they would give you 100 different definitions of what AGI is. Researchers at Google DeepMind just released a paper this month that sort of offers a framework. They have five levels. Level \u2014 I guess they have levels ranging from level 0, which is no AI, all the way up to level 5, which is superhuman. And they suggest that currently, ChatGPT, Bard, LLaMA 2, are all at level 1, which is equal to or slightly better than an unskilled human. Would you agree with that? Where are we \u2014 if you say this is a term that means something and you define it that way, how close are we?\n\nsam altman\n\nUm, I think the thing that matters is the curve and the rate of progress. And there\u2019s not going to be some milestone that we all agree, like, OK, we\u2019ve passed it, and now, it\u2019s called AGI. Like, what I would say is, we currently have systems that are \u2014 like, there will be researchers who will write papers like that. You know, academics will debate it, and people in industry will debate it. And I think most of the world just cares, like, is this thing useful to me or not? And we currently have systems that are somewhat useful, clearly. Like, and you know, whether we want to say, like, it\u2019s a level 1 or 2, I don\u2019t know, but people use it a lot, and they really love it. There\u2019s huge weaknesses in the current systems. But it doesn\u2019t mean that \u2014 I\u2019m a little embarrassed by GPTs, but people still like \u2018em. And that\u2019s good. Like, it\u2019s nice to do useful stuff for people. So yeah, call it a level 1. Doesn\u2019t bother me at all. I am embarrassed by it. We will make them much better. But at their current state, they are still, like, delighting people and being useful to people.\n\ncasey newton\n\nYeah, I also think it underrates them slightly to say that they\u2019re just better than unskilled humans. Like, when I use ChatGPT, it is better than skilled humans for some \u2014\n\nsam altman\n\nAt some things, and worse than unskilled \u2014 worse than any human in many other things.\n\nkevin roose\n\nBut I guess this is one of the questions that people ask me the most \u2014 and, I imagine, ask you \u2014 is like, what are today\u2019s AI systems useful and not useful for doing?\n\nsam altman\n\nI would say the main thing that they\u2019re bad at \u2014 well, many things, but one that is on my mind a lot is they\u2019re bad at reasoning. And a lot of the valuable human things require some degree of complex reasoning. But they\u2019re good at a lot of other things. Like, GPT 4 is vastly superhuman in terms of its world knowledge. Like, it knows \u2014 there\u2019s a lot of things in there. And it\u2019s just, it\u2019s very different than how we think about evaluating human intelligence. So it can\u2019t do these basic reasoning tasks. On the other hand, it knows more than any human has ever known. On the other hand, again, sometimes it totally makes stuff up in a way that a human would not. But if you are using it to be a coder, for example, it can hugely increase your productivity. And there\u2019s value there, even though it has all of these other weak points. If you are a student, you can learn a lot more than you could without using this tool in some ways. Value there, too.\n\ncasey newton\n\nLet\u2019s talk about GPTs, which you announced at your recent developer conference. For those who haven\u2019t had a chance to use one yet, Sam, what\u2019s a GPT?\n\nsam altman\n\nIt\u2019s like a custom version of ChatGPT that you can get to behave in a certain way. You can give it limited ability to do actions. You can give it knowledge to refer to. You can say, like, act this way. It\u2019s super easy to make, and it\u2019s a first step towards more powerful AI systems and agents.\n\ncasey newton\n\nWe\u2019ve had some fun with them on the show. There\u2019s a \u201cHard Fork\u201d bot that you can ask about anything that\u2019s happened on any episode of the show. It works pretty well, we found, when we did some testing. But I want to talk about where this is going. What is the GPTs that you\u2019ve released a first step toward?\n\nsam altman\n\nAIs that can accomplish useful tasks. Like, the \u2014 I think we need to move towards this with great care. We don\u2019t \u2014 I think it would be a bad idea to put, like \u2014 turn powerful agents for you on the internet. But AIs that can act on your behalf to do something with a company that can access your data, that can help you be good at a task \u2014 I think that\u2019s going to be an exciting way we use computers. Like, we have this belief that we\u2019re heading towards a vision where there are new interfaces, new user experiences possible, because finally, the computer can understand you and think. And so the sci-fi vision of a computer that you just, like, tell what you want and it figures out how to do it \u2014 this is a step towards that.\n\ncasey newton\n\nRight now, I think what\u2019s holding a lot of people back in \u2014 a lot of companies and organizations back in using this kind of AI in their work is that it can be unreliable, it can make up things, it can give wrong answers, which is fine if you\u2019re doing creative writing assignments, but not if you\u2019re a hospital or a law firm or something else with big stakes. How do we solve this problem of reliability? And do you think we\u2019ll ever get to the low fault tolerance that is needed for these really high-stakes applications?\n\nsam altman\n\nSo first of all, I think this is, like, a great example of people understanding the technology, making smart decisions with it. Society and the technology co-evolving together. Like, what you see is that people are using it where appropriate and where it\u2019s helpful, and not using it where you shouldn\u2019t. And for all of the fear that people have had, both users and companies seem to really understand the limitations and are making appropriate decisions about where to roll it out. It \u2014 the kind of controllability, reliability, whatever you want to call it \u2014 that is going to get much better. I think we\u2019ll see a big step forward there over the coming years. And \u2014 and I think that there will be a time. I don\u2019t know if it\u2019s, like, 2026, 2028, 2030, whatever, but there will be a time where we just don\u2019t talk about this anymore.\n\ncasey newton\n\nYeah? It seems to me, though, that that is something that becomes very important to get right in the \u2014 as you build these more powerful GPTs, right? Once I tell \u2014 like, I would love to have a GPT be my assistant, go through my emails, hey, don\u2019t forget to respond to this before the end of the day \u2014\n\nsam altman\n\nThe reliability has got to be way up before that happens.\n\ncasey newton\n\nYeah, yeah. That makes sense. You mentioned, as we started to talk about GPTs, that you have to do this carefully. For folks who haven\u2019t spent as much time reading about this, explain what are some things that could go wrong. You guys are obviously going to be very careful with this. Other people who are going to build GPT-like things might not put the same kind of controls in place. So what can you imagine other people doing that you, as the CEO, would say to your folks, hey, it\u2019s not going to be able to do that?\n\nsam altman\n\nWell, that example that you just gave, like, if you let it act as your assistant and go, like, send emails, do financial transfers for you, like, it\u2019s very easy to imagine how that could go wrong. But I think most people who would use this don\u2019t want that to happen on their behalf either. And so there\u2019s more resilience to this sort of stuff than people think.\n\ncasey newton\n\nI think that\u2019s right. I mean, for what it\u2019s worth, on the whole \u2014 on the hallucination thing, which it does feel like has maybe been the longest conversation that we\u2019ve had about ChatGPT in general since it launched \u2014 I just always think about Wikipedia as a resource I use all the time. And I don\u2019t want Wikipedia to be wrong, but 100 percent of the time, it doesn\u2019t matter if it does. I am not relying on it for life-saving information, right? ChatGPT, for me, is the same, right? It\u2019s like, hey, I mean, it\u2019s great for just kind of bar trivia, like, hey, you know, what\u2019s the history of this conflict in the world?\n\nsam altman\n\nYeah, I mean, we want to get that a lot better, and we will. Like, I think the next model will just hallucinate much less.\n\nkevin roose\n\nIs there an optimal level of hallucination in an AI model? Because I\u2019ve heard researchers say, well, you actually don\u2019t want it to never hallucinate, because that would mean making it not creative \u2014 that new ideas come from making stuff up. That it\u2019s not necessarily tethered to \u2014\n\nsam altman\n\nThis is why I tend to use the word, \u201ccontrollability,\u201d and not \u201creliability.\u201d You want it to be reliable when you want. You want it to \u2014 either you instruct it, or it just knows based off of the context, that you are asking a factual query, and you want the 100 percent black-and-white answer. But you also want it to know when you want it to hallucinate or you want it to make stuff up. As you just said, like, new discovery happens because you come up with new ideas, most of which are wrong, and you discard those and keep the good ones and sort of add those to your understanding of reality. Or if you\u2019re telling a creative story, you want that. So if these models \u2014 like, if these models didn\u2019t hallucinate at all, ever, they wouldn\u2019t be so exciting. They wouldn\u2019t do a lot of the things that they can do. But you only want them to do that when you want them to do that. And so like, the way I think about it is, like, model capability personalization and controllability. And those are, like, the three axes we have to push on. And controllability means no hallucinations when you don\u2019t want it, lots of it when you\u2019re trying to invent something new.\n\ncasey newton\n\nLet\u2019s maybe start moving into some of the debates that we\u2019ve been having about AI over the past year. And actually, I want to start with something that I haven\u2019t heard as much, but that I do bump into when I use your products, which is like, they can be quite restrictive in how you use them. I think, mostly for great reasons, right? Like, I think you guys have learned a lot of lessons from the past era of tech development. At the same time, I feel like \u2014 like, I\u2019ve tried to ask ChatGPT a question about sexual health. I feel like it\u2019s going to call the police on me, right? So I\u2019m just curious how you\u2019ve approached that subject.\n\nsam altman\n\nYeah. Look. One thing \u2014 no one wants to be scolded by a computer, ever. Like, that is not a good feeling. And so you should never feel like you\u2019re going to have the police called on you. That\u2019s more, like, horrible, horrible, horrible. We have started very conservative, which I think is a defensible choice. Other people may have made a different one. But again, that principle of controllability \u2014 what we\u2019d like to get to is a world where, if you want some of the guardrails relaxed a lot, and that\u2019s \u2014 like, you\u2019re not like a child or something \u2014 then fine, we\u2019ll relax the guardrails. It should be up to you. But I think starting super conservative here, although annoying, is a defensible decision. And I wouldn\u2019t have gone back and made it differently. We have relaxed it already. We will relax it much more, but we want to do it in a way where it\u2019s user-controlled.\n\nkevin roose\n\nYeah. Are there certain red lines you won\u2019t cross, things that you will never let your models be used for, other than things that are obviously illegal or dangerous?\n\nsam altman\n\nYeah, certainly things that are illegal and dangerous, we won\u2019t. There\u2019s a lot of other things that I could say, but they so depend \u2014 where those red lines will be so depend on how the technology evolves, that it\u2019s hard to say right now, like, here\u2019s the exhaustive set. We really try to just study the models and predict capabilities as we go, but we get \u2014 if we learn something new, we change our plans.\n\nkevin roose\n\nYeah. One other area where things have been shifting a lot over the past year is in AI regulation and governance. I think a year ago, if you had asked the average congressperson, what do you think of AI, they would have said, what\u2019s that?\n\ncasey newton\n\n\u201cGet out of my office!\u201d\n\nkevin roose\n\n(LAUGHING) Right. We just recently saw the Biden White House put out an executive order about AI. You\u2019ve obviously been meeting a lot with lawmakers and regulators, not just in the US, but around the world. What\u2019s your view of how AI regulation is shaping up?\n\nsam altman\n\nIt\u2019s a really tricky point to get across. What we believe is that on the frontier systems, there does need to be proactive regulation there. But heading into overreach and regulatory capture would be really bad. And there\u2019s a lot of amazing work that\u2019s going to happen with smaller models, smaller companies, open-source efforts. And it\u2019s really important that regulation not strangle that. So it\u2019s like I\u2019ve sort of become a villain for this, but I think \u2014\n\ncasey newton\n\nYou have?\n\nsam altman\n\nYeah.\n\ncasey newton\n\nHow do you feel about this?\n\nsam altman\n\nLike, annoyed, but I have bigger problems in my life right now. But this message of, like, regulate us, regulate the really capable models that can have significant consequences, but leave the rest of the industry alone, is just \u2014 it\u2019s a hard message to get across. Sure.\n\ncasey newton\n\nHere is an argument that was made to me by a high-ranking executive at a major tech company as some of this debate was playing out. This person said to me that there is, essentially, no harms that these models can have that the internet itself doesn\u2019t enable, right? And that to do any sort of work like is proposed in this executive order, to have to inform the Biden administration, is just essentially pulling up the ladder behind you and ensuring that the folks who\u2019ve already raised the money can reap all of the profits of this new world and will leave the little people behind. So I\u2019m curious what you make of that argument.\n\nsam altman\n\nI disagree with it on a bunch of levels. First of all, I wish the threshold for when you do have to report was set differently and based off of evals and capability thresholds.\n\ncasey newton\n\nNot FLOPS?\n\nsam altman\n\nNot FLOPS.\n\ncasey newton\n\nOK.\n\nsam altman\n\nBut there\u2019s no small company training with that many FLOPS anyway, so that\u2019s a little bit \u2014\n\ncasey newton\n\nYeah.\n\nkevin roose\n\nFor the listener who maybe didn\u2019t listen to our last episode about \u2014\n\ncasey newton\n\nListen to your FLOPS episode!\n\nkevin roose\n\n\u2014 the FLOPS are the sort of measure of the amount of computing that is used to train these models. The executive order says if you\u2019re above a certain computing threshold, you have to tell the government that you\u2019re training a model that big.\n\nsam altman\n\nYeah. But no small effort is training at 10-to-the-26th FLOPS. Currently, no big effort is either. So that\u2019s a dishonest comment. Second of all, the burden of just saying, like, here\u2019s what we\u2019re doing, is not that great. But \u2014 third of all, the underlying thing there \u2014 there\u2019s nothing you can do here that you couldn\u2019t already do on the internet. That\u2019s the real \u2014 either dishonesty or lack of understanding. You could maybe, say, with GPT 4, you can\u2019t do anything you can\u2019t do on the internet. But I don\u2019t think that\u2019s really true, even at GPT 4. Like, there are some new things. And GPT 5 and 6, there will be very new things. And saying that we\u2019re going to be cautious and responsible and have some testing around that \u2014 I think that\u2019s going to look more prudent in retrospect than it maybe sounds right now.\n\ncasey newton\n\nI\u2019d say, for me, these seem like the absolute gentlest regulations you could imagine. It\u2019s like, tell the government and report on any safety testing you did?\n\nsam altman\n\nSeems reasonable.\n\ncasey newton\n\nYeah.\n\nkevin roose\n\nI mean, people are not just saying that these fears are unjustified of AI and sort of existential risk. Some people, some of the more vocal critics of OpenAI, have said that OpenAI \u2014 that you are specifically lying about the risks of human extinction from AI creating fear, so that regulators will come in and make laws or give executive orders that prevent smaller competitors from being able to compete with you. Andrew Ng, who is, I think, one of your professors at Stanford, recently said something to this effect. What\u2019s your response to that? I\u2019m curious if you have thoughts about that.\n\nsam altman\n\nYeah. Like, I actually don\u2019t think we\u2019re all going to go extinct. I think it\u2019s going to be great. I think we\u2019re, like, heading towards the best world ever. But when we deal with a dangerous technology as a society, we often say that we have to confront and successfully navigate the risks to get to enjoy the benefits. And that\u2019s a pretty consensus thing. I don\u2019t \u2014 I don\u2019t think that\u2019s a radical position. You \u2014 you can imag \u2014 I can imagine that if this technology stays on the same curve, there are systems that are capable of significant harm in the future. And you know, like, Andrew also said not that long ago that he thought it was, like, totally irresponsible to talk about AGI, because it was just never happening.\n\nkevin roose\n\nI think he compared it to worrying about overpopulation on Mars.\n\nsam altman\n\nAnd I think now, he might say something different. So, like, it\u2019s \u2014 humans are very bad at having intuition for exponentials. Again, I think it\u2019s going to be great. Like, I wouldn\u2019t work on this if I didn\u2019t think it was going to be great. People love it already, and I think they\u2019re going to love it a lot more. But that doesn\u2019t mean we don\u2019t need to be responsible and accountable and thoughtful about what the downsides could be. And in fact, I think the tech industry often has only talked about the good and not the bad. And that doesn\u2019t go well either.\n\ncasey newton\n\nThe exponential thing is real. I have dealt with this. I\u2019ve talked about the fact that I was only using GPT 3.5 until a few months ago, and finally, at the urging of a friend, upgraded. And I thought \u2014\n\nsam altman\n\nI would have given you a free upgrade. I\u2019m sorry you waited.\n\ncasey newton\n\n(LAUGHING) I should have asked.\n\nsam altman\n\nBut it\u2019s a real improvement.\n\ncasey newton\n\nIt is a real improvement, and not just in the sense of, oh, the copy that it generates is better. It actually transformed my sense of how quickly the industry was moving. It made me think, oh, like, the next generation of this is going to be radically better. And so I think that part of what we\u2019re dealing with is just that it has not been widely distributed enough to get people to reckon with the implications.\n\nsam altman\n\nI disagree with that. I mean, I think that maybe the tech experts say, like, oh, this is not a big deal, whatever. Like, most of the world is like \u2014 who has used even the free version, is like, oh, man, they got real AI.\n\ncasey newton\n\nYeah. Yeah, and you went around the world this year, talking to people in a lot of different countries. I\u2019d be curious to what extent that informed what you just said.\n\nsam altman\n\nSignificantly. I mean, I had a little bit of a sample bias, right? Because the people that wanted to meet me were probably pretty excited. But you do get a sense. And there\u2019s quite a lot of excitement \u2014 maybe more excitement in the rest of the world than the US.\n\nkevin roose\n\nSam, I want to ask you about something else that people are not happy about when it comes to these language and image models, which is this issue of copyright. I think a lot of people view what OpenAI and other companies did, which is hoovering up work from across the internet, using it to train these models that can, in some cases, output things that are similar to the work of living authors or writers or artists, and they just think, like, this is the original sin of the AI industry, and we are never going to forgive them for doing this. What do you think about that? And what would you say to artists or writers who just think that this was a moral lapse? Forget about the legal, whether you\u2019re allowed to do it or not. That it was just unethical for you and other companies to do that in the first place.\n\nsam altman\n\nWell, we block that stuff. Like, you can\u2019t go to DALL-E and generate some \u2014 I mean, you get \u2014 speaking of being annoyed, like, we may be too aggressive on that. But I think \u2014 I think it\u2019s the right thing to do until we figure out some sort of economic model that works for people. And we\u2019re doing some things there now, but we\u2019ve got more to do. Other people in industry do allow quite a lot of that. And I get why artists are annoyed.\n\nkevin roose\n\nI guess I\u2019m talking less about the output question than just the act of taking all of this work, much of it copyrighted, without the explicit permission of the people who created it and using it to train these models. Do you think that \u2014 what would you say to the people who just say, Sam, that was the wrong move, you should have asked, and we will never forgive you for it?\n\nsam altman\n\nWell, first of all, I always have empathy for people who are like, hey, you did this thing, and it\u2019s affecting me, and we can talk about it first, or it was just a new thing. Like, the \u2014 I do think that in the same way humans can read the internet and learn, AI should be allowed to read the internet and learn. It shouldn\u2019t be regurgitating, shouldn\u2019t be violating any copyright laws. But if we\u2019re really going to say that AI doesn\u2019t get to read the internet and learn, and if you read a physics textbook and learn how to do a physics calculation, now, every time you do that in the rest of your life, like, you got to figure out how to \u2014 that seems not a good solution to me. But on individuals\u2019 private work, under \u2014 yeah, we try not to train on that stuff. We really don\u2019t want to be here upsetting people. Again, I think other people in the industry have taken different approaches. And we\u2019ve also done some things that I think, now that we understand more, we will do differently in the future. Like, what we do differently \u2014 we want to figure out new economic models, so that, say, if you\u2019re an artist, we don\u2019t just totally block you. We don\u2019t just not train on your data, which a lot of artists also say, no, I want this in here. I want whatever. But we have a way to help share revenue with you. GPTs are maybe going to be an interesting first example of this. Because people will be able to put private data in there and say, hey, use this version, and there can be a revenue share around it.\n\ncasey newton\n\nI feel like that might be a good place to take a break, and then come back and talk about the future.\n\nkevin roose\n\nYes. Let\u2019s take a break. [MUSIC PLAYING]\n\ncasey newton\n\nWell, I had one question about the future that kind of came out of what we were talking about before the break, which is \u2014 and it\u2019s so big, but I truly need to hear your thoughts on this, which is, what is the future of the internet app as ChatGPT rises? And the reason I ask is, I now have a hotkey on my computer that I type when I want to know something. And it just accesses \u2014 it accesses ChatGPT directly through software called Raycast. And because of this, I am using Google search not nearly as much. I am visiting websites not nearly as much. That has implications for all the publishers and for, frankly, just the model itself. Because presumably, if the economics change, there will be fewer web pages created. There\u2019s less data for ChatGPT to access. So I\u2019m just curious what you have thought about the internet in a world where your product succeeds and the way you want it to.\n\nsam altman\n\nI do think, if this all works, it should really change how we use the internet. There\u2019s a lot of things that the interface 4 is, like, perfect. If you want to mindlessly watch TikTok videos, perfect. But if you\u2019re trying to get information or get a task accomplished, it\u2019s actually quite bad, relative to what we should all aspire for. And you can totally imagine a world where you have a task that, right now, takes, like, hours of stuff, clicking around the internet, and bringing stuff together. And you just ask ChatGPT to do one thing, and it goes off and computes, and you get the answer back. And I\u2019ll be disappointed if we don\u2019t use the internet differently.\n\ncasey newton\n\nYeah. Do you think that the economics of the internet as it is today are robust enough to withstand the challenge that AI poses?\n\nsam altman\n\nProbably.\n\ncasey newton\n\nOK.\n\nsam altman\n\nWhat do you think?\n\ncasey newton\n\nWell, I worry in particular about the publishers. The publishers have been having a hard time already for a million other reasons. But to the extent that they\u2019re driven by advertising and visits to web pages, and to the extent that the visits to the web pages are driven by Google search in particular, a world where web search is just no longer the front page to most of the internet, I think, does require a different kind of web economics.\n\nsam altman\n\nI think it does require a shift, but I think the value is \u2014 so what I thought you were asking about was like, is there going to be enough value there for some economic model to work. And I think that\u2019s definitely going to be the case. Yeah, the model may have to shift. I would love it if ads become less a part of the internet. Like, I was thinking the other day, like \u2014 I just had this, like, for whatever reason, this thought in my head as I was browsing around the internet, being like, there\u2019s more ads than content, everywhere.\n\ncasey newton\n\nI was reading a story today, scrolling on my phone, and I managed to get it to a point where between all of the ads on my relatively large phone screen, there was one line of text from the article visible.\n\nsam altman\n\nYou know, one of the reasons I think people like ChatGPT, even if they can\u2019t articulate it, is we don\u2019t do ads \u2014\n\ncasey newton\n\nYeah.\n\nsam altman\n\n\u2014 like, as an intentional choice. Because there\u2019s plenty of ways you could imagine us putting ads.\n\ncasey newton\n\nTotally.\n\nsam altman\n\nBut we made the choice that ads-plus-AI can get a little dystopic. We\u2019re not saying never. Like, we do want to offer a free service. But a big part of our mission fulfillment, I think, is if we can continue to offer ChatGPT for free at a high quality of service to anybody who wants it and just say, like, hey, here\u2019s free AI, and good free AI \u2014 and no ads. Because I think that really does \u2014 especially as the AI gets really smart, that really does get a little strange.\n\ncasey newton\n\nYeah. Yeah, yeah.\n\nkevin roose\n\nI know we talked about AGI and it not being your favorite term, but it is a term that people in the industry use as sort of a benchmark or a milestone or something that they\u2019re aiming for. And I\u2019m curious what you think the barriers between here and AGI are. Maybe, let\u2019s define AGI as sort of a computer that can do any cognitive task that a human can.\n\nsam altman\n\nIf it \u2014 let\u2019s say we make an AI that is really good, but it can\u2019t go discover novel physics. Would you call that an AGI?\n\nkevin roose\n\nI probably would. Yeah.\n\nsam altman\n\nYou would. OK.\n\nkevin roose\n\nWould you?\n\nsam altman\n\nUh, well, again, I don\u2019t like the term, but I wouldn\u2019t call that, we\u2019re done with the mission. I\u2019d say we still got a lot more work to do.\n\ncasey newton\n\nThe vision is to create something that is better at humans than doing original science that can invent, can discover \u2014\n\nsam altman\n\nWell, I am a believer that all real sustainable human progress comes from scientific and technological progress. And if we can have a lot more of that, I think it\u2019s great. And if the system can do things that we, unaided on our own, can\u2019t, just even as a tool that helps us go do that, then I will consider that a massive triumph and happily be \u2014 I can happily retire at that point.\n\ncasey newton\n\nMm-hmm.\n\nsam altman\n\nBut before that, I can imagine that we do something that creates incredible economic value but is not the kind of AGI superintelligence, whatever you want to call it thing that we should aspire to.\n\ncasey newton\n\nRight. What are some of the barriers to getting to that place where we\u2019re doing novel physics research?\n\nsam altman\n\nUm \u2014\n\ncasey newton\n\nAnd keep in mind, Kevin and I don\u2019t know anything about technology and \u2014\n\nsam altman\n\nThat seems unlikely to be true.\n\ncasey newton\n\n(LAUGHING) Well, if you start talking about retrieval augmented generation or anything, like, I might \u2014\n\nkevin roose\n\nI\u2019ll follow, but you\u2019ll lose Casey.\n\ncasey newton\n\nYou\u2019ll follow. Yeah.\n\nsam altman\n\nWe talked earlier about just the model\u2019s limited ability to reason. And I think that\u2019s one thing that needs to be better. The model needs to be better at reasoning. Like, GPT 4 \u2014 an example of this that my co-founder, Ilya, uses sometimes is really stuck in my mind \u2014 is there was a time in Newton\u2019s life where the right thing for him to do \u2014\n\ncasey newton\n\nYou\u2019re talking, of course, about Isaac Newton, not my life.\n\nsam altman\n\nIsaac Newton.\n\ncasey newton\n\nOK.\n\nsam altman\n\nWell, maybe you, too.\n\ncasey newton\n\nBut maybe my life. We\u2019ll find out. Stay tuned.\n\nsam altman\n\nWhere the right thing for him to do is to read every math textbook he could get his hands on. He should, like, talk to every smart professor or talk to his peers, do problem sets, whatever. And that\u2019s kind of what our models do today. And at some point, there was \u2014 he was never going to invent calculus doing that. What didn\u2019t exist in any textbook. And at some point, he had to go think of new ideas, and then test them out and build them and whatever else. And that phase, that second phase, we don\u2019t do yet. And I think you need that before it\u2019s something we want to call an AGI.\n\nkevin roose\n\nYeah. One thing that I hear from AI researchers is that a lot of the progress that has been made over the past, call it five years, in this type of AI has been just the result of just things getting bigger, right? Bigger models, more compute. Obviously, there\u2019s work around the edges in how you build these things that makes them more useful. But there hasn\u2019t really been a shift on the architectural level of the systems that these models are built on. Do you think that is going to remain true? Or do you think that we need to invent some new process or new mode or new technique to get through some of these barriers?\n\nsam altman\n\nWe will need new research ideas, and we have needed them. I don\u2019t think it\u2019s fair to say there haven\u2019t been any here. I think a lot of the people who say that are not the people building GPT 4, but they\u2019re the people sort of opining from the sidelines. But \u2014 but there is some kernel of truth to it. And the answer is, we have \u2014 OpenAI has a philosophy of, we will just do whatever works. Like, if it\u2019s time to scale the models and work on the engineering challenges, we\u2019ll go do that. If now, we need a new algorithm breakthrough, we\u2019ll go work on that. If now, we need a different kind of data mix, we\u2019ll go work on that. So, like, we just do the thing in front of us. And then, the next one, and then the next one, then the next one. And there are a lot of other people who want to write papers about level 1, 2, 3, and whatever, and there are a lot of other people who want to say, well, it\u2019s not real progress. They just made this incredible thing that people are using and loving. And it\u2019s not real sci \u2014 like, but our belief is like, we will just do whatever we can to usefully drive the progress forward. And we\u2019re kind of open-minded about how we do that.\n\nkevin roose\n\nWhat is superalignment? You all just recently announced that you are devoting a lot of resources and time and computing power to superalignment, and I don\u2019t know what it is. So can you help me understand?\n\ncasey newton\n\nIt\u2019s alignment that comes with sour cream and guacamole.\n\nkevin roose\n\nThere you go.\n\ncasey newton\n\nSan Francisco taco shop. That\u2019s a very San Francisco-specific joke, but it\u2019s pretty good. I\u2019m sorry. Go ahead, Sam.\n\nsam altman\n\nI don\u2019t \u2014 can I leave it at that? I don\u2019t really wanna follow \u2014 I mean, that was such a good answer. No, so alignment is how you get these models to behave in accordance with the human who\u2019s using them, what they want. And superalignment is how you do that for supercapable systems. So we know how to align GPT 4 pretty well, but \u2014 like, better than people thought we were going to be able to do. There\u2019s this \u2014 when we put out GPT 2 and 3, people were like, oh, it\u2019s irresponsible research, because this is always going to just, like, spew toxic shit. You\u2019re never going to get it. And it actually turns out, like, we\u2019re able to align GPT 4 reasonably well. Maybe too well.\n\ncasey newton\n\nYeah. It\u2019s \u2014 I mean, good luck getting it to talk about sex, is my official comment about ChatGPT 4. [LAUGHS]\n\nsam altman\n\nBut that\u2019s \u2014 in some sense, that\u2019s an alignment failure, because that\u2019s \u2014 it\u2019s not doing what you want it to do.\n\ncasey newton\n\nYeah.\n\nsam altman\n\nSo \u2014 but now, we have that \u2014 now, we have the social part of the problem. We can technically do it.\n\ncasey newton\n\nRight.\n\nsam altman\n\nBut we don\u2019t yet know what the new challenges will be for much more capable systems. And so that\u2019s what that team research is.\n\nkevin roose\n\nSo, like, what kinds of questions are they investigating, or what research are they doing? Because I confess, I sort of \u2014 I lose my grounding in reality when you start talking about supercapable systems and the problems that can emerge with them. Is this sort of a theoretical future forecasting team?\n\nsam altman\n\nWell, they try to do work that is useful today, but for the theoretical systems of the future. So they all have their first result coming out, I think, pretty soon. But yeah, they\u2019re interested in these questions of, as the systems get more capable than humans, what is it going to take to reliably solve the alignment challenge?\n\ncasey newton\n\nAnd I mean, this is the stuff where my brain does feel like it starts to melt as I ponder the implications, right? Because you\u2019ve made something that is smarter than every human, but you, the human, have to be smart enough to ensure that it always acts in your interests, even though, by definition, it is way smarter.\n\nsam altman\n\nWe need some help there.\n\nkevin roose\n\nYeah, I do want to stick on this issue of alignment or superalignment, because I think there\u2019s an unspoken assumption in there that \u2014 well, you just put it as, alignment is what the user wants it to behave like. And obviously, there are a lot of users with good intentions.\n\nsam altman\n\nNo, no, yeah, it has to be like what society and the user can intersect on. There are going to have to be some rules here.\n\nkevin roose\n\nAnd I guess, where do you derive those rules? Because if you\u2019re Anthropic, you use the UN Declaration of Human Rights and the Apple terms of service, and that becomes \u2014\n\ncasey newton\n\nThe two most important documents \u2014 \u2014 in rights governance.\n\nkevin roose\n\nBut If you\u2019re not just going to borrow someone else\u2019s rules, how do you decide which values these things should align themselves to?\n\nsam altman\n\nSo we\u2019re doing this thing \u2014 we\u2019ve been doing this thing, where we\u2019ve been doing these democratic input governance grants, where we\u2019re giving different research teams money to go off and study different proposals. And there\u2019s some very interesting ideas in there about how to fairly decide that. The naive approach to this that I have always been interested in \u2014 maybe we\u2019ll try at some point \u2014 is what if you had hundreds of millions of ChatGPT users spend an hour, a few hours a year, answering questions about what they thought the default setting should be, what the wide bound should be. Eventually, you need more than just ChatGPT users. You need the whole world represented in some way, because even if you\u2019re not using it, you\u2019re still impacted by it. But to start, what if you literally just had ChatGPT chat with its users? It can \u2014 I think it\u2019s very important. It would be very important in this case to let the users make final decisions, of course. But you could imagine it\u2019s saying, like, hey, you answered this question this way. Here\u2019s how this would impact other users in a way you might not have thought of. If you want to stick with your answer, that\u2019s totally up to you, but are you sure, given this new data? And then, you could imagine GPT 5 or whatever just learning that collective preference set. And I think that\u2019s interesting to consider.\n\ncasey newton\n\nYeah. I want to \u2014\n\nsam altman\n\nBetter than the Apple terms of service, let\u2019s say.\n\ncasey newton\n\n[CHUCKLES]: I want to ask you about this feeling. Kevin and I call it \u201cAI vertigo.\u201d Is this a widespread term that people use?\n\nkevin roose\n\nNo, I think you invented this.\n\ncasey newton\n\nSo there is this moment when you contemplate, even just kind of the medium AI future, you start to think about what it might mean for the job market, your own job, your daily life, or society. And there is this kind of dizziness that I find sets in. This year, I actually had a nightmare about AGI. And then, I sort of asked around, and I feel like people who work on this stuff \u2014 like, that\u2019s not uncommon. I wonder, for you, if you have had these moments of vertigo, if you continue to have them. Or is there at some point where you think about it long enough, that you feel like you get your legs underneath you?\n\nsam altman\n\nI think I used to have \u2014 I mean, there were some \u2014 I can point to these moments, but there were some very strange, like, extreme vertigo moments, particularly around the launch of GPT 3. But you do get your legs under you.\n\ncasey newton\n\nYeah. What \u2014\n\nsam altman\n\nAnd I think the future will somehow be less different than we think. Like, it\u2019s this amazing thing to say, right? Like, we invent AGI, and it matters less than we think. It doesn\u2019t sound like a sentence that parses. And yet, it\u2019s what I expect to happen.\n\nkevin roose\n\nWhy is that?\n\nsam altman\n\nThere\u2019s a lot of inertia in society, and humans are remarkably adaptable to any amount of change.\n\nkevin roose\n\nHmm. One question I get a lot, that I imagine you do, too, is from people who want to know what they can do. You mentioned adaptation as being necessary on a societal level. I think for many years, the conventional wisdom was that if you wanted to adapt to a changing world, you should learn how to code. That was like the classic advice.\n\nsam altman\n\nIt may not be such good advice anymore.\n\nkevin roose\n\nExactly. So now, AI systems can code pretty well. For a long time, the conventional wisdom was that creative work was sort of untouchable by machines. If you were a factory worker, you might get automated out of your job. But if you were an artist or a writer, that was impossible for computers to do. Now, we see that\u2019s no longer safe. So where is this sort of high ground here? Like, where can people focus their energy if they want skills and abilities that AI is not going to be able to replace?\n\nsam altman\n\nMy answer is \u2014 my meta answer is, you always \u2014 it\u2019s always the right bet to just get good at the most powerful new tools, most capable new tools. And so when computer programming was that, you did want to become a programmer. And now that AI tools totally change what one person can do, you want to get really good at using AI tools. And so, like, having a sense for how to work with ChatGPT and other things \u2014 that is the high ground. And that\u2019s like \u2014 that\u2019s \u2014 we\u2019re not going back. Like, that\u2019s going to be part of the world. And you can use it in all sorts of ways, but getting fluent at it, I think, is really important.\n\nkevin roose\n\nI want to challenge that. Because I think you\u2019re partially right in that I think there is an opportunity for people to embrace AI and sort of become more resilient to disruption that way. But I also think if you look back through history, it\u2019s not like we learn how to do something new, and then the old way just goes away, right? We still make things by hand. There\u2019s still an artisanal market. So do you think there\u2019s going to be people who just decide, you know what? I don\u2019t want to use this stuff.\n\nsam altman\n\nTotally.\n\nkevin roose\n\nAnd there\u2019s going to be something valuable in their, sort of \u2014 I don\u2019t know \u2014 non-AI-assisted work?\n\nsam altman\n\nI expect, like \u2014 I expect that if we look forward to the future, things \u2014 everything that \u2014 things that we want to be cheap can get much cheaper, and things that we want to be expensive are going to be astronomically expensive.\n\nkevin roose\n\nLike what?\n\nsam altman\n\nReal estate, handmade goods, art. And so totally, like, there will be a huge premium on things like that. And there will be many people who really \u2014 there\u2019s always been a \u2014 even when machine-made products have been much better, there has always been a premium on handmade products. And I\u2019d expect that to intensify.\n\ncasey newton\n\nThis is also a bit of a curveball. Very curious to get your thoughts. Where do you come down on the idea of AI romances? Are these net good for society?\n\nsam altman\n\nI don\u2019t want one, personally.\n\ncasey newton\n\nYou don\u2019t want one. OK. But it\u2019s clear that there is a huge demand for this, right? Yeah. Like, I think that \u2014 I mean, you know, Replica is building these. They seem like they\u2019re doing very well. I would be shocked if this is not a multi-billion dollar company, right?\n\nsam altman\n\nSomeone will \u2014\n\ncasey newton\n\nThat\u2019s what I\u2019m saying. Yeah, somebody will. Yeah.\n\nsam altman\n\nFor sure.\n\ncasey newton\n\nYeah. Do you \u2014 like, I just personally think we\u2019re going to have a big culture war. Like, I think Fox News is going to be doing segments about the generation loss to AI girlfriends and boyfriends at some point within the next few years. But at the same time, you look at all the data on loneliness, and it seems like, well, if we can give people companions that make them happy during the day, it could be a net-good thing.\n\nsam altman\n\nIt\u2019s complicated.\n\ncasey newton\n\nYeah.\n\nsam altman\n\nI have misgivings, but I don\u2019t \u2014 this is not a place where I think I get to impose what I think is good on other people.\n\ncasey newton\n\nTotally. But OK, but it sounds like this is not at the top of your product roadmap \u2014 is building the boyfriend API.\n\nsam altman\n\nNo.\n\ncasey newton\n\nAll right.\n\nkevin roose\n\nYou recently posted on X that you expect AI to be capable of superhuman persuasion well before it is superhuman at general intelligence, which may lead to some very strange outcomes. Can you expand on that? What are some things that AI might become very good at persuading us to do, and what are some of those strange outcomes you\u2019re worried about?\n\nsam altman\n\nThe thing I was thinking about at that moment was the upcoming election. There\u2019s a huge focus on the US 2024 election. There\u2019s a huge focus on deepfakes and the impact of AI there. And I think that\u2019s reasonable to worry about, good to worry about. But we already have some societal antibodies towards people seeing, like, doctored photos or whatever. And yeah, they\u2019re going to get more compelling. It\u2019s going to be more. But we kind of know that those are there. There\u2019s a lot of discussion about that. There\u2019s almost no discussion about what are the new things AI can do to influence an election \u2014 AI tools can do to influence an election. And one of those is to carefully \u2014 one-on-one persuade individual people.\n\ncasey newton\n\nTailored messages.\n\nsam altman\n\nTailored messages. That\u2019s a new thing that the content farms couldn\u2019t quite do.\n\nkevin roose\n\nRight. And that\u2019s not AGI, but that could still be pretty harmful.\n\nsam altman\n\nI think so, yeah.\n\nkevin roose\n\nI know we are running out of time, but I do want to push us a little bit further into the future than the sort of \u2014 I don\u2019t know \u2014 maybe five-year horizon we\u2019ve been talking about. If you can imagine a good post-AGI world, a world in which we have reached this threshold, whatever it is, what does that world look like? Does it have a government? Does it have companies? / What do people do all day?\n\nsam altman\n\nLike, a lot of material abundance. People are \u2014 people continue to be very busy, but the way we define work always moves. Like, if you \u2014 our jobs would not have seemed like real jobs to people several hundred years ago, right? This would have seemed like incredibly silly entertainment. It\u2019s important to me. It\u2019s important to you. And hopefully, it has some value to other people as well. There will be \u2014 and the jobs of the future may seem \u2014 I hope they seem even sillier to us. But I hope the people get even more fulfillment, and I hope society gets even more fulfillment out of them. But everybody can have a really great quality of life, like, to a degree that I think we probably just can\u2019t imagine now. Of course, we\u2019ll still have governments. Of course, people will still squabble over whatever they squabble over. Less different in all of these ways than someone would think. And then, like, unbelievably different in terms of what you can get a computer to do for you.\n\nkevin roose\n\nOne fun thing about becoming a very prominent person in the tech industry as you are is that people have all kinds of theories about you. One fun one that I heard the other day is that you have a secret Twitter account where you are way less measured and careful.\n\nsam altman\n\nI don\u2019t anymore. I did for a while. I decided I just couldn\u2019t keep up with the OpSec.\n\ncasey newton\n\nIt\u2019s so hard to lead a double life.\n\nkevin roose\n\nWhat was your \u2014 what was your secret Twitter account?\n\nsam altman\n\nObviously, I can\u2019t. I mean, I had a good alt. A lot of people have good alts. But \u2014\n\ncasey newton\n\nYour name is literally Sam \u201cAlt-man.\u201d I mean, it would have been weird if you didn\u2019t have one.\n\nsam altman\n\nBut I think I just got too \u2014 like, too well-known or something to be doing that.\n\nkevin roose\n\nYeah. Well, and the theory that I heard attached to this was that you are secretly an accelerationist, a person who wants AI to go as fast as possible, and that all this careful diplomacy that you\u2019re doing and asking for regulation \u2014 this is really just the sort of polite face that you put on for society, but deep down, you just think we should go all gas, no breaks, toward the future.\n\nsam altman\n\nNo, I certainly don\u2019t think all gas, no brakes to the future, but I do think we should go to the future. And that probably is what differentiates me than, like, most of the AI companies is, I think AI is good. Like, I don\u2019t secretly hate what I do all day. I think it\u2019s going to be awesome. Like, I want to see this get built. I want people to benefit from this. So all gas, no brake \u2014 certainly not. And I don\u2019t even think most people who say it mean it. But I am a believer that this is a tremendously beneficial technology and that we have got to find a way, safely and responsibly, to get it into the hands of the people, to confront the risks, so that we get to enjoy the huge rewards. And maybe, relative to the prior of most people who work on AI, that does make me an accelerationist. But compared to those accelerationist people, I\u2019m clearly not them. So you know, I\u2019m somewhere \u2014 I think you want the CEO of this company to be somewhere \u2014\n\nkevin roose\n\nYou\u2019re accelerationist-adjacent.\n\nsam altman\n\n\u2014 in the middle, which I think I am.\n\ncasey newton\n\nYou\u2019re gas and brakes.\n\nsam altman\n\nI believe this will be the most important and beneficial technology humanity has ever has yet invented. And I also believe that if we\u2019re not careful about it, it can be quite disastrous. And so we have to navigate it carefully.\n\ncasey newton\n\nYeah.\n\nkevin roose\n\nYeah.\n\ncasey newton\n\nSam, thanks so much for coming on \u201cHard Fork.\u201d\n\nsam altman\n\nThank you guys.\n\nkevin roose\n\nWhen we come back, we\u2019ll have some notes on that interview, now with the benefit of hindsight. [MUSIC PLAYING] So Casey, now, with five days of hindsight on this interview and after everything that has transpired between the time that we originally recorded it and now, are there any things that Sam said that stuck out to you as being particularly relevant to understanding this conflict?\n\ncasey newton\n\nI keep coming back to the question that you asked him about whether he was a closet accelerationist. Right? Is he somebody who is telling the world, hey, I\u2019m trying to do this in a very gradual, iterative way, but behind the scenes, is working to hit the accelerator? And during the interview, he gave a sort of very diplomatic answer, as you might expect, to that question. But learning what we have learned over the past few days, I do feel like he is on the more accelerationist side of things. And certainly, all of the people rallying to his defense on social media over the weekend \u2014 a good number of them were rallying because they think he is the one who is pushing AI forward. How about you? What did you think?\n\nkevin roose\n\nTotally. I thought that was very interesting and, now, with the additional context of the last three days, explains a lot about the conflict between Sam Altman and the board. We still don\u2019t, obviously, know exactly what happened. But I can imagine that Sam going around saying things like, I think that the future is going to be amazing, and I think that everything\u2019s going to be great with AI \u2014 I can see why that would land poorly with board members who are much more concerned, from the looks of things, about how the future is going to look. So it seems like he is sort of an optimist who is running a company where the board of that company is less optimistic about AI. And that just seems like a fundamental tension that it sounds like they were not able to get past. I was also struck by something else that he said. It was interesting. When we talked about GPTs, these build-your-own chat bots that OpenAI released at Dev Day a few weeks ago, he said that he was embarrassed, because they were so simple and sort of not all that functional and pretty prosaic. And that\u2019s just such a striking contrast, because some of the reporting that came out over the weekend suggested that the GPTs were actually one of the things that scared Ilya Sutskever and the board, that giving these AIs more agency and more autonomy and allowing them to do things on the internet was, at least if you believe the reporting, part of what made the board so anxious.\n\ncasey newton\n\nYes. And at the same time, if it is true that the board and Ilya found out about GPTs at Developer Day, that speaks to some fundamental problems in how this company was being run. And I don\u2019t know if that is a Sam thing or a board thing or what, but you would think that by the time the keynote was being delivered, all of those stakeholders would have been looped in.\n\nkevin roose\n\nTotally. And I guess my other reflection on that interview is that it just sounded like Sam had no idea that any of this was brewing. This did not sound like someone who was trying to carefully walk the line between being optimistic and being scared of existential risk. This did not sound like someone who thought that he was on thin ice with his board. This sounded like someone who was very confidently charging ahead with his vision for the future of AI.\n\ncasey newton\n\nThat\u2019s right.\n\nkevin roose\n\nI really hope we are not doing more emergency podcasts on this. Could the news just give us a little break for a minute?\n\ncasey newton\n\nWell, if I were you, Kevin, I would clear your Tuesday morning.\n\nkevin roose\n\n[LAUGHS]: Oh, god. Happy Thanksgiving.\n\ncasey newton\n\nHappy Thanksgiving!\n\nkevin roose"
    },
    {
        "metadata": {
            "title": "OpenAI announces platform for making custom ChatGPTs - The Verge",
            "description": "OpenAI announces platform for making custom ChatGPTs  The Verge",
            "published date": "Mon, 06 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiWGh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDIzLzExLzYvMjM5NDg5NTcvb3BlbmFpLWNoYXRncHQtZ3B0LWN1c3RvbS1kZXZlbG9wZXItcGxhdGZvcm3SAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "With the release of ChatGPT one year ago, OpenAI introduced the world to the idea of an AI chatbot that can seemingly do anything. Now, the company is releasing a platform for making custom versions of ChatGPT for specific use cases \u2014 no coding required.\n\nIn the coming weeks, these AI agents, which OpenAI is calling GPTs, will be accessible through the GPT Store. Details about how the store will look and work are scarce for now, though OpenAI is promising to eventually pay creators an unspecified amount based on how much their GPTs are used. GPTs will be available to paying ChatGPT Plus subscribers and OpenAI enterprise customers, who can make internal-only GPTs for their employees.\n\n\u201cSince launching ChatGPT, people have been asking for ways to customize ChatGPT to fit specific ways that they use it,\u201d OpenAI said in a statement shared with The Verge. \u201cWe launched Custom Instructions in July that let you set some preferences, but requests for more control kept coming. Many power users maintain a list of carefully crafted prompts and instruction sets, manually copying them into ChatGPT. GPTs now do all of that for you.\u201d\n\nAn example of what a custom GPT looks like. Image: OpenAI\n\nDuring a recent demo I received of OpenAI\u2019s GPT platform, a bot called \u201cCreative Writing Coach\u201d critiqued an uploaded PDF of a writing sample. Over a period of about two minutes, I watched another GPT be spun up to help attendees navigate DevDay. The platform auto-named the bot \u201cEvent Navigator,\u201d generated a profile picture for it using DALL-E, and ingested a PDF attachment with the event\u2019s schedule to inform its answers.\n\nOpenAI\u2019s interface lets you guide how you want a GPT to interact with people before you publish. The DevDay Event Navigator agent I saw during my demo was instructed to be helpful and concise and to avoid scheduling conflicts. OpenAI autogenerated several conversation starter prompts, such as \u201cWhat\u2019s the first session today?\u201d\n\nEach GPT can be granted access to web browsing, DALL-E, and OpenAI\u2019s Code Interpreter tool for writing and executing software. There\u2019s also a \u201cKnowledge\u201d section in the builder interface for uploading custom data, like the DevDay event schedule. With another feature called Actions, OpenAI is letting GPTs hook into external services for accessing data like emails, databases, and more, starting with Canva and Zapier.\n\nWhat OpenAI\u2019s GPT builder interface looks like. OpenAI\n\nThe introduction of custom GPTs means that OpenAI is now competing with other AI bot platforms like Character.AI and Meta, which recently introduced a slew of its own AI personas in WhatsApp, Instagram, and Messenger. OpenAI is positioning its platform as being more utility-focused than its competitors, rather than emphasizing bots that act like people, though it isn\u2019t against people building GPTs with human-like personas.\n\nOpenAI is now competing with other AI bot platforms like Character.AI and Meta\n\nThe creators of GPTs won\u2019t be able to view the chats people are having with them, and it\u2019s unclear what high-level usage data they\u2019ll get access to. OpenAI says it will be monitoring activity to block things like fraud, hate speech, and \u201cadult themes.\u201d When the GPT Store launches down the road, OpenAI will only accept agents from people who have verified their identity. Initially, GPTs will be accessible through shareable web links."
    },
    {
        "metadata": {
            "title": "ChatGPT-maker OpenAI signs deal with AP to license news stories - The Associated Press",
            "description": "ChatGPT-maker OpenAI signs deal with AP to license news stories  The Associated Press",
            "published date": "Thu, 13 Jul 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiXmh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL29wZW5haS1jaGF0Z3B0LWFzc29jaWF0ZWQtcHJlc3MtYXAtZjg2Zjg0YzViY2MyZjNiOTgwNzRiMzg1MjFmNWY3NWHSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "ChatGPT-maker OpenAI and The Associated Press said Thursday that they\u2019ve made a deal for the artificial intelligence company to license AP\u2019s archive of news stories.\n\n\u201cThe arrangement sees OpenAI licensing part of AP\u2019s text archive, while AP will leverage OpenAI\u2019s technology and product expertise,\u201d the two organizations said in a joint statement.\n\nFinancial terms of the deal were not disclosed.\n\nOpenAI and other technology companies must ingest large troves of written works, such as books, news articles and social media chatter, to improve their AI systems known as large language models. Last year\u2019s release of ChatGPT has sparked a boom in \u201cgenerative AI\u201d products that can create new passages of text, images and other media.\n\nThe tools have raised concerns about their propensity to spout falsehoods that are hard to notice because of the system\u2019s strong command of the grammar of human languages. They also have raised questions about to what extent news organizations and others whose writing, artwork, music or other work was used to \u201ctrain\u201d the AI models should be compensated.\n\nThis week, the U.S. Federal Trade Commission told OpenAI it had opened an investigation into whether the company had engaged in unfair or deceptive privacy or data security practices in scraping public data \u2014 or caused harm by publishing false information through its chatbot products. The FTC did not immediately reply to a request for comment on the investigation, which The Washington Post was first to report.\n\nAlong with news organizations, book authors have sought compensation for their works being used to train AI systems. More than 4,000 writers --- among them Nora Roberts, Margaret Atwood, Louise Erdrich and Jodi Picoult \u2014 signed a letter late last month to the CEOs of OpenAI, Google, Microsoft, Meta and other AI developers accusing them of exploitative practices in building chatbots that \u201cmimic and regurgitate\u201d their language, style and ideas. Some novelists and the comedian Sarah Silverman have also sued OpenAI for copyright infringement.\n\n\u201cWe are pleased that OpenAI recognizes that fact-based, nonpartisan news content is essential to this evolving technology, and that they respect the value of our intellectual property,\u201d said a written statement from Kristin Heitmann, AP senior vice president and chief revenue officer. \u201cAP firmly supports a framework that will ensure intellectual property is protected and content creators are fairly compensated for their work.\u201d\n\nThe two companies said they are also examining \u201cpotential use cases for generative AI in news products and services,\u201d though didn\u2019t give specifics. OpenAI and AP both \u201cbelieve in the responsible creation and use of these AI systems,\u201d the statement said.\n\nOpenAI will have access to AP news stories going back to 1985.\n\nThe AP deal is valuable to a company like OpenAI because it provides a trove of material that it can use for training purposes, and is also a hedge against losing access to material because of lawsuits that have threatened its access to material, said Nick Diakopoulos, a professor of communications studies and computer science at Northwestern University.\n\n\u201cIn order to guard against how the courts may decide, maybe you want to go out and sign licensing deals so you\u2019re guaranteed legal access to the material you\u2019ll need,\u201d Diakopoulos said.\n\nThe AP doesn\u2019t currently use any generative AI in its news stories, but has used other forms of AI for nearly a decade, including to automate corporate earnings reports and recap some sporting events. It also runs a program that helps local news organizations incorporate AI into their operations, and recently launched an AI-powered image archive search.\n\nThe deal\u2019s effects could reach far beyond the AP because of the organization\u2019s size and its deep ties to other news outlets, said news industry analyst Ken Doctor.\n\nWhen AP decided to open up its content for free on the internet in the 1990s, it led many newspaper companies to do the same, which \u201cturned out to be a very bad idea\u201d for the news business, Doctor said.\n\nHe said navigating \u201ca new, AI-driven landscape is deeply uncertain\u201d and presents similar risks.\n\n\u201cThe industry is far weaker today. AP is in OK shape. It\u2019s stable. But the newspaper industry around it is really gasping for air,\u201d Doctor said. \u201cOn the positive side, AP has the clout to do a deal like this and can work with local publishers to try to assess both the potential and the risk.\u201d\n\n___\n\nAssociated Press writer David Bauder contributed to this report."
    },
    {
        "metadata": {
            "title": "OpenAI, maker of ChatGPT, hit with proposed class action lawsuit alleging it stole people\u2019s data - CNN",
            "description": "OpenAI, maker of ChatGPT, hit with proposed class action lawsuit alleging it stole people\u2019s data  CNN",
            "published date": "Wed, 28 Jun 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8wNi8yOC90ZWNoL29wZW5haS1jaGF0Z3B0LW1pY3Jvc29mdC1kYXRhLXN1ZWQvaW5kZXguaHRtbNIBSmh0dHBzOi8vYW1wLmNubi5jb20vY25uLzIwMjMvMDYvMjgvdGVjaC9vcGVuYWktY2hhdGdwdC1taWNyb3NvZnQtZGF0YS1zdWVk?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnn.com",
                "title": "CNN"
            }
        },
        "article": "CNN \u2014\n\nOpenAI, the company behind the viral ChatGPT tool, has been hit with a lawsuit alleging the company stole and misappropriated vast swaths of peoples\u2019 data from the internet to train its AI tools.\n\nThe proposed class action lawsuit, filed Wednesday in a California federal court, claims that OpenAI secretly scraped \u201cmassive amounts of personal data from the internet,\u201d according to the complaint. The nearly 160-page complaint alleges that this personal data, including \u201cessentially every piece of data exchanged on the internet it could take,\u201d was also seized by the company without notice, consent or \u201cjust compensation.\u201d\n\nMoreover, this data scraping occurred at an \u201cunprecedented scale,\u201d the suit claims.\n\nOpenAI did not immediately respond to CNN\u2019s request for comment Wednesday. Microsoft, a major investor into OpenAI, was also named as a defendant in the suit and did not immediately respond to a request for comment.\n\n\u201cBy collecting previously obscure personal data of millions and misappropriating it to develop a volatile, untested technology, OpenAI put everyone in a zone of risk that is incalculable \u2013 but unacceptable by any measure of responsible data protection and use,\u201d Timothy K. Giordano, a partner at Clarkson, the law firm behind the suit, said in a statement to CNN Wednesday.\n\nThe complaint also claims that OpenAI products \u201cuse stolen private information, including personally identifiable information, from hundreds of millions of internet users, including children of all ages, without their informed consent or knowledge.\u201d\n\nThe lawsuit seeks injunctive relief in the form of a temporary freeze on further commercial use of OpenAI\u2019s products. It also seeks payments of \u201cdata dividends\u201d as financial compensation to people whose information was used to develop and train OpenAI\u2019s tools.\n\nOpenAI publicly launched ChatGPT late last year, and the tool immediately went viral for its ability to generate compelling, human-sounding responses to user prompts. The success of ChatGPT spurred an apparent AI arms race in the tech world, as companies big and small are now racing to develop and deploy AI tools into as many products as possible."
    },
    {
        "metadata": {
            "title": "ChatGPT maker OpenAI faces class action lawsuit over data to train AI - The Washington Post",
            "description": "ChatGPT maker OpenAI faces class action lawsuit over data to train AI  The Washington Post",
            "published date": "Wed, 28 Jun 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiWWh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS90ZWNobm9sb2d5LzIwMjMvMDYvMjgvb3BlbmFpLWNoYXRncHQtbGF3c3VpdC1jbGFzcy1hY3Rpb24v0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.washingtonpost.com",
                "title": "The Washington Post"
            }
        },
        "article": "SAN FRANCISCO \u2014 A California-based law firm is launching a class-action lawsuit against OpenAI, alleging the artificial-intelligence company that created popular chatbot ChatGPT massively violated the copyrights and privacy of countless people when it used data scraped from the internet to train its tech. The lawsuit seeks to test out a novel legal theory \u2014 that OpenAI violated the rights of millions of internet users when it used their social media comments, blog posts, Wikipedia articles and family recipes. Clarkson, the law firm behind the suit, has previously brought large-scale class-action lawsuits on issues ranging from data breaches to false advertising.\n\nThe firm wants to represent \u201creal people whose information was stolen and commercially misappropriated to create this very powerful technology,\u201d said Ryan Clarkson, the firm\u2019s managing partner.\n\nAdvertisement\n\nThe case was filed in federal court in the northern district of California Wednesday morning. A spokesman for OpenAI did not respond to a request for comment.\n\nThe lawsuit goes to the heart of a major unresolved question hanging over the surge in \u201cgenerative\u201d AI tools such as chatbots and image generators. The technology works by ingesting billions of words from the open internet and learning to build inferences between them. After consuming enough data, the resulting \u201clarge language models\u201d can predict what to say in response to a prompt, giving them the ability to write poetry, have complex conversations and pass professional exams. But the humans who wrote those billions of words never signed off on having a company such as OpenAI use them for its own profit.\n\n\u201cAll of that information is being taken at scale when it was never intended to be utilized by a large language model,\u201d Clarkson said. He said he hopes to get a court to institute some guardrails on how AI algorithms are trained and how people are compensated when their data is used.\n\nAdvertisement\n\nThe firm already has a group of plaintiffs and is actively looking for more.\n\nThe legality of using data pulled from the public internet to train tools that could prove highly lucrative to their developers is still unclear. Some AI developers have argued that the use of data from the internet should be considered \u201cfair use,\u201d a concept in copyright law that creates an exception if the material is changed in a \u201ctransformative\u201d way.\n\nShare this article Share\n\nThe question of fair use is \u201can open issue that we will be seeing play out in the courts in the months and years to come,\u201d said Katherine Gardner, an intellectual-property lawyer at Gunderson Dettmer, a firm that mostly represents tech start-ups. Artists and other creative professionals who can show their copyrighted work was used to train the AI models could have an argument against the companies using it, but it\u2019s less likely that people who simply posted or commented on a website would be able to win damages, she said.\n\nAdvertisement\n\n\u201cWhen you put content on a social media site or any site, you\u2019re generally granting a very broad license to the site to be able to use your content in any way,\u201d Gardner said. \u201cIt\u2019s going to be very difficult for the ordinary end user to claim that they are entitled to any sort of payment or compensation for use of their data as part of the training.\u201d\n\nThe suit also adds to the growing list of legal challenges to the companies building and hoping to profit from AI tech. A class-action lawsuit was filed in November against OpenAI and Microsoft for how the companies used computer code in the Microsoft-owned online coding platform GitHub to train AI tools. In February, Getty Images sued Stability AI, a smaller AI start-up, alleging it illegally used its photos to train its image-generating bot. And this month OpenAI was sued for defamation by a radio host in Georgia who said ChatGPT produced text that wrongfully accused him of fraud.\n\nOpenAI isn\u2019t the only company using troves of data scraped from the open internet to train their AI models. Google, Facebook, Microsoft and a growing number of other companies are all doing the same thing. But Clarkson decided to go after OpenAI because of its role in spurring its bigger rivals to push out their own AI when it captured the public\u2019s imagination with ChatGPT last year, Clarkson said.\n\nAdvertisement\n\n\u201cThey\u2019re the company that ignited this AI arms race,\u201d he said. \u201cThey\u2019re the natural first target.\u201d\n\nOpenAI doesn\u2019t share what kind of data went into its latest model, GPT4, but previous versions of the tech have been shown to have digested Wikipedia pages, news articles and social media comments. Chatbots from Google and other companies have used similar data sets.\n\nRegulators are discussing enacting new laws that require more transparency from companies about what data went into their AI. It\u2019s also possible that a court case could prompt a judge to force a company such as OpenAI to turn over information on what data it used, said Gardner, the intellectual-property lawyer.\n\nSome companies have tried to stop AI firms from scraping their data. In April, music distributor Universal Music Group asked Apple and Spotify to block scrapers, according to the Financial Times. Social media site Reddit is shutting off access to its data stream, citing how Big Tech companies have for years scraped the comments and conversations on its site. Twitter owner Elon Musk threatened to sue Microsoft for using Twitter data it had gotten from the company to train its AI. Musk is building his own AI company.\n\nAdvertisement"
    },
    {
        "metadata": {
            "title": "Exclusive: OpenAI Lobbied E.U. to Water Down AI Regulation - TIME",
            "description": "Exclusive: OpenAI Lobbied E.U. to Water Down AI Regulation  TIME",
            "published date": "Tue, 20 Jun 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiM2h0dHBzOi8vdGltZS5jb20vNjI4ODI0NS9vcGVuYWktZXUtbG9iYnlpbmctYWktYWN0L9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://time.com",
                "title": "TIME"
            }
        },
        "article": "The CEO of OpenAI, Sam Altman, has spent the last month touring world capitals where, at talks to sold-out crowds and in meetings with heads of governments, he has repeatedly spoken of the need for global AI regulation.\n\nBut behind the scenes, OpenAI has lobbied for significant elements of the most comprehensive AI legislation in the world\u2014the E.U.\u2019s AI Act\u2014to be watered down in ways that would reduce the regulatory burden on the company, according to documents about OpenAI\u2019s engagement with E.U. officials obtained by TIME from the European Commission via freedom of information requests.\n\nIn several cases, OpenAI proposed amendments that were later made to the final text of the E.U. law\u2014which was approved by the European Parliament on June 14, and will now proceed to a final round of negotiations before being finalized as soon as January.\n\nRead More: The A to Z of Artificial Intelligence\n\nIn 2022, OpenAI repeatedly argued to European officials that the forthcoming AI Act should not consider its general purpose AI systems\u2014including GPT-3, the precursor to ChatGPT, and the image generator Dall-E 2\u2014to be \u201chigh risk,\u201d a designation that would subject them to stringent legal requirements including transparency, traceability, and human oversight.\n\nThat argument brought OpenAI in line with Microsoft, which has invested $13 billion into the AI lab, and Google, both of which have previously lobbied E.U. officials in favor of loosening the Act\u2019s regulatory burden on large AI providers. Both companies have argued that the burden for complying with the Act\u2019s most stringent requirements should be on companies that explicitly set out to apply an AI to a high risk use case\u2014not on the (often larger) companies that build general purpose AI systems.\n\n\u201cBy itself, GPT-3 is not a high-risk system,\u201d said OpenAI in a previously unpublished seven-page document that it sent to E.U. Commission and Council officials in September 2022, titled OpenAI White Paper on the European Union\u2019s Artificial Intelligence Act. \u201cBut [it] possesses capabilities that can potentially be employed in high risk use cases.\u201d\n\nTIME is publishing the White Paper in full alongside this story.\n\nThese lobbying efforts by OpenAI in Europe have not previously been reported, though Altman has recently become more vocal about the legislation. In May, he told reporters in London that OpenAI could decide to \u201ccease operating\u201d in Europe if it deemed itself unable to comply with the regulation, of which he said he had \u201ca lot\u201d of criticisms. He later walked back the warning, saying his company had no plans to leave and intends to cooperate with the E.U.\n\nStill, OpenAI\u2019s lobbying effort appears to have been a success: the final draft of the Act approved by E.U. lawmakers did not contain wording present in earlier drafts suggesting that general purpose AI systems should be considered inherently high risk. Instead, the agreed law called for providers of so-called \u201cfoundation models,\u201d or powerful AI systems trained on large quantities of data, to comply with a smaller handful of requirements including preventing the generation of illegal content, disclosing whether a system was trained on copyrighted material, and carrying out risk assessments. OpenAI supported the late introduction of \u201cfoundation models\u201d as a separate category in the Act, a company spokesperson told TIME.\n\nMore from TIME\n\nBack in September 2022, however, this apparent compromise had yet to be struck. In one section of the White Paper OpenAI shared with European officials at the time, the company pushed back against a proposed amendment to the AI Act that would have classified generative AI systems such as ChatGPT and Dall-E as \u201chigh risk\u201d if they generated text or imagery that could \u201cfalsely appear to a person to be human generated and authentic.\u201d OpenAI said in the White Paper that this amendment would mean their models could \u201cinadvertently\u201d be considered high risk and recommended scrapping the amendment. The company argued that it would be sufficient to instead rely on another part of the Act, that mandates AI providers sufficiently label AI-generated content and be clear to users that they are interacting with an AI system.\n\nThe amendment that OpenAI took issue with was not included in the final text of the AI Act approved by the European Parliament in June. \u201cThey got what they asked for,\u201d says Sarah Chander, a senior policy advisor at European Digital Rights and an expert on the Act, who reviewed the OpenAI White Paper at TIME\u2019s request. The document, she says, \u201cshows that OpenAI, like many Big Tech companies, have used the argument of utility and public benefit of AI to mask their financial interest in watering down the regulation.\u201d\n\nIn a statement to TIME, an OpenAI spokesperson said: \u201cAt the request of policymakers in the E.U., in September 2022 we provided an overview of our approach to deploying systems like GPT-3 safely, and commented on the then-draft of the [AI Act] based on that experience. Since then, the [AI Act] has evolved substantially and we\u2019ve spoken publicly about the technology\u2019s advancing capabilities and adoption. We continue to engage with policymakers and support the E.U.\u2019s goal of ensuring AI tools are built, deployed and used safely now and in the future.\u201d\n\nRead More: Big Tech Is Already Lobbying to Water Down Europe\u2019s AI Rules\n\nIn June 2022, three months before sending over the White Paper, three OpenAI staff members met with European Commission officials for the first time in Brussels. \u201cOpenAI wanted the Commission to clarify the risk framework and know how they could help,\u201d an official record of the meeting kept by the Commission and obtained by freedom of information request states. \u201cThey were concerned that general purpose AI systems would be included as high-risk systems and worried that more systems, by default, would be categorized as high-risk.\u201d The message officials took away from that meeting was that OpenAI\u2014like other Big Tech companies\u2014were afraid of \u201coverregulation\u201d that could impact AI innovation, according to a European Commission source with direct knowledge of the engagement, who asked for anonymity because they were not authorized to speak publicly. OpenAI staffers said in the meeting they were aware of the risks and doing all they could to mitigate them, the source said, but the staffers did not explicitly say that, as a result of their efforts, OpenAI should be subject to less stringent regulations. Nor did they say what type of regulation they would like to see. \u201cOpenAI did not tell us what good regulation should look like,\u201d the person said.\n\nThe White Paper appears to be OpenAI\u2019s way of providing that input. In one section of the document, OpenAI described at length the policies and safety mechanisms that it uses to prevent its generative AI tools from being misused, including prohibiting the generation of images of specific individuals, informing users they are interacting with an AI, and developing tools to detect whether an image is AI-generated. After outlining these measures, OpenAI appeared to suggest that these safety measures should be enough to prevent its systems from being considered \u201chigh risk.\u201d\n\n\u201cWe believe our approach to mitigating risks arising from the general purpose nature of our systems is industry-leading,\u201d the section of the White Paper says. \u201cDespite measures such as those previously outlined, we are concerned that proposed language around general purpose systems may inadvertently result in all our general purpose AI systems being captured [as high risk] by default.\u201d\n\nOne expert who reviewed the OpenAI White Paper at TIME\u2019s request was unimpressed. \u201cWhat they\u2019re saying is basically: trust us to self-regulate,\u201d says Daniel Leufer, a senior policy analyst focused on AI at Access Now\u2019s Brussels office. \u201cIt\u2019s very confusing because they\u2019re talking to politicians saying, \u2018Please regulate us,\u2019 they\u2019re boasting about all the [safety] stuff that they do, but as soon as you say, \u2018Well, let\u2019s take you at your word and set that as a regulatory floor,\u2019 they say no.\u201d\n\nIn other sections of the White Paper, OpenAI argues for amendments to the Act that would allow AI providers to quickly update their systems for safety reasons, without having to undergo a potentially lengthy assessment by E.U. officials first.\n\nThe company also argued for carve-outs that would allow certain uses of generative AI in education and employment, sectors that the first draft of the Act suggested should be considered blanket \u201chigh risk\u201d use cases for AI. OpenAI argued that, for example, the ability of an AI system to draft job descriptions should not be considered a \u201chigh risk\u201d use case, nor the use of an AI in an educational setting to draft exam questions for human curation. After OpenAI shared these concerns last September, an exemption was added to the Act that \u201cvery much meets the wishes of OpenAI to remove from scope systems that do not have a material impact on, or that merely aid in, [human] decision making,\u201d according to Access Now\u2019s Leufer.\n\nOpenAI has continued to engage with European officials working on the AI Act. In a later meeting, on March 31 of this year, OpenAI carried out a demonstration of ChatGPT\u2019s safety features, according to an official record of the meeting kept by the European Commission, obtained via a freedom of information request. An OpenAI staff member explained during the meeting that \u201clearning by operating\u201d\u2014the company\u2019s parlance for releasing AI models into the world and adjusting them based on public usage\u2014\u201cis of great importance.\u201d\n\nOpenAI also told officials during the meeting that \u201cinstructions to AI can be adjusted in such a way that it refuses to share for example information on how to create dangerous substances,\u201d according to the record. This is not always the case. Researchers have demonstrated that ChatGPT can, with the right coaxing, be vulnerable to a type of exploit known as a jailbreak, where specific prompts can cause it to bypass its safety filters and comply with instructions to, for example, write phishing emails or return recipes for dangerous substances."
    },
    {
        "metadata": {
            "title": "Microsoft's $13 billion bet on OpenAI carries huge potential along with plenty of uncertainty - CNBC",
            "description": "Microsoft's $13 billion bet on OpenAI carries huge potential along with plenty of uncertainty  CNBC",
            "published date": "Sat, 08 Apr 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiZmh0dHBzOi8vd3d3LmNuYmMuY29tLzIwMjMvMDQvMDgvbWljcm9zb2Z0cy1jb21wbGV4LWJldC1vbi1vcGVuYWktYnJpbmdzLXBvdGVudGlhbC1hbmQtdW5jZXJ0YWludHkuaHRtbNIBamh0dHBzOi8vd3d3LmNuYmMuY29tL2FtcC8yMDIzLzA0LzA4L21pY3Jvc29mdHMtY29tcGxleC1iZXQtb24tb3BlbmFpLWJyaW5ncy1wb3RlbnRpYWwtYW5kLXVuY2VydGFpbnR5Lmh0bWw?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnbc.com",
                "title": "CNBC"
            }
        },
        "article": "In this article MSFT Follow your favorite stocks CREATE FREE ACCOUNT\n\nMicrosoft CEO Satya Nadella participates in an interview at the company's headquarters in Redmond, Washington, on March 15, 2023. Chona Kasinger | Bloomberg | Getty Images\n\nWhen Microsoft first invested $1 billion in OpenAI in 2019, the deal received no more attention than your average corporate venture round. The startup market was blazing hot, and artificial intelligence was one of many areas attracting mega-valuations, alongside electric vehicles, advanced logistics and aerospace. Three years later, the market looks very different. Startup funding has cratered following the collapse of public market multiples for high-growth, money-losing tech companies. The exception is artificial intelligence, specifically generative AI, which refers to technologies focused on producing automated text, visual and audio responses. No private company is hotter than OpenAI. In November, the San Francisco-based startup introduced ChatGPT, a chatbot that went viral thanks to its ability to craft human-like replies to users' queries about nearly any topic. Microsoft's once under-the-radar investment is now a major topic of discussion, both in venture circles and among public shareholders, who are trying to figure out what it means to the potential value of their stock. Microsoft's cumulative investment in OpenAI has reportedly swelled to $13 billion and the startup's valuation has hit roughly $29 billion. That's because Microsoft isn't just opening up its fat wallet for OpenAI. It's also the arms dealer, as the exclusive provider of computing power for OpenAI's research, products and programming interfaces for developers. Startups and multinational companies, including Microsoft, are rushing to integrate their products with OpenAI, which means massive workloads running on Microsoft's cloud servers.\n\nwatch now\n\nMicrosoft is integrating the technology into its Bing search engine, sales and marketing software, GitHub coding tools, Microsoft 365 productivity bundle and Azure cloud. Michael Turrin, an analyst at Wells Fargo , says it could all add up to over $30 billion in new annual revenue for Microsoft, with roughly half coming from Azure. What does that mean for Microsoft's investment and broader arrangement? \"It's so good that I have investors asking me how they pulled it off, or why OpenAI would even do this,\" Turrin said in an interview. However, the financial implications are anything but straightforward.\n\nBragging rights\n\nOpenAI was founded in 2015 as a nonprofit. The structure changed in 2019, when two top executives published a blog post announcing the formation of a \"capped-profit\" entity called OpenAI LP. The current setup restricts the startup's first investors from making more than 100 times their money, with lower returns for later investors, such as Microsoft. After Microsoft's investment is paid back, it will receive a percentage of OpenAI LP's profits up to the agreed-upon cap, with the rest flowing to the nonprofit body, an OpenAI spokesperson said. A Microsoft spokesperson declined to comment. Greg Brockman, an OpenAI co-founder and one of the blog post's authors, wrote in a 2019 Reddit comment that, for investors, the system \"feels commensurate with what they could make investing in a pretty successful startup (but less than what they'd get investing in the most successful startups of all time!).\" It's an unfamiliar model in Silicon Valley, where maximizing returns has long been the priority of the venture community. Nor does it make much sense to Elon Musk, who was one of OpenAI's founders and early backers. Several times this year, Musk has tweeted his concerns about OpenAI's unconventional structure and its implications for AI, particularly given Microsoft's level of ownership. \"OpenAI was created as an open source (which is why I named it 'Open' AI), non-profit company to serve as a counterweight to Google, but now it has become a closed source, maximum-profit company effectively controlled by Microsoft,\" Musk tweeted in February. \"Not what I intended at all.\" Brockman said on Reddit that if OpenAI succeeds, it could \"create orders of magnitude more value than any company has to date.\" As a major OpenAI investor, Microsoft would benefit. Aside from its investment, leaning on OpenAI has the potential to help Microsoft dramatically reverse its fortunes in AI, where it's stumbled publicly and didn't build a meaningful business on its own. Microsoft pulled the Clippy assistant from Word, Cortana from the Windows taskbar and its Tay chatbot from Twitter. Unlike areas such as advertising or security, Microsoft hasn't disclosed the scale of its AI business, though CEO Satya Nadella said in October that revenue from its Azure Machine Learning service had doubled for four consecutive quarters.\n\nIf nothing else, the work with OpenAI has given Nadella bragging rights. Here's what he said at Microsoft's annual shareholder meeting in December, a month after ChatGPT was launched: \"When I think about Azure, one of the things that we have done, in fact, in the context of even ChatGPT, which today is one of the more popular AI applications out there, guess what? It's all trained on the Azure supercomputer.\" In February, Microsoft held a press event at its headquarters in Redmond, Washington, to announce new AI-powered updates to its Bing search engine and Edge browser. Altman was one of the featured speakers. It's been a bumpy ride since then, as the Bing chatbot has held some highly publicized and creepy conversations with users, and it also served up some incorrect answers at the launch. Somewhat fortunately for Microsoft, Google's rollout of its rival Bard AI service was underwhelming, leading employees to describe it as \"rushed\" and \"botched.\" Despite the early hiccups, the enthusiasm for new technologies based on large language models, or LLMs, is palpable across the tech industry. At the core of OpenAI's bot is an LLM called GPT-4 that's learned to compose natural-sounding text after being trained on extensive online information sources. Microsoft has an exclusive license on GPT-4 and all other OpenAI models, the OpenAI spokesperson said. There are plenty other LLMs available. Last month, Google said it had given some developers early access to an LLM called PaLM. Startups AI21 Labs, Aleph Alpha and Cohere offer their own LLMs, as does Google-backed Anthropic, which has picked Google as its \"preferred\" cloud provider. Like Altman and Musk, Anthropic cofounder Dario Amodei, who was previously vice president of research at OpenAI, has expressed concerns about the unbridled power of AI. In 2021, Anthropic registered in Delaware as a public-benefit corporation, signifying an intention to have a positive impact on society even as it pursues profits. \"We were and are focused on developing innovative structures to provide incentives for safe development and deployment of AI systems and will have more to share on this in the future,\" an Anthropic spokesperson told CNBC in an email. Across the industry, one thing is clear: it's early days. Quinn Slack, CEO of code-search startup Sourcegraph, said he hasn't seen proof that the OpenAI partnership has given Microsoft a notable advantage, even though he called OpenAI the top LLM provider. \"I don't think people should look at Microsoft and say they've totally locked up OpenAI and OpenAI is doing their bidding,\" Slack said. \"I truly believe people there are motivated to build amazing technology and make it as widely used as possible. They view Microsoft as a great customer but not someone that's controlling. That's good, and I hope it stays that way.\" OpenAI has plenty of skeptics. Late last month the nonprofit Center for Artificial Intelligence and Digital Policy called on the Federal Trade Commission to stop OpenAI from releasing new commercial releases of GPT-4, describing the technology as \"biased, deceptive, and a risk to privacy and public safety.\" When considering potential exits for OpenAI, Microsoft \u2014 which does not hold an OpenAI board seat \u2014 would be the natural acquirer given its close entanglement. But that sort of deal would likely attract regulatory scrutiny, because of concerns about AI and about Microsoft stifling competition. By remaining an investor and not becoming OpenAI's owner, Microsoft could avoid Hart-Scott-Rodino reviews from U.S. competition regulators. \"I've gone through it. It's painful,\" said David Zilberman, a partner at Norwest Venture Partners. Based on its existing valuation, the more probable path for OpenAI is an eventual IPO, said Scott Raney, a managing director at Redpoint Ventures. According to PitchBook data, OpenAI is on pace to generate $200 million in revenue this year, up 150% from 2022, and then $1 billion in 2024, which would imply 400% growth. \"When you raise at a $30 billion valuation, it's kind of like, there's no turning back at that point,\" Raney said. You're saying, \"Our plan is to be a big independent standalone company.\" OpenAI's spokesperson said there are no plans to go public or get acquired. WATCH: Why ChatGPT is a game changer for AI"
    },
    {
        "metadata": {
            "title": "Sam Altman, ChatGPT Creator and OpenAI CEO, Urges Senate for AI Regulation - The New York Times",
            "description": "Sam Altman, ChatGPT Creator and OpenAI CEO, Urges Senate for AI Regulation  The New York Times",
            "published date": "Tue, 16 May 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiY2h0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMDUvMTYvdGVjaG5vbG9neS9vcGVuYWktYWx0bWFuLWFydGlmaWNpYWwtaW50ZWxsaWdlbmNlLXJlZ3VsYXRpb24uaHRtbNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "The tone of congressional hearings featuring tech industry executives in recent years can best be described as antagonistic. Mark Zuckerberg, Jeff Bezos and other tech luminaries have all been dressed down on Capitol Hill by lawmakers upset with their companies.\n\nBut on Tuesday, Sam Altman, the chief executive of the San Francisco start-up OpenAI, testified before members of a Senate subcommittee and largely agreed with them on the need to regulate the increasingly powerful A.I. technology being created inside his company and others like Google and Microsoft.\n\nIn his first testimony before Congress, Mr. Altman implored lawmakers to regulate artificial intelligence as members of the committee displayed a budding understanding of the technology. The hearing underscored the deep unease felt by technologists and government over A.I.\u2019s potential harms. But that unease did not extend to Mr. Altman, who had a friendly audience in the members of the subcommittee.\n\nThe appearance of Mr. Altman, a 38-year-old Stanford University dropout and tech entrepreneur, was his christening as the leading figure in A.I. The boyish-looking Mr. Altman traded in his usual pullover sweater and jeans for a blue suit and tie for the three-hour hearing."
    },
    {
        "metadata": {
            "title": "OpenAI brings back Sam Altman as CEO just days after his firing unleashed chaos - The Associated Press",
            "description": "OpenAI brings back Sam Altman as CEO just days after his firing unleashed chaos  The Associated Press",
            "published date": "Wed, 22 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL2FsdG1hbi1vcGVuYWktY2hhdGdwdC0zMTE4N2Y3ZjZlY2E4ZmY5ZDBlZWY3NTg1YWFjNmFjZdIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "The ousted leader of ChatGPT maker OpenAI will return to the company that fired him just days ago, concluding a short but chaotic power struggle that shocked the tech industry and underscored the conflicts around how to safely build artificial intelligence.\n\nThe San Francisco-based company said late Tuesday that it \u201creached an agreement in principle\u201d for co-founder Sam Altman to return as CEO under a different board of directors.\n\nWe have reached an agreement in principle for Sam Altman to return to OpenAI as CEO with a new initial board of Bret Taylor (Chair), Larry Summers, and Adam D'Angelo.\n\n\n\nWe are collaborating to figure out the details. Thank you so much for your patience through this. \u2014 OpenAI (@OpenAI) November 22, 2023\n\nThe agreement followed intense negotiations that began Saturday between Altman\u2019s side and the board members who pushed him out. The discussions included disagreements about Altman\u2019s future role and who would stay on the board, according to a person familiar with the talks who spoke on condition of anonymity because they were not allowed to speak publicly about such sensitive matters.\n\nAn independent investigation into Altman and the events that led to his ouster, announced earlier this week, will continue, according to the person, who described board members\u2019 slow erosion of trust in the OpenAI leader without pointing to any serious wrongdoing. The company previously made unspecified allegations that Altman had not been candid with the board.\n\nThe lack of transparency surrounding Altman\u2019s firing led to a weekend of internal conflict at the company and growing outside pressure from the startup\u2019s investors, particularly Microsoft, which on Monday hired Altman and a key ally, OpenAI co-founder and president Greg Brockman, and opened its doors to any of the other more than 700 employees who wanted to join them.\n\nThe turmoil accentuated the differences between Altman \u2014 who has become the face of generative AI\u2019s rapid commercialization since ChatGPT\u2019s arrival a year ago \u2014 and board members who have expressed deep reservations about the safety risks posed by AI as it gets more advanced.\n\nOne of the four board members who participated in Altman\u2019s ouster, OpenAI co-founder and chief scientist Ilya Sutskever, was involved in the negotiations over the weekend. But that changed when he publicly expressed regret about the decision Monday morning and joined the call for the board\u2019s resignation.\n\nThe person familiar with the talks said board members did not want the company to tank or employees to defect to Microsoft. At the same time, they did not want to acquiesce to demands that they all step down, nor did they want to reinstate Altman and Brockman on the board or install new members who might not stand up to them, the person said.\n\nIn the end, most of them did step down.\n\nThe new board will be led by former Salesforce co-CEO Bret Taylor, who chaired Twitter\u2019s board before Elon Musk took over the platform last year. The other members will be former U.S. Treasury Secretary Larry Summers and Quora CEO Adam D\u2019Angelo, the only member of the previous board to stay on.\n\n\u201cThe OpenAI episode shows how fragile the AI ecosystem is right now, including addressing AI\u2019s risks,\u201d said Johann Laux, an expert at the Oxford Internet Institute focusing on human oversight of artificial intelligence.\n\nFILE - OpenAI CEO Sam Altman participates in a discussion during the Asia-Pacific Economic Cooperation (APEC) CEO Summit, Nov. 16, 2023, in San Francisco. Altman, the ousted leader of ChatGPT-maker OpenAI, is returning to the company that fired him late last week, the latest in a saga that has shocked the artificial intelligence industry. San Francisco-based OpenAI said in a statement late Tuesday, Nov. 21: \u201cWe have reached an agreement in principle for Sam Altman to return to OpenAI as CEO with a new initial board of Bret Taylor (Chair), Larry Summers, and Adam D\u2019Angelo.\u201d (AP Photo/Eric Risberg, File)\n\nBefore the board was replaced, venture capitalist Vinod Khosla, a vocal Altman supporter whose firm is an OpenAI investor, wrote in an opinion column at The Information that board members had set back the \u201ctremendous benefits\u201d of AI by misapplying their \u201creligion of \u2018effective altruism.\u2019\u201d\n\nSome of OpenAI\u2019s board members over the years have had ties to effective altruism, the philanthropic social movement that prioritizes donating to projects that will have the greatest impact on the largest number of people, including humans in the future.\n\nWhile many effective altruists believe AI could offer powerful benefits, they also advocate for mitigating the technology\u2019s potential risks.\n\nHelping to drive Altman\u2019s return and the installation of a new board was Microsoft, which has invested billions of dollars in OpenAI and has rights to its existing technology.\n\nWhile promising to welcome OpenAI\u2019s fleeing workforce, Microsoft CEO Satya Nadella also made clear in a series of interviews Monday that he was open to the possibility of Altman returning to OpenAI as long as the startup\u2019s governance problems were solved.\n\n\u201cWe are encouraged by the changes to the OpenAI board,\u201d Nadella posted on X late Tuesday. \u201cWe believe this is a first essential step on a path to more stable, well-informed and effective governance.\u201d\n\nIn his own post, Altman said that with the new board and with Satya\u2019s support, he was \u201clooking forward to returning to OpenAI and building on our strong partnership\u201d with Microsoft.\n\nGone from the OpenAI board are its only two women: tech entrepreneur Tasha McCauley and Helen Toner, a policy expert at Georgetown\u2019s Center for Security and Emerging Technology, both of whom have expressed concerns about AI safety risks.\n\nThe leadership drama offers a glimpse into how big tech companies are taking the lead in governing AI and its risks, while governments scramble to catch up. The European Union is working to finalize the world\u2019s first comprehensive AI rules.\n\nIn the absence of regulations, \u201ccompanies decide how a technology is rolled out,\u201d said Oxford\u2019s Laux.\n\nCo-founded by Altman as a nonprofit with a mission to safely build AI that outperforms humans and benefits humanity, OpenAI later became a for-profit business \u2014 but one still run by its nonprofit board of directors.\n\nThis was not OpenAI\u2019s first experience with executive turmoil. Past examples including a 2018 falling out between board co-chairs Altman and Musk that led to Musk\u2019s exit, and a later exodus of top leaders who started the competitor Anthropic.\n\nIt\u2019s not clear yet if the board\u2019s structure will change with its new members.\n\nUnder the current structure, all profit beyond a certain cap is supposed to go back to its mission of helping humanity. The board is also tasked with deciding when AI systems have become so advanced that they are better than humans \u201cat most economically valuable work.\u201d At that point, Microsoft\u2019s intellectual property licenses no longer apply.\n\n\u201cWe are collaborating to figure out the details,\u201d OpenAI posted on social media. \u201cThank you so much for your patience through this.\u201d\n\nNadella said Brockman, who was OpenAI\u2019s board chairman until Altman\u2019s firing, also will have a key role to play in ensuring the group \u201ccontinues to thrive and build on its mission.\u201d\n\nAs for OpenAI\u2019s short-lived interim CEO Emmett Shear, the second temporary leader in the days since Altman\u2019s ouster, he posted on X that he was \u201cdeeply pleased by this result\u201d after about 72 \u201cvery intense hours of work.\u201d\n\n\u201cComing into OpenAI, I wasn\u2019t sure what the right path would be,\u201d wrote Shear, the former head of Twitch. \u201cThis was the pathway that maximized safety alongside doing right by all stakeholders involved. I\u2019m glad to have been a part of the solution.\u201d\n\nThe Associated Press and OpenAI have a licensing and technology agreement allowing OpenAI access to part of the AP\u2019s text archives.\n\n___\n\nAssociated Press writers Kelvin Chan in London and Thalia Beaty in New York contributed to this report."
    },
    {
        "metadata": {
            "title": "FTC investigates OpenAI over data leak and ChatGPT's inaccuracy - The Washington Post",
            "description": "FTC investigates OpenAI over data leak and ChatGPT's inaccuracy  The Washington Post",
            "published date": "Thu, 13 Jul 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS90ZWNobm9sb2d5LzIwMjMvMDcvMTMvZnRjLW9wZW5haS1jaGF0Z3B0LXNhbS1hbHRtYW4tbGluYS1raGFuL9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.washingtonpost.com",
                "title": "The Washington Post"
            }
        },
        "article": "The Federal Trade Commission has opened an expansive investigation into OpenAI, probing whether the maker of the popular ChatGPT bot has run afoul of consumer protection laws by putting personal reputations and data at risk. The agency this week sent the San Francisco company a 20-page demand for records about how it addresses risks related to its AI models, according to a document reviewed by The Washington Post. The salvo represents the most potent regulatory threat to date to OpenAI\u2019s business in the United States, as the company goes on a global charm offensive to shape the future of artificial intelligence policy.\n\nAnalysts have called OpenAI\u2019s ChatGPT the fastest-growing consumer app in history, and its early success set off an arms race among Silicon Valley companies to roll out competing chatbots. The company\u2019s chief executive, Sam Altman, has emerged as an influential figure in the debate over AI regulation, testifying on Capitol Hill, dining with lawmakers and meeting with President Biden and Vice President Harris.\n\nBut now the company faces a new test in Washington, where the FTC has issued multiple warnings that existing consumer protection laws apply to AI, even as the administration and Congress struggle to outline new regulations. Senate Majority Leader Charles E. Schumer (D-N.Y.) has predicted that new AI legislation is months away.\n\nAdvertisement\n\nThe FTC\u2019s demands of OpenAI are the first indication of how it intends to enforce those warnings. If the FTC finds that a company violates consumer protection laws, it can levy fines or put a business under a consent decree, which can dictate how the company handles data. The FTC has emerged as the federal government\u2019s top Silicon Valley cop, bringing large fines against Meta, Amazon and Twitter for alleged violations of consumer protection laws.\n\nThe FTC called on OpenAI to provide detailed descriptions of all complaints it had received of its products making \u201cfalse, misleading, disparaging or harmful\u201d statements about people. The FTC is investigating whether the company engaged in unfair or deceptive practices that resulted in \u201creputational harm\u201d to consumers, according to the document.\n\nThe FTC also asked the company to provide records related to a security incident that the company disclosed in March when a bug in its systems allowed some users to see payment-related information, as well as some data from other users\u2019 chat history. The FTC is probing whether the company\u2019s data security practices violate consumer protection laws. OpenAI said in a blog post that the number of users whose data was revealed to someone else was \u201cextremely low.\u201d\n\nAdvertisement\n\nThe FTC declined to comment. OpenAI CEO Sam Altman said in a tweet Thursday evening that the company will \u201cof course\u201d work with the agency.\n\n\u201c[I]t is very disappointing to see the FTC\u2019s request start with a leak and does not help build trust,\u201d he tweeted. \u201c[T]hat said, it\u2019s super important to us that [our] technology is safe and pro-consumer, and we are confident we follow the law.\u201d\n\nAltman also said that the company protects user privacy, and designs its systems \u201cto learn about the world, not private individuals.\u201d\n\nNews of the probe broke shortly before FTC Chair Lina Khan faced a combative hearing Thursday before the House Judiciary Committee, where Republican lawmakers analyzed her enforcement record and accused her of mismanaging the agency. Khan\u2019s ambitious plans to rein in Silicon Valley have suffered key losses in court. On Tuesday, a federal judge rejected the FTC\u2019s attempt to block Microsoft\u2019s $69 billion deal to buy the video game company Activision.\n\nDuring the hearing, Rep. Dan Bishop (R-N.C.) asked Khan what legal authority empowered the FTC to make such demands of a company like OpenAI, as part of a broader line of inquiry into whether Khan\u2019s FTC is overstepping its powers. He noted that libel and defamation are typically prosecuted under state laws, a reference to the FTC\u2019s questions to OpenAI about disparagement of people.\n\nAdvertisement\n\nKhan responded that libel and defamation aren\u2019t a focus of FTC enforcement, but that misuse of people\u2019s private information in AI training could be a form of fraud or deception under the FTC Act. \u201cWe\u2019re focused on, \u2018Is there substantial injury to people?\u2019 Injury can look like all sorts of things,\u201d Khan said.\n\nThe agency has repeatedly warned that action is coming on AI, in speeches, blog posts, op-eds and news conferences. In a speech at Harvard Law School in April, Samuel Levine, the director of the agency\u2019s Bureau of Consumer Protection, said the agency was prepared to be \u201cnimble\u201d in getting ahead of emerging threats.\n\n\u201cThe FTC welcomes innovation, but being innovative is not a license to be reckless,\u201d Levine said. \u201cWe are prepared to use all our tools, including enforcement, to challenge harmful practices in this area.\u201d\n\nAdvertisement\n\nThe FTC also has issued several colorful blog posts about its approach to regulating AI, at times invoking popular science fiction movies to warn the industry against running afoul of the law. The agency has warned against AI scams, using generative AI to manipulate potential customers and falsely exaggerating the capabilities of AI products. Khan also participated in a news conference with Biden administration officials in April about the risk of AI discrimination.\n\n\u201cThere is no AI exemption to the laws on the books,\u201d Khan said at that event.\n\nThe FTC\u2019s push faced swift pushback from the tech industry. Adam Kovacevich, the founder and CEO of the industry coalition Chamber of Progress, said it\u2019s clear that the FTC has oversight of data security and misrepresentation. But he said it\u2019s unclear if the agency has the authority to \u201cpolice defamation or the contents of ChatGPT\u2019s results.\u201d\n\nAdvertisement\n\n\u201cAI is making headlines right now, and the FTC is continuing to put flashy cases over securing results,\u201d he said.\n\nAmong the information the FTC is seeking from Open AI is any research, testing or surveys that assess how well consumers understand \u201cthe accuracy or reliability of outputs\u201d generated by its AI tools. The agency made extensive demands about records related to ways OpenAI\u2019s products could generate disparaging statements, asking the company to provide records of the complaints people send about its chatbot making false statements.\n\nThe agency\u2019s focus on such fabrications comes after numerous high-profile reports of the chatbot producing incorrect information that could damage people\u2019s reputations. Mark Walters, a radio talk show host in Georgia, sued OpenAI for defamation, alleging the chatbot made up legal claims against him. The lawsuit alleges that ChatGPT falsely claimed that Walters, the host of \u201cArmed American Radio,\u201d was accused of defrauding and embezzling funds from the Second Amendment Foundation. The response was provided in response to a question about a lawsuit about the foundation that Walters is not a party to, according to the complaint.\n\nAdvertisement\n\nChatGPT also said that a lawyer had made sexually suggestive comments and attempted to touch a student on a class trip to Alaska, citing an article that it said had appeared in The Washington Post. But no such article existed, the class trip never happened and the lawyer said he was never accused of harassing a student, The Post reported previously.\n\nThe FTC in its request also asked the company to provide extensive details about its products and the way it advertises them. It also demanded details about the policies and procedures that OpenAI takes before it releases any new product to the public, including a list of times that OpenAI held back a large language model because of safety risks.\n\nThe agency also demanded a detailed description of the data that OpenAI uses to train its products, which mimic humanlike speech by ingesting text, mostly scraped from Wikipedia, Scribd and other sites across the open web. The agency also asked OpenAI to describe how it refines its models to address their tendency to \u201challucinate,\u201d making up answers when the models don\u2019t know the answer to a question.\n\nAdvertisement\n\nOpenAI also has to turn over details about how many people were affected by the March security incident and information about all the steps it took to respond.\n\nThe FTC\u2019s records request, which is called a Civil Investigative Demand, primarily focuses on potential consumer protection abuses, but it also asks OpenAI to provide some details about how it licenses its models to other companies.\n\nThe United States has trailed other governments in drafting AI legislation and regulating the privacy risks associated with the technology. Countries within the European Union have taken steps to limit U.S. companies\u2019 chatbots under the bloc\u2019s privacy law, the General Data Protection Regulation. Italy temporarily blocked ChatGPT from operating there due to data privacy concerns, and Google had to postpone the launch of its chatbot Bard after receiving requests for privacy assessments from the Irish Data Protection Commission. The European Union is also expected to pass AI legislation by the end of the year.\n\nAdvertisement\n\nThere is a flurry of activity in Washington to catch up. On Tuesday, Schumer hosted an all-senator briefing with officials from the Pentagon and intelligence community to discuss the national security risks of artificial intelligence, as he works with a bipartisan group of senators to craft new AI legislation. Schumer told reporters after the session that it\u2019s going to be \u201cvery hard\u201d to regulate AI, as lawmakers try to balance the need for innovation with ensuring there are proper safeguards on the technology.\n\nOn Wednesday, Vice President Harris hosted a group of consumer protection advocates and civil liberties leaders at the White House for a discussion on the safety and security risks of AI.\n\n\u201cIt is a false choice to suggest that we either can advance innovation or we protect consumers,\u201d Harris said. \u201cWe can do both.\u201d\n\nWill Oremus contributed to this report."
    },
    {
        "metadata": {
            "title": "Sam Altman rejoins OpenAI board of directors as investigation into his ouster comes to a close - CNBC",
            "description": "Sam Altman rejoins OpenAI board of directors as investigation into his ouster comes to a close  CNBC",
            "published date": "Fri, 08 Mar 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiY2h0dHBzOi8vd3d3LmNuYmMuY29tLzIwMjQvMDMvMDgvc2FtLWFsdG1hbi1yZWpvaW5zLW9wZW5haS1ib2FyZC1jb21wYW55LWFkZHMtdGhyZWUtbmV3LW1lbWJlcnMuaHRtbNIBZ2h0dHBzOi8vd3d3LmNuYmMuY29tL2FtcC8yMDI0LzAzLzA4L3NhbS1hbHRtYW4tcmVqb2lucy1vcGVuYWktYm9hcmQtY29tcGFueS1hZGRzLXRocmVlLW5ldy1tZW1iZXJzLmh0bWw?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnbc.com",
                "title": "CNBC"
            }
        },
        "article": "Sam Altman, CEO of OpenAI, attends the 54th annual meeting of the World Economic Forum, in Davos, Switzerland, on Jan. 18, 2024.\n\nOpenAI on Friday announced its new board and the wrap-up of an internal investigation by U.S. law firm WilmerHale into the events leading up to OpenAI CEO Sam Altman's ouster.\n\nSam Altman will also rejoin OpenAI's board.\n\nThe new board members are:\n\nDr. Sue Desmond-Hellmann, former CEO of the Bill and Melinda Gates Foundation, who is also on the Board of Directors at Pfizer and on the President's Council of Advisors on Science and Technology.\n\nNicole Seligman, former EVP and Global General Counsel of Sony and President of Sony Entertainment, who is also on the Board of Directors at Paramount Global, Meira GTx and Intuitive Machines, Inc.\n\nFidji Simo, CEO and Chair of Instacart, who is also on the Board of Directors at Shopify.\n\nThe three new members will \"work closely with current board members Adam D'Angelo, Larry Summers and Bret Taylor as well as Greg, Sam, and OpenAI's senior management,\" according to a release.\n\nOpenAI will continue to expand the board moving forward, according to a Zoom call with reporters.\n\nOpenAI did not publish the investigation report but provided a summary of the findings.\n\n\"The review concluded there was a significant breakdown of trust between the prior board and Sam and Greg,\" Taylor said, adding that the review also \"concluded the board acted in good faith... [and] did not anticipate some of the instability that led afterwards.\"\n\nTaylor also said the board's concerns did not arise regarding concerns over product safety and security, OpenAI's finances or statements to customers or business partners, that it was \"simply a breakdown in trust between the board and Mr. Altman.\"\n\nWilmerHale's investigation began in December, and the lawyers submitted their report today, which included dozens of interviews with OpenAI's prior board members and advisors, current executives and other witnesses. The investigation also involved reviewing more than 30,000 documents, according to a release.\n\n\"We have unanimously concluded that Sam and Greg are the right leaders for OpenAI,\" Bret Taylor, chair of OpenAI's board, said in a release.\n\n\"I am very grateful to Bret and Larry and WilmerHale,\" Altman said on the Zoom call with reporters. He added, speaking of CTO Mira Murati, \"Mira in particular is incremental to OpenAI all the time ... but through that period in November, she has done an amazing job helping to lead the company.\"\n\nHe added that he is \"excited to be moving forward here\" and for the situation to be \"over.\" He also mentioned he wished he had acted differently regarding differences in opinion with the board.\n\nIn November, OpenAI's board ousted Altman, prompting resignations \u2013 or threats of resignations \u2013 including an open letter signed by virtually all of OpenAI's employees, and uproar from investors, including Microsoft. Within a week, Altman was back at the company, and board members Helen Toner, Tasha McCauley and Ilya Sutskever, who had voted to oust Altman, were out. Adam D'Angelo, who had also voted to oust Altman, stayed on the board.\n\nWhen Altman was asked about Sutskever's status on the Zoom call with reporters, he said there were no updates to share.\n\n\"I love Ilya... I hope we work together for the rest of our careers, my career, whatever,\" Altman said. \"Nothing to announce today.\"\n\nSince then, OpenAI has announced new board members, including former Salesforce co-CEO Bret Taylor and former Treasury Secretary Larry Summers. Microsoft obtained a nonvoting board observer position.\n\nAfter ChatGPT's launch in November 2022, it broke records at the time as the fastest-growing consumer app in history, and now has about 100 million weekly active users, along with more than 92% of Fortune 500 companies using the platform, according to OpenAI. Last year, Microsoft invested an additional $10 billion in the company, making it the biggest AI investment of the year, according to PitchBook, and OpenAI has reportedly closed a deal that will allow employees to sell shares at an $86 billion valuation, though the deal reportedly took longer to close than expected due to the events surrounding Altman's ouster.\n\nThe rollercoaster couple of weeks at the company are still affecting it months later.\n\nThis month, billionaire tech magnate Elon Musk sued OpenAI co-founders Sam Altman and Greg Brockman for breach of contract and breach of fiduciary duty, court filings revealed on Thursday.\n\nIn his complaint, Musk and his attorneys allege that the ChatGPT maker \"has been transformed into a closed-source de facto subsidiary of the largest technology company in the world: Microsoft.\" They also argue that this arrangement goes against a founding agreement and 2015 certification of incorporation that OpenAI established with Musk, who was a pivotal donor to a cofounder of OpenAI in its early years.\n\nAs part of Microsoft's contract with OpenAI, the tech giant only has rights to OpenAI's \"pre-AGI\" technology, and it is up to OpenAI's board to determine whether the company has reached that milestone. Musk argued in his filing that since the OpenAI board shuffle in November \u2013 when Toner, McCauley and Sutskever were removed \u2013 the new board is \"ill-equipped\" to independently determine whether OpenAI has reached AGI and therefore whether its technology is outside the scope of the exclusivity deal with Microsoft.\n\nLawyers told CNBC that they had doubts about the legal viability of Musk's case, and OpenAI has said it plans to file a motion to dismiss all of Musk's claims.\n\nIn response to the high-profile lawsuit, OpenAI reproduced old emails from Musk in which the Tesla and SpaceX CEO encouraged the rising startup to raise at least $1 billion in funding, and agreed that it should \"start being less open\" over time and \"not share\" the company's science with the public.\n\nMusk's lawsuit also follows some controversy over Altman's previous chip endeavors and investments.\n\nJust before Altman's brief ouster, he was reportedly seeking billions for a new and not-yet-formed chip venture code-named \"Tigris\" to eventually compete with Nvidia, traveling to the Middle East to raise money from investors.\n\nIn 2018, Altman personally invested in an AI chip startup called Rain Neuromorphics, based near OpenAI's San Francisco headquarters, and in 2019, OpenAI signed a letter of intent to spend $51 million on Rain's chips. In December, the U.S. compelled a Saudi Aramco-backed venture capital firm to sell its shares in Rain."
    },
    {
        "metadata": {
            "title": "OpenAI publishes Elon Musk\u2019s emails. \u2018We\u2019re sad that it\u2019s come to this\u2019 - CNN",
            "description": "OpenAI publishes Elon Musk\u2019s emails. \u2018We\u2019re sad that it\u2019s come to this\u2019  CNN",
            "published date": "Wed, 06 Mar 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiRmh0dHBzOi8vd3d3LmNubi5jb20vMjAyNC8wMy8wNi90ZWNoL29wZW5haS1lbG9uLW11c2stZW1haWxzL2luZGV4Lmh0bWzSAT9odHRwczovL2FtcC5jbm4uY29tL2Nubi8yMDI0LzAzLzA2L3RlY2gvb3BlbmFpLWVsb24tbXVzay1lbWFpbHM?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnn.com",
                "title": "CNN"
            }
        },
        "article": "New York CNN \u2014\n\nOpenAI fired back at Elon Musk, who sued the ChatGPT company last week for chasing profit and diverging from its original, nonprofit mission. Tuesday night, OpenAI published several of Musk\u2019s emails from the early days of the company that appear to show Musk acknowledging OpenAI needed to make a ton of money to fund the incredible computing resources needed to power its AI ambitions.\n\nIn the emails, parts of which have been redacted, Musk argues that the company stood virtually no chance of building a successful generative AI platform by raising cash alone, and the company needed to find alternate sources of revenue to survive.\n\nIn a November 22, 2015, email to CEO Sam Altman, Musk, an OpenAI co-founder, said the company needed to raise much more than $100 million to \u201cavoid sounding hopeless.\u201d Musk suggested a $1 billion funding commitment and promised that he would cover whatever did not get raised.\n\nOpenAI in a blog post Tuesday night said Musk never followed through on his promise, committing $45 million in funding for OpenAI, while other donors raised $90 million. Lawyers for Musk declined to comment on OpenAI\u2019s claims.\n\nMusk, in a February 1, 2018, email, told company executives that the only path forward for OpenAI was for Tesla, his electric car company, to buy it. The company refused, and Musk left OpenAI later that year.\n\nIn December 2018, Musk emailed Altman and other executives that OpenAI would not be relevant \u201cwithout a dramatic change in execution and resources.\u201d\n\n\u201cThis needs billions per year immediately or forget it,\u201d Musk emailed. \u201cI really hope I\u2019m wrong.\u201d\n\nOpenAI executives agreed. In 2019, they formed OpenAI LP, a for-profit entity that exists within the larger company\u2019s structure. That for-profit company took OpenAI from effectively worthless to a valuation of $90 billion in just a few years \u2014 and Altman is largely credited as the mastermind of that plan and the key to the company\u2019s success.\n\nMicrosoft has since committed $13 billion in a close partnership with OpenAI.\n\nMusk\u2019s complaint, filed last week in California state court, said that company and its partnership with Microsoft violated OpenAI\u2019s founding charter, representing a breach of contract. Musk is asking for a jury trial and for the company, Altman and co-founder Greg Brockman to pay back the profit they received from the business.\n\nOpenAI was founded as a check on what the founders believed is a serious threat that artificial generative intelligence, or AGI, posed to humanity. The company created a board of overseers to review any product the company created, and its products\u2019 code was made public.\n\nThe company said in its blog post that it has not diverged from its mission, and it would move to dismiss all of Musk\u2019s claims. It said its technology is broadly available and improves people\u2019s lives, while the company continues to commit to the safety of its products.\n\n\u201cWe\u2019re sad that it\u2019s come to this with someone whom we\u2019ve deeply admired\u2014someone who inspired us to aim higher, then told us we would fail, started a competitor, and then sued us when we started making meaningful progress towards OpenAI\u2019s mission without him,\u201d the company said in its blog post."
    },
    {
        "metadata": {
            "title": "Opinion | Sam Altman Is Back at OpenAI. I Have a Question for Him. - The New York Times",
            "description": "Opinion | Sam Altman Is Back at OpenAI. I Have a Question for Him.  The New York Times",
            "published date": "Thu, 23 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiQWh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMTEvMjMvb3Bpbmlvbi9zYW0tYWx0bWFuLW9wZW5haS5odG1s0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "One of the nice things about OpenAI is that it was built on distrust. It began as a nonprofit research lab because its founders didn\u2019t think artificial intelligence should be pioneered by commercial firms, which are driven overwhelmingly by the profit motive.\n\nAs it evolved, OpenAI turned into what you might call a fruitful contradiction: a for-profit company overseen by a nonprofit board with a corporate culture somewhere in between.\n\nMany of the people at the company seem simultaneously motivated by the scientist\u2019s desire to discover, the capitalist\u2019s desire to ship product and the do-gooder\u2019s desire to do this all safely.\n\nThe events of the past week \u2014 Sam Altman\u2019s firing, all the drama, his rehiring \u2014 revolve around one central question: Is this fruitful contradiction sustainable?"
    },
    {
        "metadata": {
            "title": "OpenAI braces for copyright lawsuits with New York Times, authors - The Associated Press",
            "description": "OpenAI braces for copyright lawsuits with New York Times, authors  The Associated Press",
            "published date": "Tue, 09 Jan 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMibWh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL29wZW5haS1uZXcteW9yay10aW1lcy1jaGF0Z3B0LWxhd3N1aXQtZ3Jpc2hhbS1ueXQtNjlmNzhjNDA0YWNlNDJjMDA3MGZkZmI5ZGQ0Y2FlYjfSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "A barrage of high-profile lawsuits in a New York federal court will test the future of ChatGPT and other artificial intelligence products that wouldn\u2019t be so eloquent had they not ingested huge troves of copyrighted human works.\n\nBut are AI chatbots \u2014 in this case, widely commercialized products made by OpenAI and its business partner Microsoft \u2014 breaking copyright and fair competition laws? Professional writers and media outlets will face a difficult fight to win that argument in court.\n\n\u201cI would like to be optimistic on behalf of the authors, but I\u2019m not. I just think they have an uphill battle here,\u201d said copyright attorney Ashima Aggarwal, who used to work for academic publishing giant John Wiley & Sons.\n\nOne lawsuit comes from The New York Times. Another from a group of well-known novelists such as John Grisham, Jodi Picoult and George R.R. Martin. A third from bestselling nonfiction writers, including an author of the Pulitzer Prize-winning biography on which the hit movie \u201cOppenheimer\u201d was based.\n\nTHE LAWSUITS\n\nEach of the lawsuits makes different allegations, but they all center on the San Francisco-based company OpenAI \u201cbuilding this product on the back of other peoples\u2019 intellectual property,\u201d said attorney Justin Nelson, who is representing the nonfiction writers and whose law firm is also representing The Times.\n\n\u201cWhat OpenAI is saying is that they have a free ride to take anybody else\u2019s intellectual property really since the dawn of time, as long as it\u2019s been on the internet,\u201d Nelson said.\n\nThe Times sued in December, arguing that ChatGPT and Microsoft\u2019s Copilot are competing with the same outlets they are trained on and diverting web traffic away from the newspaper and other copyright holders who depend on advertising revenue generated from their sites to keep producing their journalism. It also provided evidence of the chatbots spitting out Times articles word-for-word. At other times the chatbots falsely attributed misinformation to the paper in a way it said damaged its reputation.\n\nOne senior federal judge is so far presiding over all three cases, as well as a fourth from two more nonfiction authors who filed another lawsuit last week. U.S. District Judge Sidney H. Stein has been at the Manhattan-based court since 1995 when he was nominated by then-President Bill Clinton.\n\nTHE RESPONSE\n\nOpenAI and Microsoft haven\u2019t yet filed formal counter-arguments on the New York cases, but OpenAI made a public statement this week describing The Times lawsuit as \u201cwithout merit\u201d and saying that the chatbot\u2019s ability to regurgitate some articles verbatim was a \u201crare bug.\u201d\n\n\u201cTraining AI models using publicly available internet materials is fair use, as supported by long-standing and widely accepted precedents,\u201d said a Monday blog post from the company. It went on to suggest that The Times \u201ceither instructed the model to regurgitate or cherry-picked their examples from many attempts.\u201d\n\nOpenAI cited licensing agreements made last year with The Associated Press, the German media company Axel Springer and other organizations as offering a glimpse into how the company is trying to support a healthy news ecosystem. OpenAI is paying an undisclosed fee to license AP\u2019s archive of news stories. The New York Times was engaged in similar talks before deciding to sue.\n\nOpenAI said earlier this year that access to AP\u2019s \u201chigh-quality, factual text archive\u201d would improve the capabilities of its AI systems. But its blog post this week downplayed the importance of news content for AI training, arguing that large language models learn from an \u201cenormous aggregate of human knowledge\u201d and that \u201cany single data source \u2014 including The New York Times \u2014 is not significant for the model\u2019s intended learning.\u201d\n\nWHO\u2019S GOING TO WIN?\n\nMuch of the AI industry\u2019s argument rests on the \u201cfair use\u201d doctrine of U.S. copyright law that allows for limited uses of copyrighted materials such as for teaching, research or transforming the copyrighted work into something different.\n\nIn response, the legal team representing The Times wrote Tuesday that what OpenAI and Microsoft are doing is \u201cnot fair use by any measure\u201d because they\u2019re taking from the newspaper\u2019s investment in its journalism \u201cto build substitutive products without permission or payment.\u201d\n\nSo far, courts have largely sided with tech companies in interpreting how copyright laws should treat AI systems. In a defeat for visual artists, a federal judge in San Francisco last year dismissed much of the first big lawsuit against AI image-generators, though artists have since amended their complaint. Another California judge shot down part of comedian Sarah Silverman\u2019s arguments against Facebook parent Meta but her case was amended in December and joined with another one that includes writers Ta-Nehisi Coates and Michael Chabon.\n\nThe most recent lawsuits have brought more detailed evidence of alleged harms, but Aggarwal said when it comes to using copyrighted content to train AI systems that deliver a \u201csmall portion of that to users, the courts just don\u2019t seem inclined to find that to be copyright infringement.\u201d\n\nTech companies cite as precedent Google\u2019s success in beating back legal challenges to its online book library. The U.S. Supreme Court in 2016 let stand lower court rulings that rejected authors\u2019 claim that Google\u2019s digitizing of millions of books and showing snippets of them to the public amounted to copyright infringement.\n\nBut judges interpret fair use arguments on a case-by-case basis and it is \u201cactually very fact-dependent,\u201d depending on economic impact and other factors, said Cathy Wolfe, an executive at the Dutch firm Wolters Kluwer who also sits on the board of the Copyright Clearance Center, which helps negotiate print and digital media licenses in the U.S.\n\n\u201cJust because something is free on the internet, on a website, doesn\u2019t mean you can copy it and email it, let alone use it to conduct commercial business,\u201d Wolfe said. \u201cWho\u2019s going to win, I don\u2019t know, but I\u2019m certainly a proponent for protecting copyright for all of us. It drives innovation.\u201d\n\nBEYOND THE COURTS\n\nSome media outlets and other content creators are looking beyond the courts and calling for lawmakers or the U.S. Copyright Office to strengthen copyright protections for the AI era. A panel of the U.S. Senate Judiciary Committee heard testimony Wednesday from media executives and advocates in a hearing dedicated to AI\u2019s effect on journalism.\n\nRoger Lynch, chief executive of the Conde Nast magazine chain, planned to tell senators that generative AI companies \u201care using our stolen intellectual property to build tools of replacement.\u201d\n\n\u201cWe believe that a legislative fix can be simple \u2014 clarifying that the use of copyrighted content in conjunction with commercial Gen AI is not fair use and requires a license,\u201d says a copy of Lynch\u2019s prepared remarks.\n\n___\n\nThis story was first published on January 9, 2024. It was updated on January 10, 2024 to make clear that a lawsuit brought by artists against AI image-generators and another lawsuit against Meta brought by authors, including Sarah Silverman, have been amended after judges dismissed parts of each case."
    },
    {
        "metadata": {
            "title": "Inside OpenAI's Crisis Over the Future of Artificial Intelligence - The New York Times",
            "description": "Inside OpenAI's Crisis Over the Future of Artificial Intelligence  The New York Times",
            "published date": "Sat, 09 Dec 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMTIvMDkvdGVjaG5vbG9neS9vcGVuYWktYWx0bWFuLWluc2lkZS1jcmlzaXMuaHRtbNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "Around noon on Nov. 17, Sam Altman, the chief executive of OpenAI, logged into a video call from a luxury hotel in Las Vegas. He was in the city for its inaugural Formula 1 race, which had drawn 315,000 visitors including Rihanna and Kylie Minogue.\n\nMr. Altman, who had parlayed the success of OpenAI\u2019s ChatGPT chatbot into personal stardom beyond the tech world, had a meeting lined up that day with Ilya Sutskever, the chief scientist of the artificial intelligence start-up. But when the call started, Mr. Altman saw that Dr. Sutskever was not alone \u2014 he was virtually flanked by OpenAI\u2019s three independent board members.\n\nInstantly, Mr. Altman knew something was wrong.\n\nUnbeknownst to Mr. Altman, Dr. Sutskever and the three board members had been whispering behind his back for months. They believed Mr. Altman had been dishonest and should no longer lead a company that was driving the A.I. race. On a hush-hush 15-minute video call the previous afternoon, the board members had voted one by one to push Mr. Altman out of OpenAI.\n\nNow they were delivering the news. Shocked that he was being fired from a start-up he had helped found, Mr. Altman widened his eyes and then asked, \u201cHow can I help?\u201d The board members urged him to support an interim chief executive. He assured them that he would."
    },
    {
        "metadata": {
            "title": "Sam Altman joins Microsoft as OpenAI names its third CEO in 3 days - CNN",
            "description": "Sam Altman joins Microsoft as OpenAI names its third CEO in 3 days  CNN",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiSWh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8xMS8yMC90ZWNoL3NhbS1hbHRtYW4tam9pbnMtbWljcm9zb2Z0L2luZGV4Lmh0bWzSAUJodHRwczovL2FtcC5jbm4uY29tL2Nubi8yMDIzLzExLzIwL3RlY2gvc2FtLWFsdG1hbi1qb2lucy1taWNyb3NvZnQ?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnn.com",
                "title": "CNN"
            }
        },
        "article": "New Delhi/New York CNN \u2014\n\nMicrosoft has hired Sam Altman to power up its innovation in artificial intelligence after the co-founder of OpenAI was ousted as CEO in a chaotic boardroom coup on Friday. Meanwhile, the ChatGPT company will get its third CEO in three days.\n\nIt\u2019s another major shakeup to the balance of power over artificial intelligence, the most significant new technology in decades.\n\nGreg Brockman, another co-founder of OpenAI, is also joining Microsoft (MSFT) \u2014 the startup\u2019s biggest financial backer. Brockmann quit as OpenAI president after Altman was fired.\n\nEmmett Shear, the former CEO of Amazon\u2019s streaming service Twitch, will join OpenAI as interim CEO. He replaces Mira Murati, who was named interim CEO when Altman was fired. She will return to her role as OpenAI\u2019s chief technology officer.\n\n\u201cWe look forward to getting to know Emmett Shear,\u201d Microsoft CEO Satya Nadella said in a post on X, formerly known as Twitter. \u201cAnd we\u2019re extremely excited to share the news that Sam Altman and Greg Brockman, together with colleagues, will be joining Microsoft to lead a new advanced AI research team.\u201d\n\nA win for Microsoft\n\nWith its $13 billion investment, Microsoft is OpenAI\u2019s biggest stakeholder. Altman will be the CEO of the \u201cnew group,\u201d Nadella said in his post. Shares of Microsoft rose about 2% in premarket trading Monday.\n\nAltman\u2019s move to Microsoft brought to an end a weekend of feverish speculation that the OpenAI board could perform a dramatic U-turn and rehire the high-flying Silicon Valley entrepreneur and investor.\n\nAltman spent Sunday at OpenAI\u2019s headquarters, posting on X a photo of himself holding a green guest badge connected to a lanyard labeled \u201cOpenAI.\u201d He wrote: \u201cfirst and last time i ever wear one of these.\u201d Multiple news outlets, including the Wall Street Journal and New York Times, reported that the board that fired Altman had been having second thoughts and engaged with him and Brockman to discuss their return.\n\nWith the Monday announcement that Altman would be joining Microsoft, that speculation appeared to have been put to bed \u2014 at least for now.\n\nIn an open letter Monday morning, hundreds of OpenAI employees called for the resignation of OpenAI\u2019s board, accusing it of mishandling Altman\u2019s firing, and threatened to quit and move to Microsoft along with Altman.\n\nIn the wake of the announcement of his move to Microsoft on Monday, Altman posted on X, saying, \u201cwe have more unity and commitment and focus than ever before. we are all going to work together some way or other, and i\u2019m so excited. one team, one mission.\u201d\n\nHe also praised OpenAI\u2019s leadership team, including Murati. \u201c[They] have been doing an incredible job through this that will be in the history books,\u201d Altman said on X.\n\nLater on Monday, Altman said that for him and Nadella, the \u201ctop priority remains to ensure [OpenAI] continues to thrive. we are committed to fully providing continuity of operations to our partners and customers.\u201d\n\nThe new face(s) of AI\n\nIn a post on X early Monday, Shear described the chance to join OpenAI as \u201ca once-in-a-lifetime\u201d opportunity.\n\n\u201cI took this job because I believe that OpenAI is one of the most important companies currently in existence. When the board shared the situation and asked me to take the role, I did not make the decision lightly,\u201d he added.\n\nShear had left his role as CEO of Twitch in March. In his X post, Shear said it took him just a \u201cfew hours\u201d of reflection to make up his mind to join OpenAI.\n\n\u201cUltimately I felt that I had a duty to help if I could,\u201d Shear said.\n\nBut Shear also noted that he\u2019s taking over a company with a severely damaged reputation after the bungled firing of Altman and the whirlwind of a weekend in which it flirted with a return of its just-ousted CEO. Shear said that process was \u201chandled very badly, which has seriously damaged our trust.\u201d\n\nHe said the company would hire an independent investigator to report on what happened in the lead-up to Altman\u2019s firing. Shear did not go into details about why Altman was given the boot, but he said that it was not related to concerns about Altman leading the company in an unsafe direction or opposition to his efforts to make money.\n\n\u201cThe board did *not* remove Sam over any specific disagreement on safety, their reasoning was completely different from that,\u201d Shear said. \u201cI\u2019m not crazy enough to take this job without board support for commercializing our awesome models.\u201d\n\nBased on the results of the probe, and his discussions with other stakeholders, Shear said he would make \u201csignificant\u201d changes to OpenAI in the coming month.\n\n\u201cOpenAI\u2019s stability and success are too important to allow turmoil to disrupt them like this,\u201d he said.\n\nAs CEO, he\u2019ll have to work with Altman, Brockman and other former OpenAI employees who quit and will make the move to Microsoft, which is the startup\u2019s biggest strategic partner. He\u2019ll also have to work with Murati, who praised Altman on his way out, and who \u2014 like many OpenAI employees, including some who quit in protest at his dismissal \u2013 posted on X Monday that \u201cOpenAI is nothing without its people.\u201d\n\nAltman has remained mostly quiet about his firing and hiring over the weekend. On Monday morning, responding to Nadella\u2019s X post, he said, \u201cthe mission continues.\u201d\n\n\u201cI\u2019m super excited to have you join as CEO of this new group, Sam, setting a new pace for innovation,\u201d Nadella posted in response. \u201cWe\u2019ve learned a lot over the years about how to give founders and innovators space to build independent identities and cultures within Microsoft, including GitHub, Mojang Studios, and LinkedIn, and I\u2019m looking forward to having you do the same.\u201d\n\nMoving too fast?\n\nThe details of Altman\u2019s firing remain murky. In its announcement Friday, OpenAI claimed Altman had been insufficiently \u201ccandid\u201d with the board, and that had hindered the board\u2019s ability to carry out its responsibilities.\n\nThat ambiguous language sent the rumor mill flying. But Brockman gave vivid first-hand details in a post on X.\n\nHe said Altman had found out he was being fired just minutes before the company made the news public. Brockman suggested Altman had been fired because of a disagreement with the company\u2019s research division, led by another co-founder and chief scientist Ilya Sutskever.\n\nA key factor was tension between Altman, who favored developing AI more aggressively, and members of the OpenAI board, who wanted to move more cautiously, according to CNN contributor Kara Swisher, who spoke to sources knowledgeable about the crisis.\n\nAltman had been privately pushing the company to bring products to market more quickly and to sell them for a profit. In public, however, Altman has long cautioned about risks posed by AI.\n\n\u201cIs [AI] gonna be like the printing press that diffused knowledge, power, and learning widely across the landscape that empowered ordinary, everyday individuals that led to greater flourishing, that led above all to greater liberty?\u201d he said in a May Senate subcommittee hearing pressing for regulation. \u201cOr is it gonna be more like the atom bomb\u2026?\u201d\n\nAt the same time, Altman had OpenAI place its foot firmly on the gas pedal.\n\nThe startup\u2019s executives and iPhone designer Jony Ive had reportedly held talks to raise $1 billion from Japan\u2019s SoftBank for an AI device to replace the smartphone. And OpenAI had won a multibillion-dollar investment commitment from Microsoft as part of a partnership that included rapid deployment of ChatGPT-like technology across Microsoft\u2019s search engine Bing and other products.\n\nMore recently, Altman announced that OpenAI would make its tools widely available so anyone could create their own version of ChatGPT.\n\nMicrosoft was not informed of Altman\u2019s firing until \u201cjust before\u201d the public announcement, Swisher said, and employees were not given any advance warning.\n\n\u2013CNN\u2019s Clare Duffy contributed to this report."
    },
    {
        "metadata": {
            "title": "The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work - The New York Times",
            "description": "The Times Sues OpenAI and Microsoft Over A.I. Use of Copyrighted Work  The New York Times",
            "published date": "Wed, 27 Dec 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMTIvMjcvYnVzaW5lc3MvbWVkaWEvbmV3LXlvcmstdGltZXMtb3Blbi1haS1taWNyb3NvZnQtbGF3c3VpdC5odG1s0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "The New York Times sued OpenAI and Microsoft for copyright infringement on Wednesday, opening a new front in the increasingly intense legal battle over the unauthorized use of published work to train artificial intelligence technologies.\n\nThe Times is the first major American media organization to sue the companies, the creators of ChatGPT and other popular A.I. platforms, over copyright issues associated with its written works. The lawsuit, filed in Federal District Court in Manhattan, contends that millions of articles published by The Times were used to train automated chatbots that now compete with the news outlet as a source of reliable information.\n\nThe suit does not include an exact monetary demand. But it says the defendants should be held responsible for \u201cbillions of dollars in statutory and actual damages\u201d related to the \u201cunlawful copying and use of The Times\u2019s uniquely valuable works.\u201d It also calls for the companies to destroy any chatbot models and training data that use copyrighted material from The Times.\n\nIn its complaint, The Times said it approached Microsoft and OpenAI in April to raise concerns about the use of its intellectual property and explore \u201can amicable resolution,\u201d possibly involving a commercial agreement and \u201ctechnological guardrails\u201d around generative A.I. products. But it said the talks had not produced a resolution."
    },
    {
        "metadata": {
            "title": "What OpenAI Really Wants - WIRED",
            "description": "What OpenAI Really Wants  WIRED",
            "published date": "Tue, 05 Sep 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiNWh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS93aGF0LW9wZW5haS1yZWFsbHktd2FudHMv0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.wired.com",
                "title": "WIRED"
            }
        },
        "article": "The air crackles with an almost Beatlemaniac energy as the star and his entourage tumble into a waiting Mercedes van. They\u2019ve just ducked out of one event and are headed to another, then another, where a frenzied mob awaits. As they careen through the streets of London\u2014the short hop from Holborn to Bloomsbury\u2014it\u2019s as if they\u2019re surfing one of civilization\u2019s before-and-after moments. The history-making force personified inside this car has captured the attention of the world. Everyone wants a piece of it, from the students who\u2019ve waited in line to the prime minister.\n\nInside the luxury van, wolfing down a salad, is the neatly coiffed 38-year-old entrepreneur Sam Altman, cofounder of OpenAI; a PR person; a security specialist; and me. Altman is unhappily sporting a blue suit with a tieless pink dress shirt as he whirlwinds through London as part of a monthlong global jaunt through 25 cities on six continents. As he gobbles his greens\u2014no time for a sit-down lunch today\u2014he reflects on his meeting the previous night with French president Emmanuel Macron. Pretty good guy! And very interested in artificial intelligence.\n\nAs was the prime minister of Poland. And the prime minister of Spain.\n\nRiding with Altman, I can almost hear the ringing, ambiguous chord that opens \u201cA Hard Day\u2019s Night\u201d\u2014introducing the future. Last November, when OpenAI let loose its monster hit, ChatGPT, it triggered a tech explosion not seen since the internet burst into our lives. Suddenly the Turing test was history, search engines were endangered species, and no college essay could ever be trusted. No job was safe. No scientific problem was immutable.\n\nAltman didn\u2019t do the research, train the neural net, or code the interface of ChatGPT and its more precocious sibling, GPT-4. But as CEO\u2014and a dreamer/doer type who\u2019s like a younger version of his cofounder Elon Musk, without the baggage\u2014one news article after another has used his photo as the visual symbol of humanity\u2019s new challenge. At least those that haven\u2019t led with an eye-popping image generated by OpenAI\u2019s visual AI product, Dall-E. He is the oracle of the moment, the figure that people want to consult first on how AI might usher in a golden age, or consign humans to irrelevance, or worse.\n\nAltman\u2019s van whisks him to four appearances that sunny day in May. The first is stealthy, an off-the-record session with the Round Table, a group of government, academia, and industry types. Organized at the last minute, it\u2019s on the second floor of a pub called the Somers Town Coffee House. Under a glowering portrait of brewmaster Charles Wells (1842\u20131914), Altman fields the same questions he gets from almost every audience. Will AI kill us? Can it be regulated? What about China? He answers every one in detail, while stealing glances at his phone. After that, he does a fireside chat at the posh Londoner Hotel in front of 600 members of the Oxford Guild. From there it\u2019s on to a basement conference room where he answers more technical questions from about 100 entrepreneurs and engineers. Now he\u2019s almost late to a mid-afternoon onstage talk at University College London. He and his group pull up at a loading zone and are ushered through a series of winding corridors, like the Steadicam shot in Goodfellas. As we walk, the moderator hurriedly tells Altman what he\u2019ll ask. When Altman pops on stage, the auditorium\u2014packed with rapturous academics, geeks, and journalists\u2014erupts."
    },
    {
        "metadata": {
            "title": "OpenAI Hit With First Defamation Suit Over ChatGPT Hallucination - Bloomberg Law",
            "description": "OpenAI Hit With First Defamation Suit Over ChatGPT Hallucination  Bloomberg Law",
            "published date": "Wed, 07 Jun 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMic2h0dHBzOi8vbmV3cy5ibG9vbWJlcmdsYXcuY29tL3RlY2gtYW5kLXRlbGVjb20tbGF3L29wZW5haS1oaXQtd2l0aC1maXJzdC1kZWZhbWF0aW9uLXN1aXQtb3Zlci1jaGF0Z3B0LWhhbGx1Y2luYXRpb27SAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://news.bloomberglaw.com",
                "title": "Bloomberg Law"
            }
        },
        "article": "OpenAI LLC is facing a defamation lawsuit from a Georgia radio host who claimed the viral artificial intelligence program ChatGPT generated a false legal complaint accusing him of embezzling money.\n\nThe first-of-its-kind case comes as generative AI programs face heightened scrutiny over their ability to spread misinformation and \u201challucinate\u201d false outputs, including fake legal precedent.\n\nMark Walters said in his Georgia state court suit that the chatbot provided the false complaint to Fred Riehl, the editor-in-chief of the gun publication AmmoLand, who was reporting on a real life legal case playing out in Washington state.\n\nRiehl asked ChatGPT to provide a summary of Second Amendment Foundation v. Ferguson, a case in Washington federal court accusing the state\u2019s Attorney General Bob Ferguson of abusing his power by chilling the activities of the gun rights foundation.\n\nHowever, ChatGPT allegedly provided a summary of the case to Riehl that said the Second Amendment Foundation\u2019s founder Alan Gottlieb was suing Walters for \u201cdefrauding and embezzling funds\u201d from the foundation as chief financial officer and treasurer.\n\n\u201cEvery statement of fact in the summary pertaining to Walters is false,\u201d according to the defamation suit, filed on June 5.\n\nOpenAI didn\u2019t immediately return a request for comment.\n\nWalters, the host of Armed America Radio, isn\u2019t a party to the Ferguson case and has never been employed by the Second Amendment Foundation, the lawsuit said. The Second Amendment Foundation\u2019s case \u201chas nothing at all to do with financial accounting claims against anyone.\u201d\n\nThe truth and reliability of AI chatbot outputs has sparked numerous controversies recently, as researchers and users uncover hallucinations\u2014confident chatbot responses that are untrue.\n\nAn Australian mayor made headlines in April when he said he was preparing to sue OpenAI over ChatGPT outputs falsely claiming that he was imprisoned for bribery. A New York lawyer who used ChatGPT to draft legal briefs could face sanctions after he cited case law that never existed.\n\nRiehl asked ChatGPT to provide the entire text of the Second Amendment Foundation\u2019s complaint, and the chatbot allegedly generated \u201ca complete fabrication\u201d that \u201cbears no resemblance to the actual complaint, including an erroneous case number.\u201d\n\n\u201cChatGPT\u2019s allegations concerning Walters were false and malicious, expressed in print, writing, pictures, or signs, tending to injure Walter\u2019s reputation and exposing him to public hatred, contempt, or ridicule,\u201d the lawsuit said.\n\nJohn Monroe Law PC represents Walters.\n\nThe case is Walters v. OpenAI LLC, Ga. Super. Ct., No. 23-A-04860-2, complaint filed 6/5/23."
    },
    {
        "metadata": {
            "title": "Company that created ChatGPT is thrown into turmoil after Microsoft hires its ousted CEO - The Associated Press",
            "description": "Company that created ChatGPT is thrown into turmoil after Microsoft hires its ousted CEO  The Associated Press",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiYmh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL2FsdG1hbi1haS1jaGF0Z3B0LWxlYWRlcnNoaXAtbWljcm9zb2Z0LWExMTBiMTczYzNlZmY0YTM3NDk5MjAxN2YwNWNkNDVh0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "The company that created ChatGPT was thrown into turmoil Monday after Microsoft hired its ousted CEO and many employees threatened to follow him in a conflict that centered in part on how to build artificial intelligence that\u2019s smarter than humans.\n\nThe developments followed a weekend of drama that shocked the AI field and fueled speculation about the future of OpenAI, which named a new chief executive on Friday and then replaced her on Sunday. The newest CEO vowed to investigate the firing of co-founder and CEO Sam Altman, who\u2019s been instrumental in OpenAI\u2019s transformation from a nonprofit research laboratory into a world-renowned commercial startup that inaugurated the era of generative artificial intelligence.\n\nMicrosoft, which has been a close partner of the company and invested billions of dollars in it, announced that Altman and OpenAI\u2019s former president, Greg Brockman, would lead its new advanced AI research team. Brockman, also an OpenAI co-founder, quit in protest after Altman was fired.\n\nHundreds of OpenAI employees, including other top executives, threatened to join them at Microsoft in an open letter addressed to OpenAI\u2019s four-member board that called for the board\u2019s resignation and Altman\u2019s return.\n\n\u201cIf the architects and vision and brains behind these products have now left, the company will be a shell of what it once was,\u201d said Sarah Kreps, director of Cornell University\u2019s Tech Policy Institute. \u201cAll of that brain trust going to Microsoft will then mean that these impressive tools will be coming out of Microsoft. It will be hard to see OpenAI continue to thrive as a company.\u201d\n\nMicrosoft CEO Satya Nadella wrote on X, formerly known as Twitter, that he was \u201cextremely excited\u201d to bring on the pair and looked \u201cforward to getting to know\u201d the new management team at OpenAI.\n\nAltman later said on X that his top priority with Nadella is to ensure that OpenAI \u201ccontinues to thrive\u201d and that it is committed to \u201cfully providing continuity of operations to our partners and customers.\u201d\n\nSam Altman, right, then CEO of ChatGPT maker OpenAI, and Mira Murati, chief technology officer, appear at OpenAI DevDay, OpenAI\u2019s first developer conference, on Monday, Nov. 6, 2023 in San Francisco. The board of Open AI says it has pushed out Altman and appointed Murati as interim CEO role effective immediately. (AP Photo/Barbara Ortutay)\n\nOpenAI said Friday that Altman was pushed out after a review found he was \u201cnot consistently candid in his communications\u201d with the board of directors, which had lost confidence in his ability to lead the company.\n\nIn an X post Monday, OpenAI\u2019s new interim chief executive, Emmett Shear, said he would hire an independent investigator to look into Altman\u2019s ouster and write a report within 30 days.\n\n\u201cIt\u2019s clear that the process and communications around Sam\u2019s removal\u201d were handled \u201cvery badly,\u201d wrote Shear, who co-founded Twitch, an Amazon-owned livestreaming service popular with video gamers.\n\nHe said he also plans in the next month to \u201creform the management and leadership team in light of recent departures.\u201d After that, Shear said, he would \u201cdrive changes in the organization,\u201d including \u201csignificant governance changes if necessary.\u201d\n\nOriginally started as a nonprofit, and still governed as one, OpenAI\u2019s stated mission is to safely build AI that is \u201cgenerally smarter than humans.\u201d Debates have swirled around that goal and whether it conflicts with the company\u2019s increasing commercial success.\n\nThe reason behind the board\u2019s removal of Altman was not a \u201cspecific disagreement on safety,\u201d nor does the board oppose commercialization of AI models, Shear said.\n\nOpenAI last week declined to answer questions about Altman\u2019s alleged lack of candor. The company\u2019s statement said his behavior was hindering the board\u2019s ability to exercise its responsibilities.\n\nA key driver of the shakeup, OpenAI\u2019s co-founder, chief scientist and board member Ilya Sutskever, expressed regrets for his participation in the ouster.\n\n\u201cI never intended to harm OpenAI. I love everything we\u2019ve built together and I will do everything I can to reunite the company,\u201d he said Monday on X.\n\nThe open letter began circulating Monday. According to a copy obtained by The Associated Press, the number of signatures amounted to a majority of the company\u2019s 770 employees. The AP was not able to independently confirm that all of the signatures were from OpenAI employees.\n\n\u201cEveryone at @OpenAI is united,\u201d one of the signatories, research scientist Noam Brown, said on X. \u201cThis is not a civil war. Unless Sam and Greg are brought back, there will be no OpenAI left to govern.\u201d\n\nThe letter alleged that after Altman\u2019s firing, the company\u2019s remaining executive team had recommended that the board resign and be replaced with a \u201cqualified board\u201d that could stabilize the company. But the board resisted and said allowing OpenAI to be destroyed would be consistent with its mission, according to the letter.\n\nOpenAI has said since its 2015 founding that its goal is to advance AI in a way that benefits all humanity.\n\nA company spokesperson confirmed that the board received the letter.\n\nMicrosoft declined to comment on the letter.\n\nAfter Altman was pushed out, he stirred speculation about coming back into the fold in a series of tweets. He posted a selfie with an OpenAI guest pass Sunday, saying this is \u201cfirst and last time i ever wear one of these.\u201d\n\nHours earlier, he tweeted, \u201ci love the openai team so much,\u201d which drew heart replies from Brockman and Mira Murati, OpenAI\u2019s chief technology officer who was initially named as interim CEO.\n\nIt\u2019s not clear what transpired between the announcement of Murati\u2019s interim role Friday and Shear\u2019s hiring, though she was among several employees Monday who tweeted, \u201cOpenAI is nothing without its people.\u201d Altman replied to many with heart emojis.\n\nThe board consists of Sutskever, Quora CEO Adam D\u2019Angelo, tech entrepreneur Tasha McCauley and Helen Toner of the Georgetown Center for Security and Emerging Technology. None of them responded to calls or emails seeking comment. Because of its nonprofit structure, the board differs from most startup boards that are typically led by investors.\n\nSam Altman participates in a discussion during the Asia-Pacific Economic Cooperation (APEC) CEO Summit, Thursday, Nov. 16, 2023, in San Francisco. (AP Photo/Eric Risberg, File)\n\nAltman helped catapult ChatGPT to global fame based on its ability to respond to questions and produce human-like passages of text in a seemingly natural way.\n\nIn the past year, he has become Silicon Valley\u2019s most in-demand voice on the promise and potential dangers of artificial intelligence.\n\nEarlier this year, he went on a world tour to meet with government officials, drawing big crowds at public events as he discussed the risks of AI and attempts to regulate the emerging technology.\n\nBut as money poured into OpenAI this year, helping to advance its development of more capable AI, it also brought more conflict around whether that fast pace of commercialization fit with the startup\u2019s founding vision, said Kreps, the Cornell University professor. But rather than slow that pace, Altman\u2019s ouster may simply shift it out of OpenAI.\n\nAltman \u201creally has a walk-on-water aura, and I think a lot of it is well deserved,\u201d Kreps said. \u201cHe\u2019s the one who has attracted the investment, and he\u2019ll do that wherever it is.\u201d\n\nMicrosoft\u2019s shares rose 2% on Monday and hit an all-time high.\n\nThe AP and OpenAI have a licensing and technology agreement allowing OpenAI access to part of the AP\u2019s text archives.\n\n___\n\nAssociated Press writers Brian P. D. Hannon in Bangkok and Haleluya Hadero in New York contributed to this report."
    },
    {
        "metadata": {
            "title": "Sarah Silverman is suing OpenAI and Meta for copyright infringement - The Verge",
            "description": "Sarah Silverman is suing OpenAI and Meta for copyright infringement  The Verge",
            "published date": "Sun, 09 Jul 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMijwFodHRwczovL3d3dy50aGV2ZXJnZS5jb20vMjAyMy83LzkvMjM3ODg3NDEvc2FyYWgtc2lsdmVybWFuLW9wZW5haS1tZXRhLWNoYXRncHQtbGxhbWEtY29weXJpZ2h0LWluZnJpbmdlbWVudC1jaGF0Ym90cy1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1hadIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "Comedian and author Sarah Silverman, as well as authors Christopher Golden and Richard Kadrey \u2014 are suing OpenAI and Meta each in a US District Court over dual claims of copyright infringement.\n\nThe suits alleges, among other things, that OpenAI\u2019s ChatGPT and Meta\u2019s LLaMA were trained on illegally-acquired datasets containing their works, which they say were acquired from \u201cshadow library\u201d websites like Bibliotik, Library Genesis, Z-Library, and others, noting the books are \u201cavailable in bulk via torrent systems.\u201d\n\nGolden and Kadrey each declined to comment on the lawsuit, while Silverman\u2019s team did not respond by press time.\n\nIn the OpenAI suit, the trio offers exhibits showing that when prompted, ChatGPT will summarize their books, infringing on their copyrights. Silverman\u2019s Bedwetter is the first book shown being summarized by ChatGPT in the exhibits, while Golden\u2019s book Ararat is also used as an example, as is Kadrey\u2019s book Sandman Slim. The claim says the chatbot never bothered to \u201creproduce any of the copyright management information Plaintiffs included with their published works.\u201d\n\nAs for the separate lawsuit against Meta, it alleges the authors\u2019 books were accessible in datasets Meta used to train its LLaMA models, a quartet of open-source AI Models the company introduced in February.\n\nThe complaint lays out in steps why the plaintiffs believe the datasets have illicit origins \u2014 in a Meta paper detailing LLaMA, the company points to sources for its training datasets, one of which is called ThePile, which was assembled by a company called EleutherAI. ThePile, the complaint points out, was described in an EleutherAI paper as being put together from \u201ca copy of the contents of the Bibliotik private tracker.\u201d Bibliotik and the other \u201cshadow libraries\u201d listed, says the lawsuit, are \u201cflagrantly illegal.\u201d\n\nIn both claims, the authors say that they \u201cdid not consent to the use of their copyrighted books as training material\u201d for the companies\u2019 AI models. Their lawsuits each contain six counts of various types of copyright violations, negligence, unjust enrichment, and unfair competition. The authors are looking for statutory damages, restitution of profits, and more.\n\nLawyers Joseph Saveri and Matthew Butterick, who are representing the three authors, write on their LLMlitigation website that they\u2019ve heard from \u201cwriters, authors, and publishers who are con\u00adcerned about [ChatGPT\u2019s] uncanny abil\u00adity to gen\u00ader\u00adate text sim\u00adi\u00adlar to that found in copy\u00adrighted tex\u00adtual mate\u00adri\u00adals, includ\u00ading thou\u00adsands of books.\u201d\n\nSaveri has also started litigation against AI companies on behalf of programmers and artists. Getty Images also filed an AI lawsuit, alleging that Stability AI, who created the AI image generation tool Stable Diffusion, trained its model on \u201cmillions of images protected by copyright.\u201d Saveri and Butterick are also representing authors Mona Awad and Paul Tremblay in a similar case over the company\u2019s chatbot.\n\nLawsuits like this aren\u2019t just a headache for OpenAI and other AI companies; they are challenging the very limits of copyright. As we\u2019ve said on The Vergecast every time someone gets Nilay going on copyright law, we\u2019re going to see lawsuits centered around this stuff for years to come.\n\nWe\u2019ve reached out to Meta, OpenAI, and the Joseph Saveri Law Firm for comment, but they did not respond by press time."
    },
    {
        "metadata": {
            "title": "It's not just Elon Musk: ChatGPT-maker OpenAI confronting a mountain of legal challenges - The Associated Press",
            "description": "It's not just Elon Musk: ChatGPT-maker OpenAI confronting a mountain of legal challenges  The Associated Press",
            "published date": "Wed, 06 Mar 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMibWh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL29wZW5haS1zYW0tYWx0bWFuLWludmVzdGlnYXRpb24tY2hhdGdwdC1lbG9uLW11c2stZmY5ZjIwYjFlNjg3MDY0ZDlmN2Y0M2FmNTg0NDE0OGXSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "After a year of basking in global fame, the San Francisco company OpenAI is now confronting a multitude of challenges that could threaten its position at the vanguard of artificial intelligence research.\n\nSome of its conflicts stem from decisions made well before the debut of ChatGPT, particularly its unusual shift from an idealistic nonprofit to a big business backed by billions of dollars in investments.\n\nIt\u2019s too early to tell if OpenAI and its attorneys will beat back a barrage of lawsuits from Elon Musk, The New York Times and bestselling novelists such as John Grisham, not to mention escalating scrutiny from government regulators, or if any of it will stick.\n\nFeud with Elon Musk\n\nOpenAI isn\u2019t waiting for the court process to unfold before publicly defending itself against legal claims made by billionaire Elon Musk, an early funder of OpenAI who now alleges it has betrayed its founding nonprofit mission to benefit humanity as it pursued profits instead.\n\nIn its first response since the Tesla CEO sued last week, OpenAI vowed to get the claim thrown out and released emails from Musk that purport to show he supported making OpenAI a for-profit company and even suggested merging it with the electric vehicle maker.\n\nLegal experts have expressed doubt about whether Musk\u2019s arguments, centered around an alleged breach of contract, will hold up in court. But it has already forced open the company\u2019s internal conflicts about its unusual governance structure, how \u201copen\u201d it should be about its research and how to pursue what\u2019s known as artificial general intelligence, or AI systems that can perform just as well as \u2014 or even better than \u2014 humans in a wide variety of tasks.\n\nIts own internal investigation\n\nThere\u2019s still a lot of mystery about what led OpenAI to abruptly fire its co-founder and CEO Sam Altman in November, only to have him return days later with a new board that replaced the one that ousted him. OpenAI tapped the law firm WilmerHale to investigate what happened, but it\u2019s unclear how broad its scope will be and to what extent OpenAI will publicly release its findings.\n\nAmong the big questions is what OpenAI \u2014 under its previous board of directors \u2014 meant in November when it said Altman was \u201cnot consistently candid in his communications\u201d in a way that hindered the board\u2019s ability to exercise its responsibilities. While now primarily a for-profit business, OpenAI is still governed by a nonprofit board of directors whose duty is to advance its mission.\n\nThe investigators are probably looking more closely at that structure as well as the internal conflicts that led to communication breakdowns, said Diane Rulke, a professor of organizational behavior and theory at Carnegie Mellon University.\n\nRulke said it would be \u201cuseful and very good practice\u201d for OpenAI to publicly release at least part of the findings, especially given the underlying concerns about how future AI technology will affect society.\n\n\u201cNot only because it was a major event, but because OpenAI works with a lot of businesses, a lot of companies and their impact is widespread,\u201d Rulke said. \u201cEven though they\u2019re a privately held company, it\u2019s very much in the public interest to know what happened at OpenAI.\u201d\n\nGovernment scrutiny\n\nOpenAI\u2019s close business ties to Microsoft have invited scrutiny from antitrust regulators in the U.S. and Europe. Microsoft has invested billions of dollars into OpenAI and switched on its vast computing power to help build the smaller company\u2019s AI models. The software giant has also secured exclusive rights to infuse much of the technology into Microsoft products.\n\nUnlike a big business merger, such partnerships don\u2019t automatically trigger a government review. But the Federal Trade Commission wants to know if such arrangements \u201cenable dominant firms to exert undue influence or gain privileged access in ways that could undermine fair competition,\u201d FTC Chair Lina Khan said in January.\n\nFTC is awaiting responses to \u201ccompulsory orders\u201d it sent to both companies \u2014 as well as OpenAI rival Anthropic and its own cloud computing backers, Amazon and Google \u2014 requiring them to provide information about the partnerships and the decision-making around them. The companies\u2019 responses are due as soon as next week. Similar scrutiny is happening in the European Union and the United Kingdom.\n\nCopyright lawsuits\n\nBestselling novelists, nonfiction authors, The New York Times and other media outlets have sued OpenAI over allegations that the company violated copyright laws in building the AI large language models that power ChatGPT. Several of the lawsuits also target Microsoft. (The Associated Press took a different approach in securing a deal last year that gives OpenAI access to the AP\u2019s text archive for an undisclosed fee).\n\nOpenAI has argued that its practice of training AI models on huge troves of writings found on the internet is protected by the \u201cfair use\u201d doctrine of copyright law. Federal judges in New York and San Francisco must now sort through evidence of harm brought by numerous plaintiffs, including Grisham, comedian Sarah Silverman and \u201cGame of Thrones\u201d author George R. R. Martin.\n\nThe stakes are high. The Times, for instance, is asking a judge to order the \u201cdestruction\u201d of all of OpenAI\u2019s GPT large language models \u2014 the foundation of ChatGPT and most of OpenAI\u2019s business \u2014 if they were trained on its news articles.\n\n\u2014\u2014\u2014\u2014-\n\nAssociated Press business writer Kelvin Chan contributed to this report."
    },
    {
        "metadata": {
            "title": "OpenAI won't let politicians use its tech for campaigning, for now - The Washington Post",
            "description": "OpenAI won't let politicians use its tech for campaigning, for now  The Washington Post",
            "published date": "Mon, 15 Jan 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiY2h0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS90ZWNobm9sb2d5LzIwMjQvMDEvMTUvb3BlbmFpLWVsZWN0aW9uLW1pc2luZm9ybWF0aW9uLWRpc2luZm9ybWF0aW9uL9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.washingtonpost.com",
                "title": "The Washington Post"
            }
        },
        "article": "Artificial intelligence company OpenAI laid out its plans and policies to try to stop people from using its technology to spread disinformation and lies about elections, as billions of people in some of the world\u2019s biggest democracies head to the polls this year. The company, which makes the popular ChatGPT chatbot, DALL-E image generator and provides AI technology to many companies, including Microsoft, said in a Monday blog post that it wouldn\u2019t allow people to use its tech to build applications for political campaigns and lobbying, to discourage people from voting or spread misinformation about the voting process. OpenAI said it would also begin putting embedded watermarks \u2014 a tool to detect AI-created photographs \u2014 into images made with its DALL-E image-generator \u201cearly this year.\u201d\n\n\u201cWe work to anticipate and prevent relevant abuse \u2014 such as misleading \u2018deepfakes,\u2019 scaled influence operations, or chatbots impersonating candidates,\u201d OpenAI said in the blog post.\n\nAdvertisement\n\nPolitical parties, state actors and opportunistic internet entrepreneurs have used social media for years to spread false information and influence voters. But activists, politicians and AI researchers have expressed concern that chatbots and image generators could increase the sophistication and volume of political misinformation.\n\nOpenAI\u2019s measures come after other tech companies have also updated their election policies to grapple with the AI boom. In December, Google said it would restrict the kind of answers its AI tools give to election-related questions. It also said it would require political campaigns that bought ad spots from it to disclose when they used AI. Facebook parent Meta also requires political advertisers to disclose if they used AI.\n\nBut the companies have struggled to administer their own election misinformation polices. Though OpenAI bars using its products to create targeted campaign materials, an August report by The Washington Post showed these policies were not enforced.\n\nAdvertisement\n\nThere have already been high-profile instances of election-related lies being generated by AI tools. In October, The Post reported that Amazon\u2019s Alexa home speaker was falsely declaring that the 2020 presidential election was stolen and full of election fraud.\n\nShare this article Share\n\nSen. Amy Klobuchar (D-Minn.) has expressed concern that ChatGPT could interfere with the electoral process, telling people to go to a fake address when asked what to do if lines are too long at a polling location.\n\nIf a country wanted to influence the U.S. political process it could, for example, build human-sounding chatbots that push divisive narratives in American social media spaces, rather than having to pay human operatives to do it. Chatbots could also craft personalized messages tailored to each voter, potentially increasing their effectiveness at low costs.\n\nAdvertisement\n\nIn the blog post, OpenAI said it was \u201cworking to understand how effective our tools might be for personalized persuasion.\u201d The company recently opened its \u201cGPT Store,\u201d which allows anyone to easily train a chatbot using data of their own.\n\nGenerative AI tools do not have an understanding of what is true or false. Instead, they predict what a good answer might be to a question based on crunching through billions of sentences ripped from the open internet. Often, they provide humanlike text full of helpful information. They also regularly make up untrue information and pass it off as fact.\n\nImages made by AI have already shown up all over the web, including in Google search, being presented as real images. They\u2019ve also started appearing in U.S. election campaigns. Last year, an ad released by Florida Gov. Ron DeSantis\u2019s campaign used what appeared to be AI-generated images of Donald Trump hugging former White House coronavirus adviser Anthony S. Fauci. It\u2019s unclear which image generator was used to make the images.\n\nAdvertisement\n\nOther companies, including Google and Photoshop maker Adobe, have said they will also use watermarks in images generated by their AI tools. But the technology isn\u2019t a magic cure for the spread of fake AI images. Visible watermarks can be easily cropped or edited out. Embedded, cryptographic ones, which are not visible to the human eye, can be distorted simply by flipping the image or changing its color.\n\nTech companies say they\u2019re working to improve this problem and make them tamper-proof, but for now none seem to have figured out how to do that effectively yet."
    },
    {
        "metadata": {
            "title": "Sarah Silverman and novelists sue ChatGPT-maker OpenAI for ingesting their books - The Associated Press",
            "description": "Sarah Silverman and novelists sue ChatGPT-maker OpenAI for ingesting their books  The Associated Press",
            "published date": "Wed, 12 Jul 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiY2h0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL3NhcmFoLXNpbHZlcm1hbi1zdWluZy1jaGF0Z3B0LW9wZW5haS1haS04OTI3MDI1MTM5YTgxNTFlMjYwNTMyNDlkMWFlZWMyMNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "Ask ChatGPT about comedian Sarah Silverman\u2019s memoir \u201cThe Bedwetter\u201d and the artificial intelligence chatbot can come up with a detailed synopsis of every part of the book.\n\nDoes that mean it effectively \u201cread\u201d and memorized a pirated copy? Or it scraped so many customer reviews and online chatter about the bestseller or the musical it inspired that it passes for an expert?\n\nThe U.S. courts may now help sort that out after Silverman sued ChatGPT-maker OpenAI for copyright infringement this week, joining a growing number of writers who say they unwittingly built the foundation for Silicon Valley\u2019s red-hot AI boom.\n\nSilverman\u2019s lawsuit says she never gave permission for OpenAI to ingest the digital version of her 2010 book to train its AI models, and it was likely stolen from a \u201cshadow library\u201d of pirated works. It says the memoir was copied \u201cwithout consent, without credit, and without compensation.\u201d\n\nIt\u2019s one of a mounting number of cases that could crack open the secrecy of OpenAI and its rivals about the valuable data used to train increasingly widely used \u201cgenerative AI\u201d products that create new text, images and music. And it raises questions about the ethical and legal bedrock of tools that the McKinsey Global Institute projects will add the equivalent of $2.6 trillion to $4.4 trillion to the global economy.\n\n\u201cThis is an open, dirty secret of the whole machine learning industry,\u201d said Matthew Butterick, one of the lawyers representing Silverman and other authors in seeking a class-action case. \u201cThey love book data and they get it from these illicit sites. We\u2019re kind of blowing the whistle on that whole practice.\u201d\n\nOpenAI declined to comment on the allegations. Another lawsuit from Silverman makes similar claims about an AI model built by Facebook and Instagram parent company Meta, which also declined comment.\n\nIt may be a tough case for writers to win, especially after Google\u2019s success in beating back legal challenges to its online book library. The U.S. Supreme Court in 2016 let stand lower court rulings that rejected authors\u2019 claim that Google\u2019s digitizing of millions of books and showing small portions of them to the public amount to \u201ccopyright infringement on an epic scale.\u201d\n\n\u201cI think what OpenAI has done with books is awfully close to what Google was allowed to do with its Google Books project and so will be legal,\u201d said Deven Desai, associate professor of law and ethics at the Georgia Institute of Technology.\n\nWhile only a handful have sued, including Silverman and bestselling novelists Mona Awad and Paul Tremblay, concerns about the tech industry\u2019s AI-building practices have gained traction in literary and artist communities.\n\nOther prominent authors \u2014 among them Nora Roberts, Margaret Atwood, Louise Erdrich and Jodi Picoult \u2014 signed a letter late last month to the CEOs of OpenAI, Google, Microsoft, Meta and other AI developers accusing them of exploitative practices in building chatbots that \u201cmimic and regurgitate\u201d their language, style and ideas.\n\n\u201cMillions of copyrighted books, articles, essays and poetry provide the \u2018food\u2019 for AI systems, endless meals for which there has been no bill,\u201d said the open letter organized by the Authors Guild and signed by more than 4,000 writers. \u201cYou\u2019re spending billions of dollars to develop AI technology. It is only fair that you compensate us for using our writings, without which AI would be banal and extremely limited.\u201d\n\nThe AI systems behind popular products such as ChatGPT, Google\u2019s Bard and Microsoft\u2019s Bing chatbot are known as large language models that have \u201clearned\u201d by analyzing and picking up patterns from a wide body of ingested text. They\u2019ve awed the public with their strong command of human language, though they\u2019re also known for a tendency to spout falsehoods.\n\nWhile the models have also been trained on news articles and social media feeds, books are particularly valuable, as OpenAI acknowledged in a 2018 paper cited in Silverman\u2019s lawsuit.\n\nThe earliest version of OpenAI\u2019s large language model, known as GPT-1, relied on a dataset compiled by university researchers called the Toronto Book Corpus that included thousands of unpublished books, some in the adventure, fantasy and romance genres.\n\n\u201cCrucially, it contains long stretches of contiguous text, which allows the generative model to learn to condition on long-range information,\u201d OpenAI researchers said at the time. Other tech companies such as Google and Amazon relied on the same data, which is no longer available in its original form.\n\nBut since then, OpenAI and other top AI developers have grown more secretive about their sources of data, even as they have ingested larger troves of written works. Butterick said circumstantial evidence points to the use of so-called shadow libraries of pirated content that held the works of Silverman and other plaintiffs.\n\n\u201cIt\u2019s important for their models because books are the best source of long-form, well-edited, coherent writing,\u201d he said. \u201cYou basically can\u2019t have a high-quality language model unless you have books in your training data.\u201d\n\nIt could be weeks or months before a formal response is due from OpenAI. But once the case proceeds, tech executives could have to testify under oath about the sources of books they downloaded.\n\n\u201cAs far as we know, the other side hasn\u2019t denied it,\u201d said Joseph Saveri, another of Silverman\u2019s lawyers. \u201cThey don\u2019t have an alternative explanation for this.\u201d\n\nSaveri said authors aren\u2019t necessarily asking tech companies to throw away their algorithms and training data and start over \u2014 though the U.S. Federal Trade Commission has set a precedent for forcing companies to destroy ill-gotten AI data. But some way of compensating writers is needed, he said."
    },
    {
        "metadata": {
            "title": "Opinion: OpenAI's drama marks a new and scary era in artificial intelligence - Los Angeles Times",
            "description": "Opinion: OpenAI's drama marks a new and scary era in artificial intelligence  Los Angeles Times",
            "published date": "Wed, 29 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiaWh0dHBzOi8vd3d3LmxhdGltZXMuY29tL29waW5pb24vc3RvcnkvMjAyMy0xMS0yOS9vcGVuYWktc2FtLWFsdG1hbi1maXJpbmctY2hhdGdwdC1hcnRpZmljaWFsLWludGVsbGlnZW5jZdIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.latimes.com",
                "title": "Los Angeles Times"
            }
        },
        "article": "Sam Altman\u2019s dismissal and rapid reinstatement as CEO of OpenAI, the creator of ChatGPT, confirms that the future of AI is firmly in the hands of people focused on speed and profits, at the expense of all else. This elite will now impose their vision for technology on the rest of humanity. Most of us will not enjoy the consequences.\n\nThe founders of OpenAI claimed to be creating a philanthropic organization that would benefit all of humanity or at least protect it from potential harm. OpenAI is ostensibly a nonprofit, and had a small board made up of academics and experts \u2014 and notably, did not include investors. We may never know what really happened on Nov. 17, when the board fired Altman, but the most likely interpretation is that members of this board were troubled by Altman\u2019s commercial emphasis and the headlong rush to develop new, powerful models of generative artificial intelligence.\n\nIt is encouraging to think there are still people in Silicon Valley who worry about guardrails because digital technology has already done plenty of damage to jobs, wages and democracy. For example, this sector brought us Facebook and social media, which have been used to fan the flames of hatred around the world, in the name of \u201cengagement\u201d and selling more digital ads .\n\nAdvertisement\n\nAltman forced the OpenAI board to resign, as a condition of his return to the company. The new board, chaired by former Salesforce co-CEO Bret Taylor, is likely to be more sympathetic to OpenAI scaling up as fast as possible, regardless of the consequences. This recklessness is driven by the profit motive, turbocharged by venture capital, in which the most money flows to products and services that grow fastest, even if this comes with huge financial losses and tremendous societal costs.\n\nDisruption and uncontrolled growth have become religion for the tech industry, and Altman has been one of its most dedicated high priests. Yet unsustainable growth rates and large losses are not supported by the logic of the traditional capitalist market system. Venture capital created this way of operating, but OpenAI doesn\u2019t need traditional VCs, because it has Microsoft, which has already committed $10 billion to the company. Top Microsoft executives stayed focused on their goals during the Altman crisis: hire the talent, promise them unlimited money to spend, and press the pedal to the metal.\n\nWorse, the speed imperative is boosted by the predominant vision in Silicon Valley, which cares little about social responsibility or what happens to people.\n\nAn informal spokesperson for this view of the world is Marc Andreessen, legendary venture capitalist and big Altman cheerleader . In October, Andreessen put out his own \u201c Techno-Optimist\u2019s Manifesto ,\u201d which included such bizarre statements as \u201cWe had a problem of isolation, so we invented the Internet;\u201d \u201cWe have a problem of poverty, so we invent technology to create abundance;\u201d and perhaps the purest declaration of tech arrogance: \u201cGive us a real world problem, and we can invent technology that will solve it.\u201d\n\nTellingly, Mr. Andreesen does not reflect on why there are so many homeless people in the San Francisco Bay area, why there is a mental health crisis among teenagers, why Myanmar is on fire , or why the U.S. has become one of the most unequal and deeply polarized societies in modern history, despite all the technology at our disposal. One can certainly argue that these problems were exacerbated, if not created, by the tech sector.\n\nAltman previously founded Y Combinator, a start-up accelerator, which asks applicants , \u201cPlease tell us about the time you, [applicant name], most successfully hacked some (non-computer) system to your advantage.\u201d Silicon Valley leaders love to say things like \u201cm ove fast and break things \u201d (the internal Facebook motto at one point) or, as Sheera Frankel and Cecelia Kang document in their book on Facebook, \u201c An Ugly Truth \u201d: \u201cF\u2014k it, ship it.\u201d\n\nAdvertisement\n\nIn Washington D.C., any whiff of regulation or sensible guardrails drives top tech executives apoplectic. The tech bros have embraced the full-fledged libertarian fantasy in which they are the indispensable men.\n\nIn \u201c The Shape of Things to Come ,\u201d published in the early 1930s, H.G. Wells imagined a dystopian near future, in which aerial bombing campaigns came close to destroying civilization. But, after more than 20 years of disaster, Wells imagined that a new global elite controlling aviation technology would emerge to impose world peace.\n\nWells was right about the dangers posed by the unbridled and unprincipled development of technology. But his work of science fiction about a dictatorship of the elite holding the keys to the future of the world is just as disturbing today as it was in the heyday of European fascism.\n\nDaron Acemoglu and Simon Johnson are professors at MIT and co-authors of \u201cPower and Progress: Our 1,000-Year Struggle Over Technology & Prosperity.\u201d"
    },
    {
        "metadata": {
            "title": "OpenAI in Talks for Deal That Would Value Company at $80 Billion - The New York Times",
            "description": "OpenAI in Talks for Deal That Would Value Company at $80 Billion  The New York Times",
            "published date": "Fri, 20 Oct 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMTAvMjAvdGVjaG5vbG9neS9vcGVuYWktYXJ0aWZpY2FsLWludGVsbGlnZW5jZS12YWx1ZS5odG1s0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "OpenAI is in talks to complete a deal that would value the company at $80 billion or more, nearly triple its valuation less than six months ago, according to a person with knowledge of the discussions.\n\nThe company would sell existing shares in a so-called tender offer led by the venture firm Thrive Capital that would make OpenAI the most valuable start-up in San Francisco, that person said. OpenAI would also become one of the world\u2019s most valuable tech start-ups, behind ByteDance and SpaceX, according to figures from the data tracker CB Insights.\n\nNearly a year after OpenAI sparked an A.I. boom with the release of the online chatbot ChatGPT, the Silicon Valley deal-making machine continues to pump money into the field\u2019s leading companies.\n\nAmazon said last month that it would invest up to $4 billion in another San Francisco start-up, Anthropic, one of OpenAI\u2019s primary competitors. Over the summer, Cohere, a company founded by former Google researchers, raised $270 million, bringing its total funding to more than $440 million. Inflection AI, founded by a former Google executive, raised a $1.3 billion round, bringing its total to $1.5 billion."
    },
    {
        "metadata": {
            "title": "OpenAI promises to defend business customers against copyright claims - TechCrunch",
            "description": "OpenAI promises to defend business customers against copyright claims  TechCrunch",
            "published date": "Mon, 06 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiaGh0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8xMS8wNi9vcGVuYWktcHJvbWlzZXMtdG8tZGVmZW5kLWJ1c2luZXNzLWN1c3RvbWVycy1hZ2FpbnN0LWNvcHlyaWdodC1jbGFpbXMv0gFsaHR0cHM6Ly90ZWNoY3J1bmNoLmNvbS8yMDIzLzExLzA2L29wZW5haS1wcm9taXNlcy10by1kZWZlbmQtYnVzaW5lc3MtY3VzdG9tZXJzLWFnYWluc3QtY29weXJpZ2h0LWNsYWltcy9hbXAv?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://techcrunch.com",
                "title": "TechCrunch"
            }
        },
        "article": "OpenAI \u2014 bowing to peer pressure \u2014 today announced it\u2019ll step in and defend businesses using OpenAI products if they face claims around copyright infringement as it pertains to OpenAI apps and services.\n\nAs part of a new program, Copyright Shield, OpenAI says that it\u2019ll pay the legal costs incurred by customers \u2014 specifically customers using the \u201cgenerally available\u201d features of OpenAI\u2019s developer platform and ChatGPT Enterprise, the business tier of its AI-powered ChatGPT chatbot \u2014 who face lawsuits over IP claims against work generated by an OpenAI tool.\n\nThe protections seemingly don\u2019t extend to all OpenAI products, like the free and Plus tiers of ChatGPT. And it\u2019s unclear whether OpenAI is offering training data indemnity \u2014 that is, indemnity against claims made over the training data used by a customer for OpenAI\u2019s in-house generative AI models. We\u2019ll update this post as we learn more.\n\n\u201cOpenAI is committed to protecting our customers with built-in copyright safeguards in our systems,\u201d OpenAI wrote in a blog post shared with TechCrunch.\n\nGenerative AI models such as ChatGPT, GPT-4 and DALL-E 3 \u201clearn\u201d from examples to craft essays and code, create artwork and compose music \u2014 and even write lyrics to accompany that music. They\u2019re trained on millions to billions of e-books, art pieces, emails, songs, audio clips, voice recordings and more, most of which come from public websites.\n\nSome of these examples are in the public domain \u2014 at least in the case of vendors that trawl the web for training data, like OpenAI. Others aren\u2019t or come under a restrictive license that requires citation or specific forms of compensation.\n\nThe legality of vendors training on data without permission is another matter that\u2019s being hashed out in the courts. But what might possibly land generative AI users in trouble is regurgitation, or when a generative model spits out a mirror copy of a training example.\n\nPerhaps it\u2019s no surprise, then, that in a recent survey of Fortune 500 companies by Acrolinx, nearly a third said that intellectual property was their biggest concern about the use of generative AI. Another poll found that nine out of 10 developers \u201cheavily consider\u201d IP protection when making decisions on whether to use generative AI.\n\nSome generative AI vendors have pledged to defend, financially and otherwise, customers using their generative AI tools who end up on the wrong side of copyright litigation. Others have published policies to shield themselves from liability, leaving customers to foot the legal bills.\n\nIBM, Microsoft, Amazon, Getty Images, Shutterstock and Adobe are among those who\u2019ve explicitly said they\u2019ll indemnify generative AI customers over IP rights claims. Today, OpenAI joins that group \u2014 and, if recent history is any indication, it most likely won\u2019t be the last."
    },
    {
        "metadata": {
            "title": "Microsoft Emerges as the Winner in OpenAI Chaos - WIRED",
            "description": "Microsoft Emerges as the Winner in OpenAI Chaos  WIRED",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiTGh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9taWNyb3NvZnQtZW1lcmdlcy1hcy10aGUtd2lubmVyLWluLW9wZW5haS1jaGFvcy_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.wired.com",
                "title": "WIRED"
            }
        },
        "article": "Just after 2 am Pacific time on Monday morning, several OpenAI staffers\u2014including its chief technology officer, Mira Murati\u2014posted in unison on X: \u201cOpenAI is nothing without its people.\u201d Sam Altman, who was dramatically removed as the company\u2019s chief executive on Friday, reposted many of them. By then, Altman already had a new job. Satya Nadella\u2014CEO of Microsoft, a major investor and partner of OpenAI\u2014announced late on Sunday night that Altman and his cofounder Greg Brockman would be joining the tech giant to head a new \u201cadvanced AI research team.\u201d Nadella\u2019s statement seemed to suggest that others from the startup would be joining Microsoft.\n\nBy hiring Altman and Brockman amid the chaos at the top of OpenAI, Microsoft has managed to acquire one of the most successful management teams in artificial intelligence without having to buy the company\u2014whose pre-chaos valuation was $86 billion.\n\n\u201cSatya now looks like one of the most epic kingmakers,\u201d says Nathan Benaich, founder and general partner at Air Street Capital and author of the State of AI report.\n\nAt least three other senior researchers\u2014Jakub Pachocki, Aleksander M\u0105dry, and Szymon Sidor\u2014have reportedly left OpenAI.\n\n\u201cThe head and the arms and one of the legs [of OpenAI] have gone to Microsoft,\u201d says tech analyst Azeem Azhar, author of the newsletter Exponential View. \u201cThis is an enormous opportunity for Microsoft because it gets to take Sam Altman and Greg Brockman and probably a large part of the leadership team, and many of the very best engineers and researchers.\u201d\n\nAt Microsoft, Altman and Brockman will have access to huge amounts of capital and compute power, Azhar says, as well as the tech giant\u2019s support to develop other parts of the AI tech stack, including chips and consumer electronics. Altman was reportedly trying to raise billions of dollars from investors for a new chip project in the weeks running up to his firing. Altman and OpenAI had also been linked to a hardware venture with former Apple head of design Jony Ive that was reportedly hoping to build the \u201ciPhone of AI,\u201d backed by Softbank\u2019s Masayoshi Son.\n\n\u201cI\u2019m sure [Microsoft] will give Sam the leeway to go up and down the stack,\u201d Azhar says. \u201cMicrosoft itself is developing its own chips for AI. Well, Altman\u2019s group can probably help with that now, and they will be developing consumer electronics like surface computers and so on. Sam can start to head into that direction now through this group.\u201d\n\nMicrosoft shares slipped on Friday as news of the problems at OpenAI spread. OpenAI\u2019s technology has been integrated into a number of Microsoft products, including its Bing search engine, and the two companies\u2019 fortunes had been seen as deeply intertwined. The news that Altman will be moving to the company is likely to restore confidence, analysts say.\n\n\u201c[Microsoft] hired this key asset and now he will oversee OpenAI from Redmond along with Nadella which is music to the ears of investors,\u201d Dan Ives, senior equity research analyst covering the technology sector at Wedbush Securities, said in an email. \u201cIf Microsoft lost Altman he could have gone to Amazon, Google, Apple, or a host of other tech companies craving to get the face of AI globally in their doors. Instead he is safely in Microsoft\u2019s HQ now leading the company\u2019s key AI efforts.\u201d\n\nIn an increasingly competitive AI industry, this is more than just steadying the ship after a chaotic few days for Microsoft. \u201cMicrosoft would never have thought they would get this level of talent, right? And especially at the senior level,\u201d says Imran Ghory, general partner at VC Blossom Capital.\n\nWhat it means for OpenAI isn\u2019t clear, but the weekend\u2019s events have also punctured a pervasive myth that the company\u2019s lead in the industry is bulletproof. \u201cThe weekend\u2019s chaos has shown us that no one is immune from the laws of corporate physics. Considering Sam\u2019s centrality, it\u2019s the most baffling decision from an AI lab I\u2019ve witnessed,\u201d Benaich says. \u201cPeople who are investing in OpenAI took the view that it was invincible. History teaches you that no one is invincible.\"\n\nUpdated 12-22-2023, 5am EST: This article was updated to correct the attribution of a quote."
    },
    {
        "metadata": {
            "title": "Could OpenAI be the next tech giant? - The Economist",
            "description": "Could OpenAI be the next tech giant?  The Economist",
            "published date": "Mon, 18 Sep 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vd3d3LmVjb25vbWlzdC5jb20vYnVzaW5lc3MvMjAyMy8wOS8xOC9jb3VsZC1vcGVuYWktYmUtdGhlLW5leHQtdGVjaC1naWFudNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.economist.com",
                "title": "The Economist"
            }
        },
        "article": "Listen to this story. Enjoy more audio and podcasts on iOS or Android Your browser does not support the <audio> element.\n\nT he creation of a new market is like the start of a long race. Competitors jockey for position as spectators excitedly clamour. Then, like races, markets enter a calmer second phase. The field orders itself into leaders and laggards. The crowds thin.\n\nImage: The Economist\n\nIn the contest to dominate the future of artificial intelligence, Open AI , a company backed by Microsoft, established an early lead by launching Chat GPT last November. The app reached 100m users faster than any before it. Rivals scrambled. Google and its corporate parent, Alphabet, rushed the release of their chatbot, Bard. So did startups like Anthropic. Venture capitalists poured over $40bn into AI firms in the first half of 2023, nearly a quarter of all venture dollars this year. Then the frenzy died down. Public interest in AI peaked a couple of months ago, according to data from Google searches. The number of visitors to Chat GPT \u2019s website fell from 210m in May to 180m now (see chart 1).\n\nThe emerging order still sees Open AI ahead technologically. Its latest AI model, GPT -4, is beating others on a variety of benchmarks (such as an ability to answer reading and maths questions). In head-to-head comparisons, it ranks roughly as far ahead of the current runner-up, Anthropic\u2019s Claude 2, as the world\u2019s top chess player does against his closest rival\u2014a decent lead, even if not insurmountable. More important, Open AI is beginning to make real money. According to the Information, an online technology publication, it is earning revenues at an annualised rate of $1bn, compared with a trifling $28m in the year before Chat GPT \u2019s launch.\n\nCan Open AI translate its early edge into an enduring advantage, and join the ranks of big tech? To do so it must avoid the fate of erstwhile tech pioneers, from Netscape to Myspace, which were overtaken by rivals that learnt from their early successes and stumbles. And as it is a first mover, the decisions it takes will also say much about the broader direction of a nascent industry.\n\nOpen AI is a curious firm. It was founded in 2015 by a clutch of entrepreneurs including Sam Altman, its current boss, and Elon Musk, Tesla\u2019s technophilic chief executive, as a non-profit venture. Its aim was to build artificial general intelligence ( AGI ), which would equal or surpass human capacity in all types of intellectual tasks. An intermediate goal was an AI that could master a video game called \u201cDota\u201d. In working on that problem, Open AI \u2019s boffins alighted on a simple approach that involved harnessing oodles of computing power, says an early employee who has since left. When in 2017 researchers at Google published a paper describing a revolutionary machine-learning technique they christened the \u201ctransformer\u201d, Open AI \u2019s engineers realised they could scale it up by combining untold quantities of data scraped from the internet with processing oomph. The result was the generative pre-trained transformer, or GPT for short.\n\nObtaining the necessary resources required Open AI to employ some engineering of the financial variety. In 2019 it created a \u201ccapped-profit company\u201d within its non-profit structure. To begin with, investors in this business could make 100 times their initial investment\u2014but no more. Rather than distribute equity, the firm distributes claims on a share of future profits that come without ownership rights (\u201cprofit-participation units\u201d). What is more, Open AI says it may reinvest all profits until the board decides that Open AI \u2019s goal of achieving AGI has been reached. Open AI stresses that it is a \u201chigh-risk investment\u201d and should be viewed as more akin to a \u201cdonation\u201d. \u201cWe\u2019re not for everybody,\u201d says Brad Lightcap, Open AI \u2019s chief operating officer and its financial guru.\n\nMaybe not. Mr Musk pulled out in 2018. Some potential investors were scared away from Open AI \u2019s most recent funding round by its complex structure. But Mr Altman and Mr Lightcap were able to win over others. To become more attractive the company has loosened its profit cap to one based on an annual rate of return (though it will not confirm what the maximum rate is). And academic debates about the meaning of AGI aside, the profit units themselves can be sold on the market just like standard equities. The firm has already offered several opportunities for early employees to sell their units. Investors who chose to buy in appear confident that they can achieve venture-scale returns if the firm keeps growing.\n\nSoftBank, a risk-addled tech-investment house from Japan, is thought to be the latest investor keen to place a big bet on Open AI . The startup has so far raised a total of around $14bn. Most of it, perhaps $13bn, has come from Microsoft, whose Azure cloud division is also furnishing Open AI with the computing power it needs. In exchange, the software titan will receive the lion\u2019s share of Open AI \u2019s profits\u2014if these are ever handed over. In the short term, it gets to license Open AI \u2019s technology and offer this to its own clients, which include most of the world\u2019s largest companies.\n\nIt is just as well that Open AI is attracting deep-pocketed backers. For the firm needs an awful lot of capital to procure the data and computing power necessary to keep creating ever more intelligent models. Mr Altman has said that Open AI could well end up being \u201cthe most capital-intensive startup in Silicon Valley history\u201d. Open AI \u2019s most recent model, GPT -4, is estimated to have cost around $100m to train, several times more than GPT -3.\n\nFor the time being, investors appear happy to pour more money into the business. But they eventually expect a return. And for its part Open ai has realised that, if it is to achieve its mission, it must become like any other fledgling business and think hard about its costs and its revenues.\n\nGPT -4 already exhibits a degree of cost-consciousness. For example, notes Dylan Patel of SemiAnalysis, a research firm, it was divided into 16 parts that specialise in different types of tasks. That makes it trickier to design than a monolithic model. But it is then cheaper to actually use the model once it has been trained, because not all the specialists are required to answer questions. Cost is also a big reason why Open AI is not training its next big model, GPT -5. Instead, say sources familiar with the firm, it is building GPT -4.5, which would have \u201csimilar quality\u201d to GPT -4 but cost \u201ca lot less to run\u201d.\n\nA model salesman\n\nBut it is on the revenue-generating side of business that Open AI is most transformed, and where it has been most energetic of late. AI can create a lot of value long before AGI brains are as versatile as human ones, says Mr Lightcap. Open AI \u2019s models are generalist, trained on a vast amount of data and capable of doing a variety of tasks. The Chat GPT craze has made Open AI the default option for consumers, developers and businesses keen to embrace the technology. Despite the recent dip, Chat GPT still receives 60% of traffic to the top 50 generative- AI websites, according to a study by Andreessen Horowitz, a venture-capital ( VC ) firm which has invested in Open AI (see chart 2).\n\nImage: The Economist\n\nYet Open AI is no longer only\u2014or even primarily\u2014about Chat GPT . It is increasingly becoming a business-to-business platform. It is creating bespoke products of its own for big corporate customers, which include Morgan Stanley, an investment bank. It also offers tools for developers to build products using its models; on November 6th it is expected to unveil new ones at its first developer conference.\n\nIn addition, the firm has a $175m pot to invest in smaller AI startups building applications on top of its platform, which at once promotes its models and allows it to capture value if the application-builders strike gold. To spread its technology further, it is handing out perks to AI firms at Y Combinator, a Silicon Valley startup nursery that Mr Altman used to lead. John Luttig of Founders Fund, a VC firm which also has a stake in Open AI , thinks that this vast and diverse distribution may be even more important than any technical advantage.\n\nBeing the first mover certainly plays in Open AI \u2019s favour. GPT -like models\u2019 high fixed costs erect big barriers to entry for competitors. That in turn may make it easier for Open AI to lock in corporate customers. If they are to share internal company data in order to fine-tune the model to their needs, many clients may prefer not to do so more than once\u2014for cyber-security reasons, or simply because it is costly to move data from one AI provider to another, as it already is between computing clouds. Teaching big models to think also requires lots of tacit engineering know-how, from recognising high-quality data to knowing the tricks to quickly debug the source code. Mr Altman has speculated that fewer than 50 people in the world are at the true model-training frontier. A lot work for Open AI .\n\nThese are all real advantages. But they do not guarantee Open AI \u2019s dominance. For one thing, the sort of network effects where scale begets more scale, which have helped turn Alphabet, Amazon and Meta into quasi-monopolists in search, e-commerce and social networking respectively, have yet to show up. Despite its vast number of users, GPT -4 is hardly better today than six months ago. Although further tuning with user data has made it less likely to go off the rails, its overall performance has changed in unpredictable ways, in some cases for the worse.\n\nBeing a first mover in model-building may also bring some disadvantages. The biggest cost for modellers is not training but experimentation. Plenty of ideas went nowhere before the one that worked got to the training stage. That is why Open AI is estimated to have lost some $500m last year, even though GPT -4 cost one-fifth as much to train. News of ideas that do not pay off tends to spread quickly throughout AI world. This helps Open AI \u2019s competitors avoid going down costly blind alleys.\n\nAs for customers, many want to reduce their dependence on Open AI , fearful of being locked into its products and thus at its mercy. Anthropic, which was founded by defectors from Open AI , has already become a popular second choice for many AI startups. Soon they may have more cutting-edge alternatives. Google is building Gemini, a model believed to be more powerful than GPT -4. Despite its partnership with Open AI , even Microsoft is something of a rival. It has access to GPT -4\u2019s black box, as well as a vast sales force with deep ties to the world\u2019s biggest corporate IT departments. This array of choices diminishes Open AI \u2019s pricing power. It is also forcing Mr Altman\u2019s firm to keep training better models if it wants to stay ahead.\n\nThe fact that Open AI \u2019s models are a black box also reduces its appeal to some potential users, including large businesses concerned about data privacy. They may prefer more transparent \u201copen-source\u201d models like Meta\u2019s LL a MA 2. Sophisticated software firms, meanwhile, may want to build their own model, in order to exercise full control over its behaviour.\n\nOthers are moving away from generality\u2014the ability to do many things rather than just one thing\u2014by building cheaper models that are trained on narrower sets of data, or to do a specific task. A startup called Replit has trained one just to write computer programs. It sits atop Databricks, an AI cloud platform which counts Nvidia, a $1trn maker of specialist AI semiconductors, among its investors. Character AI has designed a model that lets people create virtual personalities based on real or imagined characters that can then converse with other users. It is the second-most popular AI app behind Chat GPT .\n\nThe core question, notes Kevin Kwok, a venture capitalist (who is not a backer of Open AI ), in a forthcoming essay, is how much value is derived from a model\u2019s generality. If not much, then the industry may be dominated by many specialist firms, like Replit or Character AI . If a lot, then big models such as those of Open AI or Google may come out on top. Mr Altman still believes in size. \u201cWe will keep scaling for sure,\u201d he says, even if many of the gains \u201cwill hopefully come from other things\u201d.\n\nMike Speiser of Sutter Hill Ventures (another non-Open AI backer) suspects that the market will end up containing a handful of large generalist models, with a long tail of task-specific models. Such an oligopoly might limit the chance of an astronomical Google-like outcome, but could still earn Open AI a pretty penny. And if the company really does achieve its mission of creating a thinking machine that surpasses humans? Then all bets are off. \u25a0\n\nCorrection (19th September 2023): In an earlier version of this article we made an error in expanding the abbreviation gpt . Apologies for the hallucination.\n\nTo stay on top of the biggest stories in business and technology, sign up to the Bottom Line, our weekly subscriber-only newsletter."
    },
    {
        "metadata": {
            "title": "OpenAI closes $300M share sale at $27B-29B valuation - TechCrunch",
            "description": "OpenAI closes $300M share sale at $27B-29B valuation  TechCrunch",
            "published date": "Fri, 28 Apr 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiQ2h0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wNC8yOC9vcGVuYWktZnVuZGluZy12YWx1YXRpb24tY2hhdGdwdC_SAUdodHRwczovL3RlY2hjcnVuY2guY29tLzIwMjMvMDQvMjgvb3BlbmFpLWZ1bmRpbmctdmFsdWF0aW9uLWNoYXRncHQvYW1wLw?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://techcrunch.com",
                "title": "TechCrunch"
            }
        },
        "article": "Updated to note that the Microsoft investment closed in January. The money from VCs reported here, part of a tender offer, is separate to that.\n\nOpenAI, the startup behind the widely used conversational AI model ChatGPT, has picked up new backers, TechCrunch has learned.\n\nVC firms including Sequoia Capital, Andreessen Horowitz, Thrive and K2 Global picking up new shares, according to documents seen by TechCrunch. A source tells us Founders Fund is also investing. Altogether the VCs have put in just over $300 million at a valuation of $27 billion \u2013 $29 billion. This is separate to a big investment from Microsoft announced earlier this year, a person familiar with the development told TechCrunch, which closed in January. The size of Microsoft\u2019s investment is believed to be around $10 billion, a figure we confirmed with our source.\n\nIf all this is accurate, this is the closing of the tender offer the Wall Street Journal reported was in the works in January. We confirmed that was when discussions started, amid a viral surge of interest in OpenAI and its business.\n\nWe have reached out to the investors named here, as well as to OpenAI, for comment and will update this story as we learn more. OpenAI declined to comment on the tender offer, which is separate to the Microsoft investment that closed in January.\n\nWhile Microsoft\u2019s investment comes with a strong strategic angle \u2014 the tech giant is working to integrate OpenAI\u2019s tech across a number of areas of its business \u2014 the VCs are coming in as financial backers.\n\nFrom what we understand, the term sheets have been signed by investors and the money\u2019s been transferred; still to come is countersigning from OpenAI. The plan was to make this investment public next week.\n\nAltogether, outside investors now own more than 30% of OpenAI, the source said.\n\nAccording to PitchBook data, it appears that Peter Thiel had already been a backer but it seems this is the first time Founders Fund will be investing; K2 Global, a firm with just one partner, Ozi Amanat, and Thrive are also first-time backers of the startup. From PitchBook data, it looks like Sequoia and A16Z had been earlier investors in the company.\n\nA number of firms, including Sequoia, have had some knocks as a result of the financial crisis the tech sector has seen in the last year; in general, a number of VCs have massively slowed down their investing pace, sitting on so-called \u201cdry powder\u201d waiting for a better climate, and maybe better opportunities.\n\nSo at a moment when investors are on the hunt for interesting AI startups to back, OpenAI is likely seen as the kind of opportunity that looks good right now.\n\n\u201cThey\u2019re probably trying to use this [funding] to say hey, look, we found a golden apple,\u201d a source said of the decision to back OpenAI here and now. \u201cVenture is a very strange place where anything can happen. You can go big to broke to big again, at any time.\u201d\n\nOpenAI has an army of technical teams working across a range of areas, but the area that has attracted a lot of attention of late is GPT, short for Generative Pre-trained Transformer, which is OpenAI\u2019s family of large language models used by third parties by way of APIs.\n\nThere is also ChatGPT, the generative AI service that OpenAI released at the end of November 2022 based on GPT that lets anyone type out a natural question and get a cogent, detailed answer. ChatGPT has been a certifiable hit, with more than 1 billion visitors to its website in February, says SimilarWeb \u2014 and that\u2019s not including those using that tech via third parties.\n\nGenerative AI is very much all the rage right now, but OpenAI has its controversies, too, with many focused on that buzzy, consumer-facing ChatGPT product. People have questioned whether it lies, whether it is a \u201cvirus\u201c, how it handles privacy, if it can be manipulated to be toxic, or commit libel; and in the wake of so many more rushing into AI development, even the very nature of how \u201copen\u201d OpenAI\u2019s GPT branding will be longer term has come up for discussion.\n\nIn fairness, OpenAI has acknowledged the work that still needs to be done, and meanwhile it\u2019s continued to develop services and iterate. In February, the startup introduced a paid version of ChatGPT, called ChatGPT Plus with a faster user experience. It was upgraded with multimodal LLM GPT-4 in March.\n\nKey to the proposition, OpenAI\u2019s valuation, and the likely interest of investors is that, alongside the technology, there is also a rapidly developing ecosystem around that tech.\n\nIn addition to the hundreds of millions of people who have played around with ChatGPT, hundreds of businesses large and small have started deploying GPT and ChatGPT into their products and services. That has also been a fillip to other big tech companies speed up the roll out of their own efforts in generative AI. Google has launched Bard and Meta also introduced LLaMA to take on GPT with its proprietary LLM.\n\nOpenAI, however, has some undeniable gravity amidst the competition, not least because of its singular focus on the AI space since its founding in 2015. That\u2019s been even as it has gone through some significant changes \u2014 including shifting from its original non-profit model. We don\u2019t really know if AI will precipitate the seismic shift that many say it will, but as one person put it: OpenAI may be the closest thing we have to a winner in the space right now.\n\n\u201cWe\u2019ve been working on it for so long, but it\u2019s with gradually increasing confidence that it\u2019s really going to work,\u201d co-founder and CEO Sam Altman said at an AI conference earlier this month. \u201cWe\u2019ve been [building] the company for seven years. These things take a long, long time. I would say by and large in terms of why it worked when others haven\u2019t: It\u2019s just because we\u2019ve been on the grind sweating every detail for a long time. And most people aren\u2019t willing to do that.\u201d\n\nIn addition to ChatGPT, OpenAI has its AI-based image-generation tool called Dall-E that received a significant update in July last year. It also has speech recognition model Whisper AI.\n\nMicrosoft\u2019s efforts have included integrating OpenAI\u2019s APIs with its Azure infrastructure to support the computational requirements of the models. It also in March announced a GPT-4 integration to supercharge Bing, part of Microsoft\u2019s longstanding efforts to make a dent in the dominance of Google\u2019s search services.\n\nUpdated to remove Tiger from this story; it did not participate in this share sale but is an investor in OpenAI."
    },
    {
        "metadata": {
            "title": "OpenAI: ChatGPT back in Italy after meeting watchdog demands - The Associated Press",
            "description": "OpenAI: ChatGPT back in Italy after meeting watchdog demands  The Associated Press",
            "published date": "Fri, 28 Apr 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL2NoYXRncHQtb3BlbmFpLWRhdGEtcHJpdmFjeS1pdGFseS1iOWFiM2QxMmYyYjJjZmU0OTMyMzdmZDJiOTY3NWUyMdIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "ChatGPT\u2019s maker said Friday that the artificial intelligence chatbot is available again in Italy after the company met the demands of regulators who temporarily blocked it over privacy concerns.\n\nOpenAI said it fulfilled a raft of conditions that the Italian data protection authority wanted satisfied by an April 30 deadline to have the ban on the AI software lifted.\n\n\u201cChatGPT is available again to our users in Italy,\u201d San Francisco-based OpenAI said by email. \u201cWe are excited to welcome them back, and we remain dedicated to protecting their privacy.\u201d\n\nGenerative AI systems like ChatGPT, which use vast pools of online data like digital books, blog posts and other media to generate text, images and other content mimicking human work, have created buzz in the tech world and beyond.\n\nBut their rapid development has stirred fears among officials and even tech leaders about possible ethical and societal risks, with European Union negotiators scrambling to update draft artificial intelligence regulations that have been years in the making.\n\nLast month, the Italian watchdog, known as Garante, ordered OpenAI to temporarily stop processing Italian users\u2019 personal information while it investigated a possible data breach. The authority said it didn\u2019t want to hamper AI\u2019s development but emphasized the importance of following the EU\u2019s strict data privacy rules.\n\nOpenAI said it \u201caddressed or clarified the issues\u201d raised by the watchdog.\n\nThe measures include adding information on its website about how it collects and uses data that trains the algorithms powering ChatGPT, providing EU users with a new form for objecting to having their data used for training, and adding a tool to verify users\u2019 ages when signing up.\n\nSome Italian users shared what appeared to be screenshots of the changes, including a menu button asking users to confirm their age and links to the updated privacy policy and training data help page.\n\nThe Garante said in a statement that it \u201cwelcomes the measures OpenAI implemented\u201d and urged the company to comply with two other demands for an age-verification system and a publicity campaign informing Italians about the backstory and their right to opt out of data processing.\n\nThe watchdog imposed the ban last month after finding that some users\u2019 messages and payment information were exposed to others. It also questioned whether there was a legal basis for OpenAI to collect massive amounts of data used to train ChatGPT\u2019s algorithms and raised concerns that the system could sometimes generate false information about individuals.\n\nInfrastructure Minister Matteo Salvini on Instagram, wrote approvingly of the return of ChatGPT and said that his League party \u201cis committed to help start-ups and development in Italy.\u201d\n\nOther regulators are now looking closer at such AI systems, with France\u2019s data privacy regulator and Canada\u2019s privacy commissioner investigating after receiving complaints about ChatGPT.\n\nThe head of the Federal Trade Commission, Lina Khan, warned this week that the U.S. government will \u201cnot hesitate to crack down\u201d on harmful business practices involving artificial intelligence."
    },
    {
        "metadata": {
            "title": "Authors' lawsuit against OpenAI could 'fundamentally reshape' artificial intelligence, according to experts - ABC News",
            "description": "Authors' lawsuit against OpenAI could 'fundamentally reshape' artificial intelligence, according to experts  ABC News",
            "published date": "Mon, 25 Sep 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMigQFodHRwczovL2FiY25ld3MuZ28uY29tL1RlY2hub2xvZ3kvYXV0aG9ycy1sYXdzdWl0LW9wZW5haS1mdW5kYW1lbnRhbGx5LXJlc2hhcGUtYXJ0aWZpY2lhbC1pbnRlbGxpZ2VuY2UtZXhwZXJ0cy9zdG9yeT9pZD0xMDMzNzkyMDnSAYUBaHR0cHM6Ly9hYmNuZXdzLmdvLmNvbS9hbXAvVGVjaG5vbG9neS9hdXRob3JzLWxhd3N1aXQtb3BlbmFpLWZ1bmRhbWVudGFsbHktcmVzaGFwZS1hcnRpZmljaWFsLWludGVsbGlnZW5jZS1leHBlcnRzL3N0b3J5P2lkPTEwMzM3OTIwOQ?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://abcnews.go.com",
                "title": "ABC News"
            }
        },
        "article": "A group of prominent authors joined a proposed class action lawsuit filed against OpenAI over allegations that products like ChatGPT make illegal use of their copyrighted work, setting off a high-profile legal clash.\n\nWhile the lawsuit follows a series of similar legal challenges, it features a roster of well-known plaintiffs including authors George R.R. Martin and Jodi Picoult. The case targets a company at the center of a wave of artificial intelligence-driven programs that can instantaneously suggest recipes, compose poems and muse over existentialism.\n\n\"At the heart of these algorithms is systemic theft on a massive scale,\" the lawsuit claims.\n\nThe case could fundamentally shape the direction and capabilities of generative AI, either imposing a new set of limits on a mechanism at the core of the technology or cementing an expansive approach to online material that has fueled the rise of products currently offered, legal analysts told ABC News.\n\n\"If anyone is going to win on the straight-up copyright infringement claims against OpenAI, this is probably the lawsuit that has the best chance of it,\" James Grimmelmann, professor of digital and information law at Cornell University Law School, told ABC News.\n\nGrimmelmann described the legal filing as a \"well-drafted complaint\" that presents compelling arguments over copyright infringement while avoiding murkier concerns over trademark issues or privacy.\n\nIn a statement to ABC News, an OpenAI spokesperson said the company has held constructive discussions in general with creators and remains confident its technology will prove beneficial to them.\n\n\"Creative professionals around the world use ChatGPT as a part of their creative process. We respect the rights of writers and authors, and believe they should benefit from AI technology,\" the spokesperson said.\n\n\"We're having productive conversations with many creators around the world, including the Authors Guild, and have been working cooperatively to understand and discuss their concerns about AI. We're optimistic we will continue to find mutually beneficial ways to work together to help people utilize new technology in a rich content ecosystem,\" the spokesperson said.\n\nHere's what to know about the class action lawsuit brought by authors against OpenAI, and what it may mean for the future of artificial intelligence.\n\nWhat are the authors claiming in the lawsuit?\n\nGenerative AI programs, such as ChatGPT, respond to user prompts through an algorithm that selects words based on lessons learned from scanning billions of pieces of text across the internet.\n\nThe primary argument made in the lawsuit brought by the authors, in turn, centers on the alleged illegal use of copyrighted material for the training of the AI models, Pamela Samuelson, a professor at the University of California, Berkeley Law School who specializes in the overlap between technology and copyright, told ABC News.\n\n\"The big claim is that the ingestion of works of authorship as training data is itself a reproduction of the works,\" Samuelson said.\n\nSam Altman, CEO of OpenAI, speaks to members of the press outside the \"AI Insight Forum\" at the Russell Senate Office Building on Capitol Hill on September 13, 2023 in Washington, DC. Alex Wong/Getty Images\n\nLacking permission to use the copyrighted work, OpenAI scans and makes use of the writing, which helps foster work that publishers would otherwise pay authors to create, the lawsuit alleges.\n\nQuestions remain over the exact set of data that OpenAI uses to train its products, including whether and to what extent the company draws on copyrighted material, Brian Buckmire, an ABC News legal contributor and former public defender with the Legal Aid Society, told ABC News.\n\n\u201cWe know how copyright infringements operate but we don\u2019t know how these data sets work. We don\u2019t even have the ability to look under the hood to see what type of information they are and are not using,\u201d Buckmire said. \u201cThis lawsuit could open the pandora\u2019s box, so to speak, to give light to what\u2019s going on.\"\n\nOpenAI did not respond to ABC News' request for comment about the datasets.\n\nA similar lawsuit brought against OpenAI by comedian and actress Sarah Silverman and other authors, in July, alleged that the company scanned her 2010 memoir \"The Bedwetter\" without her permission. Silverman filed a similar suit over an AI product released by Meta, the parent company of Facebook.\n\nIn response to the claim alleging the illegal use of copyrighted material, OpenAI may argue that any alleged copying of protected works falls within an exception to copyright protection known as \"fair use,\" which allows for the limited reproduction of text for uses like commentary or criticism, Grimmelmann said.\n\nIn this vein, Grimmelmann added, OpenAI may defend its alleged use of authors' work as part of an effort to create separate, original writing rather than to regurgitate identical text.\n\n\"Fair use is famously open-ended,\" Grimmelmann said.\n\nLast week, Meta and OpenAI each filed separate motions to dismiss the cases brought by Silverman. Both filings citied \"fair use\" in defense of company conduct.\n\nArguing in defense of Meta, attorneys argued that \"fair use\" protections apply to the company's use of material for the training of its AI product, Llama.\n\n\"Copyright law does not protect facts or the syntactical, structural, and linguistic information that may have been extracted from books like Plaintiffs\u2019 during training,\" the attorneys said. \"Use of texts to train Llama to statistically model language and generate original expression is transformative by nature and quintessential fair use.\"\n\nSimilarly, attorneys arguing on behalf of OpenAI said that AI-driven chatbots such as ChatGPT, also known as large language models, amount to a novel technological use of copyrighted material that does not violate the law.\n\n\"At the heart of Plaintiffs\u2019 Complaints are copyright claims,\" attorneys for OpenAI said. \"Those claims, however, misconceive the scope of copyright, failing to take into account the limitations and exceptions (including fair use) that properly leave room for innovations like the large language models now at the forefront of artificial intelligence.\"\n\nWhat are the potential implications of the lawsuit?\n\nThe implications of the lawsuit will depend on how broadly the court chooses to interpret the challenge brought by the authors, as well as the outcomes of other similar cases, Samuelson and Grimmelmann said.\n\nHowever, the impact of this case could also hold profound implications for the language-training mechanism on which text bots across the industry rely, they added.\n\n\"If the plaintiffs' claims and their arguments get upheld in full generality then it really does fundamentally reshape the industry,\" Grimmelmann said. \"If the plaintiffs in this case are right and they get everything they want, then you can't just scrape the entire web, use all of the existing big data sets and train a model.\"\n\nThe decision could force AI companies to gain permission from authors and publishers for the use of their work, giving way to potential negotiations over licensing deals between the two sides, Grimmelmann said.\n\nIf OpenAI prevails, on the other hand, it could pave the way for private individuals or firms to widely scan the internet and establish AI models based on the results, Grimmelmann added.\n\n\"If the AI companies win really broadly and all of the claims get dismissed, it basically means anybody can create an AI model by training it on almost any data they can find,\" Grimmelmann said.\n\nThe decision could shape the information marketplace, Grimmelmann added.\n\n\"This is the biggest challenge to the assumptions that the copyright system makes since the rise of the internet or maybe the rise of mass media,\" he said."
    },
    {
        "metadata": {
            "title": "OpenAI Staff Threaten to Quit Unless Board Resigns - WIRED",
            "description": "OpenAI Staff Threaten to Quit Unless Board Resigns  WIRED",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiQWh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9vcGVuYWktc3RhZmYtd2Fsay1wcm90ZXN0LXNhbS1hbHRtYW4v0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.wired.com",
                "title": "WIRED"
            }
        },
        "article": "OpenAI was in open revolt on Monday with more than 730 employees signing an open letter threatening to leave unless the board resigns and reinstates Sam Altman as CEO, along with cofounder and former president Greg Brockman. Altman was controversially fired by the board on Friday.\n\n\u201cThe process through which you terminated Sam Altman and removed Greg Brockman from the board has jeopardized all of this work and undermined our mission and company,\u201d the letter reads. \u201cYour conduct has made it clear you did not have the competence to oversee OpenAI.\u201d\n\nRemarkably, the letter\u2019s signees include Ilya Sutskever, the company\u2019s chief scientist and a member of its board, who has been blamed for coordinating the boardroom coup against Altman in the first place. By 5:10 pm ET on Monday, some 738 out of OpenAI\u2019s around 770 employees, or about 95 percent of the company, had signed the letter.\n\nShortly before the letter was released, Sutskever posted on X: \u201cI deeply regret my participation in the board\u2019s actions. I never intended to harm OpenAI. I love everything we\u2019ve built together and I will do everything I can to reunite the company.\u201d\n\nThe letter\u2019s release follows an extraordinary, head-spinning weekend in Silicon Valley. OpenAI\u2019s board removed Altman from his position on Friday, claiming \u201che was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities.\u201d\n\nHow OpenAI\u2019s Bizarre Structure Gave 4 People the Power to Fire Sam Altman Chaos at OpenAI has its root in an unusual corporate structure designed to protect humanity against rogue AI\u2014some investors had expressed fears it could weaken the company.\n\nMira Murati, OpenAI\u2019s chief technology officer, was appointed as interim CEO.\n\nAfter blowback from investors, including Microsoft, OpenAI\u2019s board seemed open to having Altman return to lead the company. Altman posted a photo of himself wearing a visitors\u2019 badge at the company\u2019s headquarters on Sunday. But last night, the board told staff that Altman would not be returning to the company.\n\nHours later, Microsoft CEO Satya Nadella announced that Altman and Brockman would be joining the tech giant to head a new advanced AI research unit.\n\nNadella appeared to leave the door open to any OpenAI employees eager to jump ship, adding of Altman\u2019s new Microsoft subsidiary: \u201cWe look forward to moving quickly to provide them with the resources needed for their success.\u201d\n\nNadella also said on X that the new venture would be \u201csetting a new pace for innovation,\u201d suggesting a more aggressive approach than the OpenAI board was apparently comfortable with.\n\nIn another rapid-fire reshuffle, OpenAI\u2019s board chose to remove Murati and appoint another interim CEO, Emmett Shear, the former CEO of Twitch, the video game streaming site.\n\nSome OpenAI staff stayed up all night debating a course of action following news that Altman would not return to OpenAI. Many staff were frustrated about a lack of communication over Altman\u2019s firing. Dozens of employees appeared to signal their willingness to jump ship and join Altman last night by posting \u201cOpenAI is nothing without its people\u201d on X.\n\nIn their letter, the OpenAI staff threaten to join Altman at Microsoft. \u201cMicrosoft has assured us that there are positions for all OpenAI employees at this new subsidiary should we choose to join,\" they write."
    },
    {
        "metadata": {
            "title": "Now you can block OpenAI's web crawler - The Verge",
            "description": "Now you can block OpenAI's web crawler  The Verge",
            "published date": "Mon, 07 Aug 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiRmh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDIzLzgvNy8yMzgyMzA0Ni9vcGVuYWktZGF0YS1zY3JhcGUtYmxvY2stYWnSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "OpenAI now lets you block its web crawler from scraping your site to help train GPT models.\n\nOpenAI said website operators can specifically disallow its GPTBot crawler on their site\u2019s Robots.txt file or block its IP address. \u201cWeb pages crawled with the GPTBot user agent may potentially be used to improve future models and are filtered to remove sources that require paywall access, are known to gather personally identifiable information (PII), or have text that violates our policies,\u201d OpenAI said in the blog post. For sources that don\u2019t fit the excluded criteria, \u201callowing GPTBot to access your site can help AI models become more accurate and improve their general capabilities and safety.\u201d\n\nBlocking the GPTBot may be the first step in OpenAI allowing internet users to opt out of having their data used for training its large language models. It follows some early attempts at creating a flag that would exclude content from training, like a \u201cNoAI\u201d tag conceived by DeviantArt last year. It does not retroactively remove content previously scraped from a site from ChatGPT\u2019s training data.\n\nThe internet provided much of the training data for large language models such as OpenAI\u2019s GPT models and Google\u2019s Bard. However, OpenAI won\u2019t confirm if it got its data through social media posts, copyrighted works, or what parts of the internet it scraped for information. And sourcing data for AI training has become increasingly contentious. Sites, including Reddit and Twitter, have pushed to crack down on the free use of their users\u2019 posts by AI companies, while authors and other creatives have sued over alleged unauthorized use of their works. Lawmakers also latched onto data privacy and consent questions in several Senate hearings around AI regulation last month."
    },
    {
        "metadata": {
            "title": "Sam Altman: CEO of OpenAI calls for US to regulate artificial intelligence - BBC.com",
            "description": "Sam Altman: CEO of OpenAI calls for US to regulate artificial intelligence  BBC.com",
            "published date": "Wed, 17 May 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiMWh0dHBzOi8vd3d3LmJiYy5jb20vbmV3cy93b3JsZC11cy1jYW5hZGEtNjU2MTY4NjbSATVodHRwczovL3d3dy5iYmMuY29tL25ld3Mvd29ybGQtdXMtY2FuYWRhLTY1NjE2ODY2LmFtcA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bbc.com",
                "title": "BBC.com"
            }
        },
        "article": ""
    },
    {
        "metadata": {
            "title": "Nearly all OpenAI employees threaten to quit, follow Sam Altman to Microsoft - The Washington Post - The Washington Post",
            "description": "Nearly all OpenAI employees threaten to quit, follow Sam Altman to Microsoft - The Washington Post  The Washington Post",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiWGh0dHBzOi8vd3d3Lndhc2hpbmd0b25wb3N0LmNvbS90ZWNobm9sb2d5LzIwMjMvMTEvMjAvbWljcm9zb2Z0LW9wZW5haS1zYW0tc2FsdG1hbi1maXJlZC_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.washingtonpost.com",
                "title": "The Washington Post"
            }
        },
        "article": "The future of OpenAI was thrown into chaos Monday after nearly all employees at the artificial intelligence company threatened to quit and join ousted chief executive Sam Altman at Microsoft if he isn\u2019t reinstated as CEO, extending the dramatic Silicon Valley boardroom saga. More than 700 of the company\u2019s roughly 770 employees have signed a letter threatening to quit unless the current board resigns and reappoints Altman, according to a person familiar with the matter. In a bizarre twist, the letter included among the signatories Ilya Sutskever, the company\u2019s chief scientist and a key member of the company\u2019s four-person board, who voted to oust Altman on Friday.\n\n\u201cYour actions have made it obvious that you are incapable of overseeing OpenAI,\u201d the employees wrote in the letter. \u201cWe are unable to work for or with people that lack competence, judgment and care for our mission and employees.\u201d\n\nMicrosoft said Nov. 20 that it has hired Sam Altman to lead its artificial intelligence research team after he was ousted from OpenAI days earlier. (Video: Reuters)\n\nThe potential mass exodus at OpenAI puts the future of the lab in doubt, a drastic change of fate for a company that, until just days ago, was considered one of the most promising start-ups in Silicon Valley with a valuation close to $90 billion. Its demise would leave a gaping hole in the center of the AI industry and potentially force thousands of start-ups to find a new provider of AI technology or face the prospect of shutting down. That could allow Big Tech giants to amass more control over powerful new AI technology, which is rapidly making its way into everyday life. Microsoft especially appeared poised to emerge as a winner, potentially gaining significant AI talent. Still, the chaos at OpenAI, which was already giving its technology to Microsoft as part of a partnership, could also affect Microsoft\u2019s future AI products, which have relied heavily on OpenAI tech.\n\nAdvertisement\n\nIn a media blitz Monday afternoon, Microsoft CEO Satya Nadella sought to assure customers and investors that his company was on solid ground no matter the outcome. He left the door open to Altman returning to OpenAI or continuing on as an AI leader at Microsoft, even though he announced late Sunday night that Altman was coming to Microsoft. \u201cI\u2019m open to both options,\u201d Nadella said in an interview on CNBC.\n\nplay Play now NaN min Follow on Podcast episode Spotify Apple Google Amazon\n\nAltman too, has signaled he could still return. \u201cWe are all going to work together some way or other,\u201d he said in a post on X Monday morning. He added that the \u201ctop priority remains to ensure OpenAI continues to thrive. We are committed to fully providing continuity of operations to our partners and customers.\u201d\n\nThat leaves the fate of Altman and OpenAI unclear, three full days after the board fired him.\n\nAdvertisement\n\nRegardless of the messaging, OpenAI employees continued to express their frustration and anger. Jan Leike, a senior OpenAI executive and respected researcher in the broader AI community who on Sunday was seen going into and out of the company\u2019s offices, said Monday on X that \u201cthe OpenAI board should resign.\u201d\n\nSkip to end of carousel Key players in OpenAI\u2019s boardroom saga arrow left arrow right Sam Altman Altman, who co-founded OpenAI in 2015 as a nonprofit research lab, was fired as CEO on Friday. The A.I. start-up announced Tuesday that he will return to his post. Bret Taylor Taylor is a Big Tech veteran, having worked at Google, Facebook and most recently as Salesforce\u2019s co-CEO, a post he held until early this year. He was board chair of Twitter during its acquisition by Elon Musk, and he led the legal battle to force Musk to go through with his acquisition of the social media platform. Larry Summers An economist and former treasury secretary under Bill Clinton, Summers was president of Harvard University until stepping down in 2006 after making controversial comments the prior year suggesting women aren\u2019t as good as men at science and engineering. He is well-connected in Silicon Valley and has been a special adviser to venture capital firm Andreessen Horowitz. Satya Nadella Nadella offered to hire Altman as head of Microsoft\u2019s new AI lab if he were to leave OpenAI. The Microsoft CEO struck a deal with OpenAI to invest billions in the company earlier this year. Emmett Shear Shear was named interim CEO of OpenAI to replace Altman and Mira Murati, who held the interim CEO title for just two days. Shear co-founded the video game streaming platform Twitch. Helen Toner Toner is one of the four members of OpenAI\u2019s board who voted to oust Altman. An AI and national security researcher at Georgetown University, she has been a proponent of making sure the AI industry has proper safeguards. She stepped down from the board. Ilya Sutskever A highly respected AI researcher, Sutskever is OpenAI\u2019s chief scientist and was one of the board members who ousted Altman, a move he later said he regretted. He is no longer on the board. Greg Brockman Brockman was one of Altman\u2019s co-founders at OpenAI and has been one of his chief lieutenants. He quit in solidarity when Altman was fired. When Altman was reinstated as CEO, he posted on X he is returning to OpenAI. Kevin Scott The Microsoft chief technology officer is one of the most powerful executives in AI. He told OpenAI employees he would hire them all if they decided to quit the company over Altman\u2019s ouster. Mira Murati Murati was an early OpenAI employee who was chief technology officer up until being appointed interim CEO after Altman\u2019s firing. The board replaced her two days later with Shear. She has supported the efforts to get Altman back to the company. Vinod Khosla The veteran venture capitalist and founder of Khosla Ventures is an early investor in OpenAI. He has vocally opposed Altman\u2019s ouster. Tasha McCauley The former OpenAI board member and tech entrepreneur also sits on the board of the Center for the Governance of AI, a British think tank that researches the impact of artificial intelligence. Adam D\u2019Angelo D\u2019Angelo is on OpenAI\u2019s board and voted to oust Altman. He was Facebook\u2019s chief technology officer in the company\u2019s early years and went on to found Quora, the question-and-answer site. 1 / 13 End of carousel\n\nEmployees who signed the letter include other senior executives such as chief technology officer Mira Murati, who had been named interim CEO by the board on Friday but was replaced by Emmett Shear on Sunday; chief operating officer Brad Lightcap; chief strategy officer Jason Kwon; and head of safety Lillian Weng. Many of the company\u2019s top researchers, AI luminaries who can command salaries in the tens of millions of dollars, also signed, including Wojciech Zaremba, Alec Radford and Bob McCrew. Without them, OpenAI would struggle to keep up with other research labs run by Google, Facebook and Anthropic AI.\n\nShare this article Share\n\nThe latest development comes after a chaotic, rapidly changing weekend for OpenAI. On Friday, its board abruptly removed Altman from his role as chief executive, saying he was \u201cnot consistently candid\u201d in his communications. That set off a dizzying two-day period during which Altman visited company headquarters to negotiate his potential return to the lab, only to see those negotiations collapse Sunday night into early Monday, when Microsoft announced Altman was joining the company, along with Greg Brockman, the former OpenAI president who had quit in solidarity with Altman. Late Sunday, OpenAI\u2019s board said it would back its ouster of Altman and appointed Shear, the co-founder of Twitch, a popular video game streaming platform, as interim CEO.\n\nAdvertisement\n\nAfter several days of being jeered by Silicon Valley heavyweights for ousting Altman, Sutskever on Monday appeared to switch his allegiance. \u201cI deeply regret my participation in the board\u2019s actions. I never intended to harm OpenAI,\u201d Sutskever wrote in a post on X, formerly Twitter. \u201cI love everything we\u2019ve built together and I will do everything I can to reunite the company.\u201d\n\nAltman and Brockman embraced the message, retweeting Sutskever\u2019s post with heart emojis. Bewildered observers tried to keep up with the twists and turns of a Silicon Valley boardroom showdown.\n\nMuch of what employees are feeling played out publicly on X, as dozens of employees at OpenAI signaled their support for Altman: \u201cOpenAI is nothing without its people.\u201d Three high-level researchers who left OpenAI on Friday after Altman was ousted are joining Microsoft, according to Brockman\u2019s post on X. They include the AI lab\u2019s former research director, Jakub Pachocki; previous head of preparedness Aleksander Madry; and former researcher Szymon Sidor.\n\nAdvertisement\n\nAltman\u2019s ouster leaves OpenAI without an executive who pushed it to build new consumer products and charge ahead with releasing AI out into the world. The firing highlighted a major rift among AI researchers and executives, where some people think the tech should be rushed forward with minimal government regulation to make money and provide helpful tools to people, while others are concerned that it could quickly surpass human intelligence and turn on its creators. OpenAI was founded as a nonprofit to provide a counter to Big Tech\u2019s power in AI, but as the company took on more investment money and began developing consumer products, some in the industry said it had abandoned its mission.\n\n\u201cHonestly, it is heartbreaking to see such a world-changing organization be ripped apart,\u201d said Sarah Guo, a venture capitalist and founder of Conviction. \u201cThe previous standard-bearer for the AI revolution, the unassailable giant in the room is vulnerable, and new leadership will have their work cut out for them to build customer and employee trust. This completely changes the strategic landscape and emboldens every other player.\u201d\n\nOther tech companies across Silicon Valley including Nvidia, Meta and Salesforce pounced on the disruption, signaling their interest in snapping up available OpenAI engineers and researchers.\n\nAdvertisement\n\n\u201cWe [love] you all from OpenAI,\u201d Jim Fan, a senior AI scientist at Nvidia, which makes AI chips, said on X.\n\nMarc Benioff, the chief executive of Salesforce, said his company will match any resigning OpenAI researcher\u2019s salary and equity package they received at the lab, \u201cto immediately\u201d join Salesforce\u2019s AI research team.\n\nMeanwhile, Shear said in a statement that he took the interim CEO job after \u201creflecting on it for just a few hours,\u201d and said he has a \u201cthree point plan for the company\u201d during his first 30 days. The plan includes the hiring of an independent investigator to \u201cdig into\u201d the company turmoil, and reforming OpenAI\u2019s management and leadership team in light of recent departures, he said."
    },
    {
        "metadata": {
            "title": "'Game of Thrones' creator and other authors sue ChatGPT-maker OpenAI for copyright infringement - The Associated Press",
            "description": "'Game of Thrones' creator and other authors sue ChatGPT-maker OpenAI for copyright infringement  The Associated Press",
            "published date": "Thu, 21 Sep 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMia2h0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL29wZW5haS1sYXdzdWl0LWF1dGhvcnMtZ3Jpc2hhbS1nZW9yZ2UtcnItbWFydGluLTM3ZjkwNzNhYjY3YWIyNWI3ZTZiMjk3NWIyYTYzYmZl0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "NEW YORK (AP) \u2014 John Grisham, Jodi Picoult and George R.R. Martin are among 17 authors suing OpenAI for \u201csystematic theft on a mass scale,\u201d the latest in a wave of legal action by writers concerned that artificial intelligence programs are using their copyrighted works without permission.\n\nIn papers filed Tuesday in federal court in New York, the authors alleged \u201cflagrant and harmful infringements of plaintiffs\u2019 registered copyrights\u201d and called the ChatGPT program a \u201cmassive commercial enterprise\u201d that is reliant upon \u201csystematic theft on a mass scale.\u201d\n\nThe suit was organized by the Authors Guild and also includes David Baldacci, Sylvia Day, Jonathan Franzen and Elin Hilderbrand among others.\n\n\u201cIt is imperative that we stop this theft in its tracks or we will destroy our incredible literary culture, which feeds many other creative industries in the U.S.,\u201d Authors Guild CEO Mary Rasenberger said in a statement. \u201cGreat books are generally written by those who spend their careers and, indeed, their lives, learning and perfecting their crafts. To preserve our literature, authors must have the ability to control if and how their works are used by generative AI.\u201d\n\nThe lawsuit cites specific ChatGPT searches for each author, such as one for Martin that alleges the program generated \u201can infringing, unauthorized, and detailed outline for a prequel\u201d to \u201cA Game of Thrones\u201d that was titled \u201cA Dawn of Direwolves\u201d and used \u201cthe same characters from Martin\u2019s existing books in the series \u201cA Song of Ice and Fire.\u201d\n\nIn a statement Wednesday, an OpenAI spokesperson said that the company respects \u201cthe rights of writers and authors, and believe they should benefit from AI technology.\n\n\u201cWe\u2019re having productive conversations with many creators around the world, including the Authors Guild, and have been working cooperatively to understand and discuss their concerns about AI. We\u2019re optimistic we will continue to find mutually beneficial ways to work together to help people utilize new technology in a rich content ecosystem,\u201d the statement reads.\n\nEarlier this month, a handful of authors that included Michael Chabon and David Henry Hwang sued OpenAI in San Francisco for \u201cclear infringement of intellectual property.\u201d\n\nIn August, OpenAI asked a federal judge in California to dismiss two similar lawsuits, one involving comedian Sarah Silverman and another from author Paul Tremblay. In a court filing, OpenAI said the claims \u201cmisconceive the scope of copyright, failing to take into account the limitations and exceptions (including fair use) that properly leave room for innovations like the large language models now at the forefront of artificial intelligence.\u201d\n\nAuthor objections to AI have helped lead Amazon.com, the country\u2019s largest book retailer, to change its policies on e-books. The online giant is now asking writers who want to publish through its Kindle Direct Program to notify Amazon in advance that they are including AI-generated material. Amazon is also limiting authors to three new self-published books on Kindle Direct per day, an effort to restrict the proliferation of AI texts."
    },
    {
        "metadata": {
            "title": "George R.R. Martin and other authors sue OpenAI for copyright infringement - The Verge",
            "description": "George R.R. Martin and other authors sue OpenAI for copyright infringement  The Verge",
            "published date": "Wed, 20 Sep 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiY2h0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDIzLzkvMjAvMjM4ODIxNDAvZ2VvcmdlLXItci1tYXJ0aW4tbGF3c3VpdC1vcGVuYWktY29weXJpZ2h0LWluZnJpbmdlbWVudNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "More authors sued OpenAI for copyright infringement, joining other writers in pursuing legal action against generative AI companies for using their books to train AI models.\n\nThe Authors Guild and 17 well-known authors like Jonathan Franzen, John Grisham, George R.R. Martin, and Jodi Picoult filed the lawsuit in the Southern District of New York. The plaintiffs hope to get the filing classified as a class action.\n\nAccording to the complaint, OpenAI \u201ccopied plaintiffs\u2019 works wholesale, without permission or consideration\u201d and fed the copyrighted materials into large language models.\n\n\u201cThese authors\u2019 livelihoods derive from the works they create. But the Defendant\u2019s LLMs endanger fiction writers\u2019 ability to make a living in that the LLMs allow anyone to generate \u2014 automatically and freely (or very cheaply) \u2014 text that they would otherwise pay writers to create,\u201d the lawsuit said.\n\nThe authors added that OpenAI\u2019s LLMs could result in derivative work \u201cthat is based on, mimics, summarizes, or paraphrases\u201d their books, which could harm their market.\n\nOpenAI, the complaint said, could have trained GPT on works in the public domain instead of pulling in copyrighted material without paying a licensing fee.\n\nOpenAI said in a statement to The Verge that the company is optimistic it is \u201chaving productive conversations with my creators around the world, including the Authors\u2019 Guild, and have been working cooperatively to understand and discuss their concerns about AI.\u201d\n\n\u201cWe\u2019re optimistic we will continue to find mutually beneficial ways to work together to help people utilize new technology in a rich content ecosystem,\u201d the company said.\n\nThis is the latest lawsuit against OpenAI from popular authors \u2014 Martin wrote Game of Thrones, Grisham\u2019s many books have been turned into films, and so on \u2014 alleging copyright infringement. Amazing Adventures of Kavalier and Clay writer Michael Chabon and others sued the company for using their books to train GPT earlier in September. Comedian Sarah Silverman and authors Christopher Golden and Richard Kadrey also sought legal action against OpenAI and Meta, while Paul Tremblay and Mona Awad filed their complaint in June.\n\nGenerative AI companies have had to thread the copyright minefield several times, with lawsuits also being filed against AI image platforms. Microsoft, which partners with OpenAI, announced it will take the legal heat if commercial users of its Copilot AI service get sued."
    },
    {
        "metadata": {
            "title": "The New York Times sues OpenAI and Microsoft for using its stories to train chatbots - The Associated Press",
            "description": "The New York Times sues OpenAI and Microsoft for using its stories to train chatbots  The Associated Press",
            "published date": "Wed, 27 Dec 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL255dC1uZXcteW9yay10aW1lcy1vcGVuYWktbWljcm9zb2Z0LTZlYTUzYThhZDNlZmEwNmVlNDY0M2I2OTdkZjBiYTU30gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "NEW YORK (AP) \u2014 The New York Times is striking back against the threat that artificial intelligence poses to the news industry, filing a federal lawsuit Wednesday against OpenAI and Microsoft seeking to end the practice of using its stories to train chatbots.\n\nThe Times says the companies are threatening its livelihood by effectively stealing billions of dollars worth of work by its journalists, in some cases spitting out Times\u2019 material verbatim to people who seek answers from generative artificial intelligence like OpenAI\u2019s ChatGPT. The newspaper\u2019s lawsuit was filed in federal court in Manhattan and follows what appears to be a breakdown in talks between the newspaper and the two companies, which began in April.\n\nThe media has already been pummeled by a migration of readers to online platforms. While many publications \u2014 most notably the Times \u2014 have successfully carved out a digital space, the rapid development of AI threatens to significantly upend the publishing industry.\n\nWeb traffic is an important component of the paper\u2019s advertising revenue and helps drive subscriptions to its online site. But the outputs from AI chatbots divert that traffic away from the paper and other copyright holders, the Times says, making it less likely that users will visit the original source for the information.\n\n\u201cThese bots compete with the content they are trained on,\u201d said Ian B. Crosby, partner and lead counsel at Susman Godfrey, which is representing The Times.\n\nAn OpenAI spokesperson said in a prepared statement that the company respects the rights of content creators and is \u201ccommitted\u201d to working with them to help them benefit from the technology and new revenue models.\n\n\u201cOur ongoing conversations with the New York Times have been productive and moving forward constructively, so we are surprised and disappointed with this development,\u201d the spokesperson said. \u201cWe\u2019re hopeful that we will find a mutually beneficial way to work together, as we are doing with many other publishers.\u201d\n\nMicrosoft did not respond to requests for comment.\n\nArtificial intelligence companies scrape information available online, including articles published by news organizations, to train generative AI chatbots. The large language models are also trained on a huge trove of other human-written materials, which helps them to build a strong command of language and grammar and to answer questions correctly.\n\nBut the technology is still under development and gets many things wrong. In its lawsuit, for example, the Times said OpenAI\u2019s GPT-4 falsely attributed product recommendations to Wirecutter, the paper\u2019s product reviews site, endangering its reputation.\n\nOpenAI and other AI companies, including rival Anthropic, have attracted billions of dollars in investments very rapidly since public and business interest in the technology exploded, particularly this year.\n\nMicrosoft has a partnership with OpenAI that allows it to capitalize on the company\u2019s AI technology. The Redmond, Washington, tech giant is also OpenAI\u2019s biggest backer and has invested at least $13 billion into the company since the two began their partnership in 2019, according to the lawsuit. As part of the agreement, Microsoft\u2019s supercomputers help power OpenAI\u2019s AI research and the tech giant integrates the startup\u2019s technology into its products.\n\nThe paper\u2019s complaint comes as the number of lawsuits filed against OpenAI for copyright infringement is growing. The company has been sued by several writers \u2014 including comedian Sarah Silverman \u2014 who say their books were ingested to train OpenAI\u2019s AI models without their permission. In June, more than 4,000 writers signed a letter to the CEOs of OpenAI and other tech companies accusing them of exploitative practices in building chatbots.\n\nAs AI technology develops, growing fears over its use have also fueled labor strikes and lawsuits in other industries, including Hollywood. Different stakeholders are realizing the technology could disrupt their entire business model, but the question will be how to respond to it, said Sarah Kreps, director of Cornell University\u2019s Tech Policy Institute.\n\nKreps said she agrees The New York Times is facing a threat from these chatbots. But she also argued solving the issue completely is going to be an uphill battle.\n\n\u201cThere\u2019s so many other language models out there that are doing the same thing,\u201d she said.\n\nThe lawsuit filed Wednesday cited examples of OpenAI\u2019s GPT-4 spitting out large portions of news articles from the Times, including a Pulitzer-Prize winning investigation into New York City\u2019s taxi industry that took 18 months to complete. It also cited outputs from Bing Chat \u2014 now called Copilot \u2014 that included verbatim excerpts from Times articles.\n\nThe Times did not list specific damages that it is seeking, but said the legal action \u201cseeks to hold them responsible for the billions of dollars in statutory and actual damages that they owe\u201d for copying and using its work. It is also asking the court to order the tech companies to destroy AI models or data sets that incorporate its work.\n\nThe News/Media Alliance, a trade group representing more than 2,200 news organizations, applauded Wednesday\u2019s action by the Times.\n\n\u201cQuality journalism and GenAI can complement each other if approached collaboratively,\u201d said Danielle Coffey, alliance president and CEO. \u201cBut using journalism without permission or payment is unlawful, and certainly not fair use.\u201d\n\nIn July, OpenAI and The Associated Press announced a deal for the artificial intelligence company to license AP\u2019s archive of news stories. This month, OpenAI also signed a similar partnership with Axel Springer, a media company in Berlin that owns Politico and Business Insider. Under the deal, users of OpenAI\u2019s ChatGPT will receive summaries of \u201cselected global news content\u201d from Axel Springer\u2019s media brands. The companies said the answers to queries will include attribution and links to the original articles.\n\nThe Times has compared its action to a copyright lawsuit more than two decades ago against Napster, when record companies sued the file-sharing service for unlawful use of their material. The record companies won and Napster was soon gone, but it has had a major impact on the industry. Industry-endorsed streaming now dominates the music business.\n\n___\n\nAP Technology Writer Matt O\u2019Brien contributed to this story."
    },
    {
        "metadata": {
            "title": "The fallout from the weirdness at OpenAI - The Economist",
            "description": "The fallout from the weirdness at OpenAI  The Economist",
            "published date": "Wed, 22 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiVWh0dHBzOi8vd3d3LmVjb25vbWlzdC5jb20vbGVhZGVycy8yMDIzLzExLzIyL3RoZS1mYWxsb3V0LWZyb20tdGhlLXdlaXJkbmVzcy1hdC1vcGVuYWnSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.economist.com",
                "title": "The Economist"
            }
        },
        "article": "Listen to this story. Enjoy more audio and podcasts on iOS or Android Your browser does not support the <audio> element.\n\nF ive very weird days passed before it seemed that Sam Altman would stay at Open AI after all. On November 17th the board of the maker of Chat gpt suddenly booted out its chief executive. On the 19th it looked as if Mr Altman would move to Microsoft, Open AI \u2019s largest investor. But employees at the startup rose up in revolt, with almost all of them, including one of the board\u2019s original conspirators, threatening to leave were Mr Altman not reinstated. Between frantic meetings, the top brass tweeted heart emojis and fond messages to each other. By the 21st, things had come full circle.\n\nAll this seems stranger still considering that these shenanigans were taking place at the world\u2019s hottest startup, which had been expected to reach a valuation of nearly $90bn. In part, the weirdness is a sign of just how quickly the relatively young technology of generative artificial intelligence has been catapulted to glory. But it also holds deeper and more disturbing lessons.\n\nOne is the sheer power of ai talent. As the employees threatened to quit, the message \u201cOpen AI is nothing without its people\u201d rang out on social media. Ever since Chat GPT\u2019s launch a year ago, demand for ai brains has been white-hot. As chaos reigned, both Microsoft and other tech firms stood ready to welcome disgruntled staff with open arms. That gave both Mr Altman and Open ai \u2019s programmers huge bargaining power and fatally undermined the board\u2019s attempts to exert control.\n\nThe episode also shines a light on the unusual structure of Open ai . It was founded in 2015 as a non-profit research lab aimed at safely developing artificial general intelligence ( agi ), which can equal or surpass humans in all types of thinking. But it soon became clear that this would require vast amounts of expensive processing power, if it were possible at all. To pay for it, a profit-making subsidiary was set up to sell AI tools, such as Chat GPT . And Microsoft invested $13bn in return for a 49% stake.\n\nOn paper, the power remained with the non-profit\u2019s board, whose aim is to ensure that agi benefits everyone, and whose responsibility is accordingly not to shareholders but to \u201chumanity\u201d. That illusion was shattered as the employees demanded Mr Altman\u2019s return, and as the prospect loomed of a rival firm housed within profit-maximising Microsoft.\n\nThe chief lesson is the folly of solely relying on corporate structures to police technology. As the potential of generative ai became clear, the contradictions in Open AI \u2019s structure were exposed. A single outfit cannot strike the best balance between advancing AI , attracting talent and investment, assessing AI \u2019s threats and keeping humanity safe. Conflicts of interest in Silicon Valley are hardly rare. Even if the people at Open AI were as brilliant as they think they are, the task would be beyond them.\n\nMuch about the board\u2019s motives in sacking Mr Altman remains unknown. Even if the directors did genuinely have humanity\u2019s interest at heart, they risked seeing investors and employees flock to another firm that would charge ahead with the technology regardless. Nor is it entirely clear what qualifies a handful of private citizens to represent the interests of Earth\u2019s remaining 7.9bn inhabitants. As part of Mr Altman\u2019s return, a new board is being appointed. It will include Larry Summers, a prominent economist; an executive from Microsoft will probably join him, as may Mr Altman.\n\nBoard senseless"
    },
    {
        "metadata": {
            "title": "Franzen, Grisham and Other Prominent Authors Sue OpenAI - The New York Times",
            "description": "Franzen, Grisham and Other Prominent Authors Sue OpenAI  The New York Times",
            "published date": "Wed, 20 Sep 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMDkvMjAvYm9va3MvYXV0aG9ycy1vcGVuYWktbGF3c3VpdC1jaGF0Z3B0LWNvcHlyaWdodC5odG1s0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "A group of prominent novelists, including John Grisham, Jonathan Franzen and Elin Hilderbrand, are joining the legal battle against OpenAI over its chatbot technology, as fears about the encroachment of artificial intelligence on creative industries continue to grow.\n\nMore than a dozen authors filed a lawsuit against OpenAI on Tuesday, accusing the company, which has been backed with billions of dollars in investment from Microsoft, of infringing on their copyrights by using their books to train its popular ChatGPT chatbot. The complaint, which was filed along with the Authors Guild, said that OpenAI\u2019s chatbots can now produce \u201cderivative works\u201d that can mimic and summarize the authors\u2019 books, potentially harming the market for authors\u2019 work, and that the writers were neither compensated nor notified by the company.\n\n\u201cThe success and profitability of OpenAI are predicated on mass copyright infringement without a word of permission from or a nickel of compensation to copyright owners,\u201d the complaint said.\n\nThe complaint, which was filed in United States District Court for the Southern District of New York, said that while OpenAI does not publicly declare which works it uses to train its models, the company has admitted to using copyrighted material. The complaint also said that OpenAI\u2019s ChatGPT is capable of producing summaries of books that include details not available in reviews or elsewhere online, which suggests the underlying program was fed the books in their entirety."
    },
    {
        "metadata": {
            "title": "OpenAI Says Board Can Overrule CEO on Safety of New AI Releases - Bloomberg",
            "description": "OpenAI Says Board Can Overrule CEO on Safety of New AI Releases  Bloomberg",
            "published date": "Mon, 18 Dec 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMicmh0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDIzLTEyLTE4L29wZW5haS1zYXlzLWJvYXJkLWNhbi1vdmVycnVsZS1jZW8tb24tc2FmZXR5LW9mLW5ldy1haS1yZWxlYXNlc9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bloomberg.com",
                "title": "Bloomberg"
            }
        },
        "article": "OpenAI said its board can choose to hold back the release of an AI model even if the company\u2019s leadership has deemed it safe, another sign of the artificial intelligence startup empowering its directors to bolster safeguards for developing the cutting-edge technology.\n\n\n\nThe arrangement was spelled out in a set of guidelines released Monday explaining how the ChatGPT-maker plans to deal with what it may deem to be extreme risks from its most powerful AI systems. The release of the guidelines follows a period of turmoil at OpenAI after Chief Executive Officer Sam Altman was briefly ousted by the board, putting a spotlight on the balance of power between directors and the company\u2019s c-suite.\n\n\n\nOpenAI\u2019s recently announced \u201cpreparedness\u201d team said it will continuously evaluate its AI systems to figure out how they fare across four different categories \u2014 including potential cybersecurity issues as well as chemical, nuclear and biological threats \u2014 and work to lessen any hazards the technology appears to pose. Specifically, the company is monitoring for what it calls \u201ccatastrophic\u201d risks, which it defines in the guidelines as \u201cany risk which could result in hundreds of billions of dollars in economic damage or lead to the severe harm or death of many individuals.\u201d\n\n\n\nAleksander Madry, who is leading the preparedness group and is on leave from a faculty position at the Massachusetts Institute of Technology, told Bloomberg News his team will send a monthly report to a new internal safety advisory group. That group will then analyze Madry\u2019s team\u2019s work and send recommendations to Altman and the company\u2019s board, which was overhauled after ousting the CEO. Altman and his leadership team can make a decision about whether to release a new AI system based on these reports, but the board has the right to reverse that decision, according to the document.\n\n\n\nOpenAI announced the formation of the \u201cpreparedness\u201d team in October, making it one of three separate groups overseeing AI safety at the startup. There\u2019s also \u201csafety systems,\u201d which looks at current products such as GPT-4, and \u201csuperalignment,\u201d which focuses on extremely powerful \u2014 and hypothetical \u2014 AI systems that may exist in the future.\n\n\n\nMadry said his team will repeatedly evaluate OpenAI\u2019s most advanced, unreleased AI models, rating them \u201clow,\u201d \u201cmedium,\u201d \u201chigh,\u201d or \u201ccritical\u201d for different types of perceived risks. The team will also make changes in hopes of reducing potential dangers they spot in AI and measure their effectiveness. OpenAI will only roll out models that are rated \u201cmedium\u201d or \u201clow,\u201d according to the new guidelines.\n\n\u201cAI is not something that just happens to us that might be good or bad,\u201d Madry said. \u201cIt\u2019s something we\u2019re shaping.\u201d"
    },
    {
        "metadata": {
            "title": "OpenAI unveils DALL-E 3, allows artists to opt out of training - TechCrunch",
            "description": "OpenAI unveils DALL-E 3, allows artists to opt out of training  TechCrunch",
            "published date": "Wed, 20 Sep 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiYGh0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wOS8yMC9vcGVuYWktdW52ZWlscy1kYWxsLWUtMy1hbGxvd3MtYXJ0aXN0cy10by1vcHQtb3V0LW9mLXRyYWluaW5nL9IBZGh0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wOS8yMC9vcGVuYWktdW52ZWlscy1kYWxsLWUtMy1hbGxvd3MtYXJ0aXN0cy10by1vcHQtb3V0LW9mLXRyYWluaW5nL2FtcC8?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://techcrunch.com",
                "title": "TechCrunch"
            }
        },
        "article": "OpenAI today unveiled an upgraded version of its text-to-image tool, DALL-E, that uses ChatGPT \u2014 OpenAI\u2019s viral AI chatbot \u2014 to take some of the pain out of prompting.\n\nMost cutting-edge, AI-powered image generation tools today take prompts \u2014 descriptions of images \u2014 and turn them into artwork in an array of styles, ranging from the photorealistic to fantastical. But crafting the right prompt can be a challenge, so much so that \u201cprompt engineering\u201d is becoming a bona fide profession.\n\nOpenAI\u2019s new tool, DALL-E 3, uses ChatGPT to help fill in prompts. Via ChatGPT, subscribers to OpenAI\u2019s premium ChatGPT plans, ChatGPT Plus and ChatGPT Enterprise, as well as Bing Chat and Bing Chat Enterprise, can type in a request for an image and hone it through conversations with the chatbot \u2014 receiving the results directly within the chat app.\n\nChatGPT will take a prompt as short as a few words and make it more descriptive, providing more guidance to the DALL-E 3 model.\n\nChatGPT integration isn\u2019t the only thing that\u2019s new with DALL-E 3. DALL-E 3 also generates higher-quality images that more accurately reflect prompts, OpenAI says \u2014 especially when dealing with longer prompts. And it better handles content that\u2019s historically tripped up image-generating models, like text and human hands.\n\nBeyond this, DALL-E 3 has new mechanisms to reduce algorithmic bias and improve safety \u2014 or so OpenAI says. For example, DALL-E 3 will reject requests that ask for an image in the style of living artists or portray public figures. And artists can now opt out of having certain \u2014 or all of \u2014 their artwork used to train future generations of OpenAI text-to-image models. (OpenAI, along with some of its rivals, is facing a lawsuit for allegedly using artists\u2019 copyrighted work to train its generative AI image models.)\n\nThe launch of DALL-E 3 comes as the generative AI race heats up, particularly in the image-synthesizing domain. Competitors like Midjourney and Stability AI continue to refine their image-generating models, putting the pressure on OpenAI to stay apace.\n\nOpenAI plans to roll out DALL-E 3 to premium ChatGPT users in October, followed by research labs and its API customers. The company didn\u2019t say when \u2014 or whether \u2014 it plans to release a free web tool, as it did with DALL-E 2 and the original DALL-E model."
    },
    {
        "metadata": {
            "title": "Sam Altman fired as CEO of OpenAI - The Verge",
            "description": "Sam Altman fired as CEO of OpenAI  The Verge",
            "published date": "Fri, 17 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiSGh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDIzLzExLzE3LzIzOTY1OTgyL29wZW5haS1jZW8tc2FtLWFsdG1hbi1maXJlZNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "Sam Altman has been fired as CEO of OpenAI, the company announced on Friday.\n\n\u201cMr. Altman\u2019s departure follows a deliberative review process by the board, which concluded that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities,\u201d the company said in its blog post. \u201cThe board no longer has confidence in his ability to continue leading OpenAI.\u201d\n\nChief technology officer Mira Murati will be the interim CEO, effective immediately. The company will be conducting a search for a permanent CEO successor. When contacted by The Verge, OpenAI\u2019s communications department declined to comment beyond the blog post. Employees at OpenAI found out about the news when it was announced publicly, according to multiple sources.\n\n\u201cI loved my time at OpenAI,\u201d Altman said in a post on X (formerly Twitter). \u201cIt was transformative for me personally, and hopefully the world a little bit.\u201d In a follow-up post, he described the experience of his firing as \u201csorta like reading your own eulogy while you\u2019re still alive.\u201d\n\nOpenAI\u2019s announcement also said that co-founder and president Greg Brockman will be stepping down as chair of the board but remain at the company. Hours after it was published, Brockman posted to X that he had quit \u201cbased on today\u2019s news.\u201d\n\nLater, in another post on X, Brockman said he and Altman were both notified of their removal from the board on Friday by OpenAI co-founder and chief scientist Ilya Sutskever, who is also a board member. \u201cSam and I are shocked and saddened by what the board did today,\u201d Brockman wrote.\n\nThis is an extremely sudden turn of events. Altman has been the face of OpenAI, which kick-started the current AI arms race with last year\u2019s hugely popular release of ChatGPT. Just last week, Altman led the keynote at the company\u2019s first-ever DevDay conference, where it announced a suite of major new updates to compete with other big tech companies like Microsoft and Google. More recently, Altman spoke at Thursday\u2019s Asia-Pacific Economic Cooperation conference.\n\nMicrosoft, which has invested billions in OpenAI, tells The Verge it will continue to partner with the company. \u201cWe have a long-term partnership with OpenAI and Microsoft remains committed to Mira and their team as we bring this next era of AI to our customers,\u201d according to a statement sent by Microsoft spokesperson Frank Shaw. Microsoft CEO Satya Nadella made a similar statement on X.\n\nAltman is a co-founder of OpenAI and initially served as a co-chair of the company alongside Elon Musk. Musk left in 2018 to avoid a conflict of interest with Tesla. He has since founded his own AI company, xAI.\n\nUnlike traditional private company boards, OpenAI\u2019s board consists mostly of outsiders. After the departures of Altman and Brockman, its remaining board members are the company\u2019s chief scientist, Ilya Sutskever; Quora CEO Adam D\u2019Angelo; Tasha McCauley, the former CEO of GeoSim Systems; and Helen Toner, the director of strategy at Georgetown\u2019s Center for Security and Emerging Technology.\n\nUpdate November 18th, 7:02PM ET: Multiple sources tell The Verge that after firing Sam Altman, OpenAI\u2019s board is now in discussions with him about returning.\n\nTom Warren contributed reporting.\n\nUpdate November 17th, 6:46PM ET: Added X posts from Sam Altman and Satya Nadella.\n\nUpdate November 17th, 7:29PM ET: Added X post from Greg Brockman and additional details."
    },
    {
        "metadata": {
            "title": "'New York Times' considers legal action against OpenAI as copyright tensions swirl - NPR",
            "description": "'New York Times' considers legal action against OpenAI as copyright tensions swirl  NPR",
            "published date": "Wed, 16 Aug 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiemh0dHBzOi8vd3d3Lm5wci5vcmcvMjAyMy8wOC8xNi8xMTk0MjAyNTYyL25ldy15b3JrLXRpbWVzLWNvbnNpZGVycy1sZWdhbC1hY3Rpb24tYWdhaW5zdC1vcGVuYWktYXMtY29weXJpZ2h0LXRlbnNpb25zLXN3aXJs0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.npr.org",
                "title": "NPR"
            }
        },
        "article": "'New York Times' considers legal action against OpenAI as copyright tensions swirl\n\nEnlarge this image toggle caption Leon Neal/Getty Images Leon Neal/Getty Images\n\nThe New York Times and OpenAI could end up in court.\n\nLawyers for the newspaper are exploring whether to sue OpenAI to protect the intellectual property rights associated with its reporting, according to two people with direct knowledge of the discussions.\n\nFor weeks, the Times and the maker of ChatGPT have been locked in tense negotiations over reaching a licensing deal in which OpenAI would pay the Times for incorporating its stories in the tech company's AI tools, but the discussions have become so contentious that the paper is now considering legal action.\n\nThe individuals who confirmed the potential lawsuit requested anonymity because they were not authorized to speak publicly about the matter.\n\nA lawsuit from the Times against OpenAI would set up what could be the most high-profile legal tussle yet over copyright protection in the age of generative AI.\n\nA top concern for the Times is that ChatGPT is, in a sense, becoming a direct competitor with the paper by creating text that answers questions based on the original reporting and writing of the paper's staff.\n\nIt's a fear heightened by tech companies using generative AI tools in search engines. Microsoft, which has invested billions into OpenAI, is now powering its Bing search engine with ChatGPT.\n\nIf, when someone searches online, they are served a paragraph-long answer from an AI tool that refashions reporting from the Times, the need to visit the publisher's website is greatly diminished, said one person involved in the talks.\n\nSo-called large language models like ChatGPT have scraped vast parts of the internet to assemble data that inform how the chatbot responds to various inquiries. The data-mining is conducted without permission. Whether hoovering up this massive repository is legal remains an open question.\n\nIf OpenAI is found to have violated any copyrights in this process, federal law allows for the infringing articles to be destroyed at the end of the case.\n\nIn other words, if a federal judge finds that OpenAI illegally copied the Times' articles to train its AI model, the court could order the company to destroy ChatGPT's dataset, forcing the company to recreate it using only work that it is authorized to use.\n\nFederal copyright law also carries stiff financial penalties, with violators facing fines up to $150,000 for each infringement \"committed willfully.\"\n\n\"If you're copying millions of works, you can see how that becomes a number that becomes potentially fatal for a company,\" said Daniel Gervais, the co-director of the intellectual property program at Vanderbilt University who studies generative AI. \"Copyright law is a sword that's going to hang over the heads of AI companies for several years unless they figure out how to negotiate a solution.\"\n\nThe Times' talks with OpenAI follow reports that the paper will not join other media organizations in attempting to negotiate with tech companies over use of content in AI models. A person at the Times said not participating is unrelated to any potential litigation against OpenAI, which declined to comment through a spokesperson.\n\nWhile a spokesman for the Times would not comment, the paper's executives have publicly nodded at the tension.\n\nIn June, Times CEO Meredith Kopit Levien said at the Cannes Lions Festival that it is time for tech companies to pay their fair share for tapping the paper's vast archives.\n\n\"There must be fair value exchange for the content that's already been used, and the content that will continue to be used to train models,\" she said.\n\nThe same month, Alex Hardiman, the paper's chief product officer, and Sam Dolnick, a deputy managing editor, described in a memo to staff a new internal initiative designed to capture the potential benefits of artificial intelligence.\n\nThey cited \"protecting our rights\" among their chief fears: \"How do we ensure that companies that use generative AI respect our intellectual property, brands, reader relationships and investments?\"\n\nA Times suit would join other copyright holders taking aim at AI companies\n\nAny potential suit the Times files would join other similar legal actions leveled against OpenAI in recent weeks.\n\nComedian Sarah Silverman joined a class-action suit against the company, alleging that she never gave ChatGPT permission to ingest a digital version of her 2010 memoir The Bedwetter, which she says the company swallowed up from an illegal online \"shadow library\"\n\nOther generative AI companies, like Stability AI, which distributes the image generator Stable Diffusion, have also been hit with copyright lawsuits.\n\nGetty Images is suing Stability AI for allegedly training an AI model on more than 12 million Getty Images photos without authorization.\n\n\"Copyright holders see these instances are reckless, and AI companies see it as gutsy,\" Vanderbilt's Gervais said. \"As always, the final answer will be determined by who ends up winning these lawsuits.\"\n\nLegal experts say AI companies are likely to invoke a defense citing what is known as \"fair use doctrine,\" which allows for the use of a work without permission in certain instances, including teaching, criticism, research and news reporting.\n\nKey question for AI suits: Will 'fair use' apply?\n\nThere are two legal precedents that will likely play a part in the pending AI copyright disputes.\n\nThe first is a 2015 federal appeals court ruling that found that Google's digitally scanning of millions of books for its Google Books library was a legally permissible use of \"fair use,\" and not copyright infringement.\n\nThe court found that Google's digital library of books did not create a \"significant market substitute\" for the books, meaning it did not compete with the original works.\n\nLegal experts say proving that in the AI cases will be a major hurdle to overcome for OpenAI.\n\nThe second case expected to be relevant to the AI copyright suits is the Andy Warhol Foundation case the Supreme Court decided in May.\n\nIn it, the high court found that Andy Warhol was not protected by fair use doctrine when he altered a photograph of Prince taken by Lynn Goldsmith. Importantly, the court found that Warhol and Goldsmith were selling the images to magazines.\n\nTherefore, the court wrote, the original and the copied work shared \"the same or highly similar purposes, or where wide dissemination of a secondary work would otherwise run the risk of substitution for the original or licensed derivatives of it.\"\n\nLawyers for the Times believe OpenAI's use of the paper's articles to spit out descriptions of news events should not be protected by fair use, arguing that it risks becoming something of a replacement for the paper's coverage.\n\nNPR's David Folkenflik contributed to this report."
    },
    {
        "metadata": {
            "title": "OpenAI In Talks to License CNN Work to Train ChatGPT as NYT Sues - Bloomberg",
            "description": "OpenAI In Talks to License CNN Work to Train ChatGPT as NYT Sues  Bloomberg",
            "published date": "Thu, 11 Jan 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDI0LTAxLTEwL29wZW5haS1pbi10YWxrcy13aXRoLWNubi1mb3gtYW5kLXRpbWUtdG8tbGljZW5zZS1jb250ZW500gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bloomberg.com",
                "title": "Bloomberg"
            }
        },
        "article": "OpenAI is in talks with CNN, Fox Corp. and Time to license their work, according to people familiar with the matter, in a growing effort to secure access to news content to build out its artificial intelligence products while facing allegations it\u2019s ripping off copyrighted materials.\n\nThe startup behind ChatGPT, a tool that lets users quickly crank out text, code and other content with simple prompts, is seeking to cut deals with numerous producers of news, video and other digital media that can be used to make the AI chatbot more accurate, relevant and up to date. OpenAI is also battling lawsuits alleging copyright infringement."
    },
    {
        "metadata": {
            "title": "OpenAI Unveils Audio Tool That Recreates Human Voices - The New York Times",
            "description": "OpenAI Unveils Audio Tool That Recreates Human Voices  The New York Times",
            "published date": "Fri, 29 Mar 2024 17:00:08 GMT",
            "url": "https://news.google.com/rss/articles/CBMiRmh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjQvMDMvMjkvdGVjaG5vbG9neS9vcGVuYWktdm9pY2UtZW5naW5lLmh0bWzSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "First, OpenAI offered a tool that allowed people to create digital images simply by describing what they wanted to see. Then, it built similar technology that generated full-motion video like something from a Hollywood movie.\n\nNow, it has unveiled technology that can recreate someone\u2019s voice.\n\nThe high-profile A.I. start-up said on Friday that a small group of businesses was testing a new OpenAI system, Voice Engine, that can recreate a person\u2019s voice from a 15-second recording. If you upload a recording of yourself and a paragraph of text, it can read the text using a synthetic voice that sounds like yours.\n\nThe text does not have to be in your native language. If you are an English speaker, for example, it can recreate your voice in Spanish, French, Chinese or many other languages."
    },
    {
        "metadata": {
            "title": "OpenAI acquires AI design studio Global Illumination - TechCrunch",
            "description": "OpenAI acquires AI design studio Global Illumination  TechCrunch",
            "published date": "Wed, 16 Aug 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiV2h0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wOC8xNi9vcGVuYWktYWNxdWlyZXMtYWktZGVzaWduLXN0dWRpby1nbG9iYWwtaWxsdW1pbmF0aW9uL9IBW2h0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wOC8xNi9vcGVuYWktYWNxdWlyZXMtYWktZGVzaWduLXN0dWRpby1nbG9iYWwtaWxsdW1pbmF0aW9uL2FtcC8?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://techcrunch.com",
                "title": "TechCrunch"
            }
        },
        "article": "OpenAI, the AI company behind the viral AI-powered chatbot ChatGPT, has acquired Global Illumination, a New York\u2013based startup leveraging AI to build creative tools, infrastructure and digital experiences.\n\nIt\u2019s OpenAI\u2019s first public acquisition in its roughly seven-year history. The terms of the deal weren\u2019t disclosed.\n\n\u201cWe\u2019re very excited for the impact they\u2019ll have here at OpenAI,\u201d OpenAI wrote in a brief post published to its official blog. \u201cThe entire team has joined OpenAI to work on our core products including ChatGPT.\u201d\n\nGlobal Illumination, launched by Thomas Dimson, Taylor Gordon and Joey Flynn, has been involved in a range of projects since its founding in 2021. Backed by VC firms Paradigm, Benchmark and Slow, Global Illumination\u2019s team designed and built products early on at Instagram and Facebook as well as YouTube, Google, Pixar and Riot Games.\n\nAs director of engineering at Instagram, Dimson was instrumental in iterating the platform\u2019s discovery algorithms. While there, he helped to start the teams responsible for Instagram\u2019s Explore tab experience, feed and Stories ranking, IGTV and general data engineering.\n\nGlobal Illumination\u2019s most recent creation is Biomes, a Minecraft-like open source sandbox multiplayer online role-playing game (MMORPG) built for the web. The game\u2019s fate is unclear, but one would presume that the team\u2019s work at OpenAI will have less of an entertainment bent.\n\nOpenAI might\u2019ve avoided acquisitions until now, but the company, backed by billions in venture capital from Microsoft and major VC players, has for several years run funds and grant programs to invest in emerging AI companies and organizations.\n\nCertainly, OpenAI is on the hunt for a commercial win. While ChatGPT achieved global fame, OpenAI reportedly spent upward of $540 million last year to develop it, including funds it used to poach talent from the likes of Google, according to The Information.\n\nOpenAI made $30 million in revenue last year. But CEO Sam Altman has reportedly told investors that the company intends to boost that figure to $200 million this year and $1 billion next year."
    },
    {
        "metadata": {
            "title": "OpenAI CEO Sam Altman is still chasing billions to build AI chips - The Verge",
            "description": "OpenAI CEO Sam Altman is still chasing billions to build AI chips  The Verge",
            "published date": "Fri, 19 Jan 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiUWh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDI0LzEvMTkvMjQwNDQzMTkvb3BlbmFpLWNoaXAtbWFudWZhY3R1cmluZy1mdW5kcmFpc2luZ9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "A new report from Bloomberg says that once-again CEO of OpenAI Sam Altman\u2019s efforts to raise billions for an AI chip venture are aimed at using that cash to develop a \u201cnetwork of factories\u201d for fabrication that would stretch around the globe and involve working with unnamed \u201ctop chip manufacturers.\u201d\n\nA major cost and limitation for running AI models is having enough chips to handle the computations behind bots like ChatGPT or DALL-E that answer prompts and generate images. Nvidia\u2019s value rose above $1 trillion for the first time last year, partly due to a virtual monopoly it has as GPT-4, Gemini, Llama 2, and other models depend heavily on its popular H100 GPUs.\n\nAccordingly, the race to manufacture more high-powered chips to run complex AI systems has only intensified. The limited number of fabs capable of making high-end chips is driving Altman or anyone else to bid for capacity years before you need it in order to produce the new chips. And going against the likes of Apple requires deep-pocketed investors who will front costs that the nonprofit OpenAI still can\u2019t afford. SoftBank Group and Abu Dhabi-based AI holding company G42 have reportedly been in talks about raising money for Altman\u2019s project.\n\nMicrosoft\u2019s new Azure Maia 100 AI processor. Image: Microsoft\n\nAWS, Azure, and Google use Nvidia\u2019s H100 processors as well. This week, Meta CEO Mark Zuckerberg told The Verge reporter Alex Heath that \u201cby the end of this year, Meta will own more than 340,000 of Nvidia\u2019s H100 GPUs\u201d as the company pursues the development of artificial general intelligence (AGI).\n\nNvidia GH200 \u201cGrace Hopper Superchip\u201d Image: Nvidia"
    },
    {
        "metadata": {
            "title": "OpenAI to offer remedies to resolve Italy's ChatGPT ban - The Associated Press",
            "description": "OpenAI to offer remedies to resolve Italy's ChatGPT ban  The Associated Press",
            "published date": "Thu, 06 Apr 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL2NoYXRncHQtb3BlbmFpLWRhdGEtcHJpdmFjeS1pdGFseS0xZTNmMDcwY2E4NmVjMjM0Y2FlNGQwOGFjODQ0Mzg3OdIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "LONDON (AP) \u2014 The company behind ChatGPT will propose measures to resolve data privacy concerns that sparked a temporary Italian ban on the artificial intelligence chatbot, regulators said Thursday.\n\nThe Italian data protection authority, known as Garante, last week blocked San Francisco-based OpenAI\u2019s popular chatbot, ordering it to temporarily stop processing Italian users\u2019 personal information while it investigates a possible breach of European Union data privacy rules.\n\nExperts said it was the first such case of a democracy imposing a nationwide ban on a mainstream AI platform.\n\nIn a video call late Wednesday between the watchdog\u2019s commissioners and OpenAI executives including CEO Sam Altman, the company promised to set out measures to address the concerns. Those remedies have not been detailed.\n\nThe Italian watchdog said it didn\u2019t want to hamper AI\u2019s development but stressed to OpenAI the importance of complying with the 27-nation EU\u2019s stringent privacy rules.\n\nThe regulators imposed the ban after some users\u2019 messages and payment information were exposed to others. They also questioned whether there\u2019s a legal basis for OpenAI to collect massive amounts of data used to train ChatGPT\u2019s algorithms and raised concerns the system could sometimes generate false information about individuals.\n\nSo-called generative AI technology like ChatGPT is \u201ctrained\u201d on huge pools of data, including digital books and online writings, and able to generate text that mimics human writing styles.\n\nThese systems have created buzz in the tech world and beyond, but they also have stirred fears among officials, regulators and even computer scientists and tech industry leaders about possible ethical and societal risks.\n\nOther regulators in Europe and elsewhere have started paying more attention after Italy\u2019s action.\n\nIreland\u2019s Data Protection Commission said it\u2019s \u201cfollowing up with the Italian regulator to understand the basis for their action and we will coordinate with all EU Data Protection Authorities in relation to this matter.\u201d\n\nFrance\u2019s data privacy regulator, CNIL, said it\u2019s investigating after receiving two complaints about ChatGPT. Canada\u2019s privacy commissioner also has opened an investigation into OpenAI after receiving a complaint about the suspected \u201ccollection, use and disclosure of personal information without consent.\u201d\n\nIn a blog post this week, the U.K. Information Commissioner\u2019s Office warned that \u201corganizations developing or using generative AI should be considering their data protection obligations from the outset\u201d and design systems with data protection as a default.\n\n\u201cThis isn\u2019t optional \u2014 if you\u2019re processing personal data, it\u2019s the law,\u201d the office said.\n\nIn an apparent response to the concerns, OpenAI published a blog post Wednesday outlining its approach to AI safety. The company said it works to remove personal information from training data where feasible, fine-tune its models to reject requests for personal information of private individuals, and acts on requests to delete personal information from its systems."
    },
    {
        "metadata": {
            "title": "OpenAI previews Voice Engine generator, acknowledging risks - NBC News",
            "description": "OpenAI previews Voice Engine generator, acknowledging risks  NBC News",
            "published date": "Fri, 29 Mar 2024 20:47:34 GMT",
            "url": "https://news.google.com/rss/articles/CBMibWh0dHBzOi8vd3d3Lm5iY25ld3MuY29tL3RlY2gvaW5ub3ZhdGlvbi9vcGVuYWktcHJldmlld3Mtdm9pY2UtZW5naW5lLWdlbmVyYXRvci1hY2tub3dsZWRnaW5nLXJpc2tzLXJjbmExNDU2NjPSAStodHRwczovL3d3dy5uYmNuZXdzLmNvbS9uZXdzL2FtcC9yY25hMTQ1NjYz?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nbcnews.com",
                "title": "NBC News"
            }
        },
        "article": "Artificial intelligence startup OpenAI released a preview Friday of a digital voice generator that it said could produce natural-sounding speech based on a single 15-second audio sample.\n\nThe software is called Voice Engine. It\u2019s the latest product to come out of the San Francisco startup that\u2019s also behind the popular chatbot ChatGPT and the image generator DALL-E.\n\nThe company said in a blog post that it had tested Voice Engine in an array of possible uses, including reading assistance to children, language translation and voice restoration for cancer patients.\n\nSome social media users reacted by highlighting possible misuses, including potential fraud assisted with unauthorized voice imitation, or deepfakes.\n\nBut OpenAI said it was holding off for now on a wider release of the software because of the potential for misuse, including during an election year. It said it first developed the product in late 2022 and had been using it behind the scenes in other products.\n\n\u201cWe are taking a cautious and informed approach to a broader release due to the potential for synthetic voice misuse,\u201d the company said in the unsigned post.\n\n\u201cWe hope to start a dialogue on the responsible deployment of synthetic voices, and how society can adapt to these new capabilities,\u201d it said. \u201cBased on these conversations and the results of these small scale tests, we will make a more informed decision about whether and how to deploy this technology at scale.\u201d\n\nThe 2024 election has already witnessed its first fake voice, which appeared in New Hampshire in a robocall in January imitating President Joe Biden. A Democratic operative later said he commissioned the fake voice using artificial intelligence and the help of a New Orleans street magician.\n\nAfter that call, the Federal Communications Commission voted unanimously to ban unsolicited AI robocalls.\n\nOpenAI acknowledged the political risks in its blog post.\n\n\u201cWe recognize that generating speech that resembles people\u2019s voices has serious risks, which are especially top of mind in an election year,\u201d it said.\n\nThe company said it was \u201cengaging with U.S. and international partners from across government, media, entertainment, education, civil society and beyond to ensure we are incorporating their feedback as we build.\u201d\n\nIt said its usage policies prohibit impersonation without consent or legal right, and it said broad deployment should be accompanied by \u201cvoice authentication experiences\u201d to verify that the original speaker knowingly added their voice to the service. It also called for a \u201cno-go voice list\u201d to prevent the creation of voices that are too similar to prominent figures.\n\nBut finding a way to detect and label AI-generated content has proven difficult for the tech industry. Proposed solutions such as \u201cwatermarking\u201d have proven easy to remove or bypass.\n\nGeoffrey Miller, an associate professor of psychology at the University of New Mexico, responded to OpenAI on the platform X asking what it would do about potential misuse by criminals.\n\n\u201cWhen millions of older adults are defrauded out of billions of dollars by these deepfake voices, will @OpenAI be ready for the tsunami of litigation that follows?\u201d he asked. The company did not immediately reply to him."
    },
    {
        "metadata": {
            "title": "Sam Altman Talks OpenAI: TIME100 Most Influential Companies - TIME",
            "description": "Sam Altman Talks OpenAI: TIME100 Most Influential Companies  TIME",
            "published date": "Wed, 21 Jun 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiTWh0dHBzOi8vdGltZS5jb20vY29sbGVjdGlvbi90aW1lMTAwLWNvbXBhbmllcy0yMDIzLzYyODQ4NzAvb3BlbmFpLWRpc3J1cHRlcnMv0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://time.com",
                "title": "TIME"
            }
        },
        "article": "You ever watch Star Trek?\u201d Sam Altman, the CEO who has become the most visible face of the current artificial-intelligence boom, has just called us an Uber. The 38-year-old serial entrepreneur has lately become known for talking up the risks of AI, but he is at his most animated in talking about its possibilities. So transformative is this new technology that responds naturally to our verbal commands that he envisions new hardware for it\u2014something, eventually, like the Star Trek holodeck, in which characters use their voice to conjure and interact with 3D simulations of the world. An interface like that feels \u201cfundamentally right,\u201d he says.\n\nAltman\u2019s company, OpenAI, is only seven years old. It has fewer than 500 employees. Pipe some pan flutes and whale sounds into the airy, light-filled lobby of its headquarters in San Francisco\u2019s Mission District, and it could almost be mistaken for a spa. But in the span of 6 months, the company\u2014through its viral product ChatGPT\u2014has vaulted AI into public consciousness. Few doubt it\u2019s at the vanguard of a revolution that will, for better or worse and probably both, change the world.\n\nChatGPT is almost certainly the most rapidly adopted product in the history of technology. It\u2019s also one of the more versatile, capable of responding to a vast array of user prompts, from \u201cTell me a joke\u201d to \u201cDraft 10 slides with ideas to grow revenue at a hair salon.\u201d It can write poetry and explain scientific concepts. Altman says he uses it for routine tasks, like pulling highlights from his overflowing email inbox or to \u201cdraft a tweet that I was having a hard time with.\u201d Essentially a super-powerful auto-complete tool trained to generate language by observing patterns in large quantities of data, it has its limits\u2014including a disconcerting inability to separate truth from fiction. OpenAI\u2019s warning about this, placed beneath the text input box, hasn\u2019t stopped people from using it for homework, investment advice, and even therapy.\n\nConsumer-facing AIs had hit the market before, but something about ChatGPT\u2019s text-message-inspired, conversational interface clicked. In the days following the Nov. 30 release, OpenAI employees were glued to their screens, posting graphs in the company Slack channel as usage numbers took off. \u201cIt just kept going up and to the right at a steeper and steeper angle,\u201d says Diane Yoon, OpenAI\u2019s vice president of people. Two months later, ChatGPT had more than 100 million unique visitors, according to data from Similarweb. Instagram took 30 months to reach that level.\n\nA weekly newsletter featuring conversations with the world\u2019s top CEOs, managers, and founders. Join the Leadership Brief.\n\nIt was the start of an AI arms race. Google declared an internal \u201cCode Red\u201d and fused its two AI labs\u2014Google Brain and DeepMind\u2014into one organization. Microsoft, having already invested $3 billion in OpenAI, poured in an additional $10 billion. Billions more flowed into startups and the stocks of public companies that could plausibly (and implausibly) claim AI would supercharge their growth. In March, OpenAI upped the stakes again, releasing an even more powerful tool called GPT-4.\n\nPhotograph by Michelle Watt for TIME\n\nTempering all the promise is real fear. There\u2019s little doubt AI will make many jobs extinct, as new technology does, even as it creates new ones. It\u2019s also enhancing the ability of bad actors to flood us with fake content masquerading as truth and fake voices that sound eerily like those of our loved ones. Can we trust what we see or hear? Altman acknowledges, with unsettling matter-of-factness, that the answer is probably no. \u201cYou can\u2019t trust a voice you hear over the phone anymore,\u201d he says. \u201cWe just all need to start telling people this is coming.\u201d\n\nIn the wrong hands, these tools could cause even worse problems, launching cyberattacks or causing havoc in financial markets. And if AIs were to become capable of making plans on their own and acting on them\u2014especially if those plans aren\u2019t \u201caligned\u201d to human values\u2014it\u2019s possible to imagine them deciding humans are obstacles to their goals. Altman himself joined dozens of other tech leaders and scientists recently to sign a statement that cast the development of AI as a risk on par with pandemics and nuclear war. The worst-case scenario, he said earlier this year, is \u201clights out for everyone.\u201d\n\nRead more: TIME\u2019s Full Interview With OpenAI CEO Sam Altman\n\nThis has become Altman\u2019s calling card, championing the possibilities of AI while urging policymakers to get going on rules of the road to mitigate the dangers. \u201cI\u2019m a Midwestern Jew,\u201d says Altman, who grew up in St. Louis. \u201cI think that fully explains my exact mental model\u2014very optimistic, and prepared for things to go super wrong at any point.\u201d A related theme of Altman\u2019s is adaptability: the idea that success comes from the ability to reorient yourself to even radically new circumstances. In his professional life, adaptability is part of a skill set that guided Altman to great wealth in his late 20s and early 30s, as he helped launch thousands of new companies as a partner and later president of the renowned startup accelerator Y Combinator. It also informs his belief that we as a species can avoid the worst of what AI could bring. \u201cSociety is capable of adapting as people are much smarter and savvier than a lot of the so-called experts think,\u201d he says. \u201cWe can manage this.\u201d\n\nThe team from OpenAI, the creator of ChatGPT \u2014 Sam Altman, left, C.E.O.; Mira Murati, C.T.O.; Greg Brockman, president; and Ilya Sutskever, chief scientist, in San Francisco, on March 13, 2023. Jim Wilson\u2014The New York Times/Redux\n\nThe evangelist preaching about risks yet plowing ahead anyway is just one of the dualities that come through in conversation with Altman. He is an outspoken advocate for AI regulation, with his own opinions about which ones should apply to his company. (\u201cYou should be skeptical of any company calling for its own regulation,\u201d he acknowledges.) He is an avowed capitalist who says he has no equity in OpenAI, structured his company to cap investors\u2019 profits, and advocates a universal basic income program to temper inequality, which many believe AI will exacerbate. He professes faith in the ability of these models to continually improve, despite the fact that he and his colleagues concede they have limited insight into how the technology will evolve. \u201cEven the people who create them don\u2019t actually know what they can and can\u2019t do,\u201d says Helen Toner, a member of OpenAI\u2019s board. \u201cI expect that it\u2019s going to be probably years before we really know all the things that GPT-4 can and can\u2019t do.\u201d\n\nHow much we\u2019re able to trust the human beings who are \u201ctuning\u201d these powerful machine algorithms\u2014both their intentions and their capabilities\u2014will be one of the great recurring questions of the coming years. In conversation with OpenAI employees across a range of departments, a recognition of AI\u2019s dangers is a near universal talking point. It\u2019s a far cry from the playbooks of tobacco, fossil-fuel, and social media executives who spent years denying possible harms before finally being forced to acknowledge reality.\n\nThe semantics start to sound a bit Orwellian. Yoon, the HR chief, says OpenAI doesn\u2019t use the term competitors, a nod to the importance of collaboration with others in the field in order to avoid bad outcomes. When asked about the AI arms race, a company spokesman objected to the metaphor, saying \u201cthe whole arms race is a bit triggering for us.\u201d\n\nOf course, it\u2019s hard to argue that OpenAI didn\u2019t play a significant role in triggering what is now unfolding in the industry. \u201cIt is a race,\u201d says Tristan Harris, the ethicist who co-founded the Center for Humane Technology, but collaboration among the major players will be key. \u201cWe need to coordinate because it\u2019s not about getting OpenAI to more safety. That wouldn\u2019t do anything because everyone else would just keep going faster.\u201d Harris is concerned that \u201cadvances in capabilities are exponential and advances in safety measures are linear,\u201d and about \u201cthe commercial incentives driving the show rather than conscious considerations about what world we want to see.\u201d\n\nAltman views the ChatGPT interface as an advance over the iPhone in its simplicity, and says it was inspired by his own love of texting as a kid. The decision to give ChatGPT a \u201chorrible,\u201d robotic name, Altman says, was a very deliberate one; he frets about the temptation to anthropomorphize AI tools, which can undermine the distinction between humans and machines. Here is another duality: ChatGPT is trained to remind users that it is incapable of having opinions. And yet ChatGPT\u2019s human qualities\u2014its conversational interface, its liberal use of the first-person pronoun\u2014were a key part of what made it go viral.\n\nDespite the massive investment from Microsoft and the shift to a profit model that, even capped, allows for 100 times returns, OpenAI still considers itself a research lab committed to its original mission of ensuring artificial general intelligence \u201cbenefits all of humanity.\u201d The company\u2019s culture is defined by it. \u201cI think it\u2019s important to push the technology as an antidote to the bad use,\u201d says chief operating officer Brad Lightcap. \u201cIf this project had started 60, 70 years ago, it probably would\u2019ve been a government-funded effort.\u201d\n\nOpenAI brought in a reported $28 million in revenue last year, which would be less than half of what a typical car dealership brings in. But Altman says he feels little immediate pressure to bring the company\u2019s commercial success to the level of its clout. Asked how much time he spends worrying about competition, he says, \u201cYou\u2019re not going to believe me on this, but almost none at all.\u201d What keeps him up at night, he says, is not surging competition from language models such as Google\u2019s LaMDA, Meta\u2019s LLaMA, and Anthropic\u2019s Claude. \u201cThis is super different than who gets a little bit more or less market share,\u201d Altman says. \u201cWe\u2019ve got to figure out how to manage this and have this go well.\u201d\n\nA few days after our visit, Altman embarked on a five-week blitz across six continents. The trip, he says, was designed to get him out of the Silicon Valley echo chamber. To some extent, it was also a victory lap\u2014and an attempt to encourage and influence global AI regulation as nation-states wake up to the power of the technology he steers. On his tour, Altman addressed the U.S. Senate, met the British and Indian Prime Ministers, opined on forthcoming E.U. AI regulation, and urged collaboration with China.\n\nOn May 24, Altman spoke at a university lecture hall in London. The queue to get in snaked down the road and around a corner. Instead of disappearing backstage after the talk, Altman hopped down into the crowd, where he was surrounded by students and journalists. He posed for selfies and gamely answered questions. After heading out through a revolving door, he had a short discussion with protesters who had come to picket, one carrying a sign that read Stop the suicide AGI race. With no bodyguard or PR handler in sight, it was a starkly different scene from earlier stage-managed tours by Mark Zuckerberg and others.\n\nAs with tech companies before his, there is some daylight between what Altman says and what happens behind the scenes. At the London event, Altman told reporters that OpenAI may decide to \u201ccease operating\u201d in the E.U. as a result of the bloc\u2019s forthcoming AI regulation. In a meeting last year with E.U. officials, which had not been previously reported, OpenAI pushed back against wording that would have required \u201cgeneral purpose\u201d AI models like ChatGPT to comply with the same rules as AI tools considered by the E.U. to be \u201chigh risk.\u201d\n\nRead More: OpenAI Lobbied E.U. to Water Down AI Regulation\n\n\n\nIn our conversations, Altman expressed deep optimism about society\u2019s ultimate ability to adjust to AI\u2019s risks. To ensure that the people you hear on the phone or see on video are who they say they are, for example, he foresees society coming to use a mix of technical and social measures, such as code words or cryptographic keys that verify identity. He sees the promise for AI to eventually do a huge amount of the mundane tasks that occupy our days, and at the same time grapple with prompts like \u201cDiscover the cure for cancer.\u201d Says Altman: \u201cThe exciting parts are almost too long to list.\u201d\n\nAnd yet, he has also mused aloud about whether OpenAI did \u201csomething really bad\u201d in creating ChatGPT. Altman has long been reported to be a doomsday prepper\u2014with guns, medicines, and gas masks at the ready. He rolls his eyes at the characterization, which he finds overblown, but adds that he does find survivalism \u201can interesting hobby.\u201d\n\n\u201cLook, if AGI goes wrong, no bunker\u2019s going to help anyone,\u201d he says, adding later: \u201cThe scary part is just sort of putting this lever into the world will for sure have unpredictable consequences.\u201d\n\nRead TIME\u2019s Full Interview With OpenAI CEO Sam Altman\n\nBuy a print of the TIME100 Companies cover featuring Sam Altman here\n\nWrite to Billy Perrigo at billy.perrigo@time.com."
    },
    {
        "metadata": {
            "title": "OpenAI said to be considering developing its own AI chips - TechCrunch",
            "description": "OpenAI said to be considering developing its own AI chips  TechCrunch",
            "published date": "Fri, 06 Oct 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8xMC8wNi9vcGVuYWktc2FpZC10by1iZS1jb25zaWRlcmluZy1kZXZlbG9waW5nLWl0cy1vd24tYWktY2hpcHMv0gFgaHR0cHM6Ly90ZWNoY3J1bmNoLmNvbS8yMDIzLzEwLzA2L29wZW5haS1zYWlkLXRvLWJlLWNvbnNpZGVyaW5nLWRldmVsb3BpbmctaXRzLW93bi1haS1jaGlwcy9hbXAv?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://techcrunch.com",
                "title": "TechCrunch"
            }
        },
        "article": "OpenAI said to be considering developing its own AI chips\n\nOpenAI, one of the best-funded AI startups in business, is exploring making its own AI chips.\n\nDiscussions of AI chip strategies within the company have been ongoing since at least last year, according to Reuters, as the shortage of chips to train AI models worsens. OpenAI is reportedly considering a number of strategies to advance its chip ambitions, including acquiring an AI chip manufacturer or mounting an effort to design chips internally.\n\nOpenAI CEO Sam Altman has made the acquisition of more AI chips a top priority for the company, Reuters reports.\n\nCurrently, OpenAI, like most of its competitors, relies on GPU-based hardware to develop models such as ChatGPT, GPT-4 and DALL-E 3. GPUs\u2019 ability to perform many computations in parallel make them well-suited to training today\u2019s most capable AI.\n\nBut the generative AI boom \u2014 a windfall for GPU makers like Nvidia \u2014 has massively strained the GPU supply chain. Microsoft is facing a shortage of the server hardware needed to run AI so severe that it might lead to service disruptions, the company warned in a summer earnings report. And Nvidia\u2019s best-performing AI chips are reportedly sold out until 2024.\n\nGPUs are also essential for running and serving OpenAI\u2019s models; the company relies on clusters of GPUs in the cloud to perform customers\u2019 workloads. But they come at a sky-high cost.\n\nAn analysis from Bernstein analyst Stacy Rasgon found that if ChatGPT queries grew to a tenth the scale of Google Search, it\u2019d require roughly $48.1 billion worth of GPUs initially and about $16 billion worth of chips a year to keep operational.\n\nOpenAI wouldn\u2019t be the first to pursue creating its own AI chips.\n\nGoogle has a processor, the TPU (short for \u201ctensor processing unit\u201d), to train large generative AI systems like PaLM-2 and Imagen. Amazon offers proprietary chips to AWS customers both for training (Trainium) and inferencing (Inferentia). And Microsoft, reportedly, is working with AMD to develop an in-house AI chip called Athena, which OpenAI is said to be testing.\n\nCertainly, OpenAI is in a strong position to invest heavily in R&D. The company, which has raised more than $11 billion in venture capital, is nearing $1 billion in annual revenue. And it\u2019s considering a share sale that could see its secondary-market valuation soar to $90 billion, according to a recent Wall Street Journal report.\n\nBut hardware is an unforgiving business \u2014 particularly AI chips.\n\nLast year, AI chipmaker Graphcore, which allegedly had its valuation slashed by $1 billion after a deal with Microsoft fell through, said that it was planning to job cuts due to the \u201cextremely challenging\u201d macroeconomic environment. (The situation grew more dire over the past few months as Graphcore reported falling revenue and increased losses.) Meanwhile, Habana Labs, the Intel-owned AI chip company, laid off an estimated 10% of its workforce. And Meta\u2019s custom AI chip efforts have been beset with issues, leading the company to scrap some its experimental hardware.\n\nEven if OpenAI commits to bringing a custom chip to market, such an effort could take years and cost hundreds of millions of dollars annually. It remains to be seen if the startup\u2019s investors, one of which is Microsoft, have the appetite for such a risky bet."
    },
    {
        "metadata": {
            "title": "OpenAI's CEO Says the Age of Giant AI Models Is Already Over - WIRED",
            "description": "OpenAI's CEO Says the Age of Giant AI Models Is Already Over  WIRED",
            "published date": "Mon, 17 Apr 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiXWh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9vcGVuYWktY2VvLXNhbS1hbHRtYW4tdGhlLWFnZS1vZi1naWFudC1haS1tb2RlbHMtaXMtYWxyZWFkeS1vdmVyL9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.wired.com",
                "title": "WIRED"
            }
        },
        "article": "The stunning capabilities of ChatGPT, the chatbot from startup OpenAI, has triggered a surge of new interest and investment in artificial intelligence. But late last week, OpenAI\u2019s CEO warned that the research strategy that birthed the bot is played out. It's unclear exactly where future advances will come from.\n\nOpenAI has delivered a series of impressive advances in AI that works with language in recent years by taking existing machine-learning algorithms and scaling them up to previously unimagined size. GPT-4, the latest of those projects, was likely trained using trillions of words of text and many thousands of powerful computer chips. The process cost over $100 million.\n\nBut the company\u2019s CEO, Sam Altman, says further progress will not come from making models bigger. \u201cI think we're at the end of the era where it's going to be these, like, giant, giant models,\u201d he told an audience at an event held at MIT late last week. \u201cWe'll make them better in other ways.\u201d\n\nAltman\u2019s declaration suggests an unexpected twist in the race to develop and deploy new AI algorithms. Since OpenAI launched ChatGPT in November, Microsoft has used the underlying technology to add a chatbot to its Bing search engine, and Google has launched a rival chatbot called Bard. Many people have rushed to experiment with using the new breed of chatbot to help with work or personal tasks.\n\nMeanwhile, numerous well-funded startups, including Anthropic, AI21, Cohere, and Character.AI, are throwing enormous resources into building ever larger algorithms in an effort to catch up with OpenAI\u2019s technology. The initial version of ChatGPT was based on a slightly upgraded version of GPT-3, but users can now also access a version powered by the more capable GPT-4.\n\nAltman\u2019s statement suggests that GPT-4 could be the last major advance to emerge from OpenAI\u2019s strategy of making the models bigger and feeding them more data. He did not say what kind of research strategies or techniques might take its place. In the paper describing GPT-4, OpenAI says its estimates suggest diminishing returns on scaling up model size. Altman said there are also physical limits to how many data centers the company can build and how quickly it can build them."
    },
    {
        "metadata": {
            "title": "A new collaboration with OpenAI charts the future of AI in higher education | ASU News - ASU News Now",
            "description": "A new collaboration with OpenAI charts the future of AI in higher education | ASU News  ASU News Now",
            "published date": "Thu, 18 Jan 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiaGh0dHBzOi8vbmV3cy5hc3UuZWR1LzIwMjQwMTE4LXVuaXZlcnNpdHktbmV3cy1uZXctY29sbGFib3JhdGlvbi1vcGVuYWktY2hhcnRzLWZ1dHVyZS1haS1oaWdoZXItZWR1Y2F0aW9u0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://news.asu.edu",
                "title": "ASU News Now"
            }
        },
        "article": "Repeatedly recognized as a leader in innovation, research and global impact, Arizona State University is continuing its mission to chart the future when it comes to artificial intelligence.\n\nToday, the university announced it has become the first higher education institution to collaborate with OpenAI, the AI research and deployment company behind ChatGPT.\n\nChatGPT was unveiled to the public just over a year ago, and since that launch, organizations' adoption of generative AI tools has surged.\n\n\u201cResearch shows that nearly two-thirds of organizations are already actively exploring the integration of AI,\u201d said ASU Chief Information Officer Lev Gonick. \u201cBy providing access to advanced AI capabilities, these tools are leveling the playing field, allowing individuals and organizations \u2014 regardless of size or resources \u2014 to harness the power of AI for creative and innovative endeavors.\u201d\n\nThe collaboration between ASU and OpenAI brings the advanced capabilities of ChatGPT Enterprise into higher education, setting a new precedent for how universities enhance learning, creativity and student outcomes.\n\n\u201cASU recognizes that augmented and artificial intelligence systems are here to stay, and we are optimistic about their ability to become incredible tools that help students to learn, learn more quickly and understand subjects more thoroughly,\u201d ASU President Michael M. Crow said. \u201cOur collaboration with OpenAI reflects our philosophy and our commitment to participating directly to the responsible evolution of AI learning technologies.\u201d\n\nStarting in February, ASU will invite submissions from faculty and staff to implement the innovative uses of ChatGPT Enterprise. The three key areas of concentration include: enhancing student success, forging new avenues for innovative research and streamlining organizational processes.\n\n\u201cThe goal is to leverage our knowledge core here at ASU to develop AI-driven projects aimed at revolutionizing educational techniques, aiding scholarly research and boosting administrative efficiency,\u201d Gonick said.\n\nThe platform prioritizes user privacy, employing enterprise-grade security measures to safeguard user data. These measures are meticulously designed to protect against digital threats, providing a secure environment to utilize the platform's functionalities.\n\n\"Learning is core to why so many users love ChatGPT. ASU continues to lead in innovation by integrating ChatGPT into its educational programs,\u201d OpenAI Chief Operating Officer Brad Lightcap said. \u201cWe\u2019re keen to learn from ASU and to work towards expanding ChatGPT's impact in higher education.\u201d\n\nThe collaboration builds on ASU\u2019s commitment to exploring AI in all forms.\n\nFor example, ASU\u2019s Knowledge Enterprise \u2013 which leads the university\u2019s groundbreaking research activity \u2013 has 19 centers, initiatives and laboratories dedicated to exploring and activating AI models, resulting in over $340M in active awards.\n\nLast year, the university announced the launch of AI Acceleration, a new team of technologists dedicated to creating the next generation of AI tools. The collaboration with OpenAI will empower new solutions being developed as part of this team\u2019s efforts.\n\n\u201cIf last year was considered to be generative AI\u2019s breakout year, then 2024 will be a time for meaningful practice and exploration to leverage the true power of this technology,\u201d Gonick said."
    },
    {
        "metadata": {
            "title": "OpenAI holds back public release of tech that can clone someone's voice in 15 seconds due to safety concerns - Fortune",
            "description": "OpenAI holds back public release of tech that can clone someone's voice in 15 seconds due to safety concerns  Fortune",
            "published date": "Sat, 30 Mar 2024 00:24:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiUGh0dHBzOi8vZm9ydHVuZS5jb20vMjAyNC8wMy8yOS9vcGVuYWktdGVjaC1jbG9uZS1zb21lb25lcy12b2ljZS1zYWZldHktY29uY2VybnMv0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://fortune.com",
                "title": "Fortune"
            }
        },
        "article": "ChatGPT-maker OpenAI is getting into the voice assistant business and showing off new technology that can clone a person\u2019s voice, but says it won\u2019t yet release it publicly due to safety concerns.\n\nThe artificial intelligence company unveiled its new Voice Engine technology Friday, just over a week after filing a trademark application for the name. The company claims that it can recreate a person\u2019s voice with just 15 seconds of recording of that person talking.\n\nOpenAI says it plans to preview it with early testers \u201cbut not widely release this technology at this time\u201d because of the dangers of misuse.\n\n\u201cWe recognize that generating speech that resembles people\u2019s voices has serious risks, which are especially top of mind in an election year,\u201d the San Francisco company said in a statement.\n\nIn New Hampshire, authorities are investigating robocalls sent to thousands of voters just before the presidential primary that featured an AI-generated voice mimicking President Joe Biden.\n\nA number of startup companies already sell voice-cloning technology, some of which is accessible to the public or for select business customers such as entertainment studios.\n\nOpenAI says early Voice Engine testers have agreed to not impersonate a person without their consent and to disclose that the voices are AI-generated. The company, best known for its chatbot and the image-generator DALL-E, took a similar approach in announcing but not widely releasing its video-generator Sora.\n\nHowever a trademark application filed on March 19 shows that OpenAI likely aims to get into the business of speech recognition and digital voice assistant. Eventually, improving such technology could help OpenAI compete with the likes of other voice products such as Amazon\u2019s Alexa."
    },
    {
        "metadata": {
            "title": "OpenAI strikes \u2018first of its kind\u2019 deal with publisher Axel Springer to use its news content in ChatGPT - CNN",
            "description": "OpenAI strikes \u2018first of its kind\u2019 deal with publisher Axel Springer to use its news content in ChatGPT  CNN",
            "published date": "Wed, 13 Dec 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiTGh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8xMi8xMy90ZWNoL29wZW4tYWktYXhlbC1zcHJpbmdlci1jaGF0Z3B0L2luZGV4Lmh0bWzSAUVodHRwczovL2FtcC5jbm4uY29tL2Nubi8yMDIzLzEyLzEzL3RlY2gvb3Blbi1haS1heGVsLXNwcmluZ2VyLWNoYXRncHQ?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnn.com",
                "title": "CNN"
            }
        },
        "article": "New York CNN \u2014\n\nOpenAI will pay publishing giant Axel Springer to use its news content in the company\u2019s artificial intelligence products, marking a \u201cfirst of its kind\u201d global publishing deal that will allow the ChatGPT creator to train its AI models on the news organization\u2019s reporting.\n\nAs part of the deal, ChatGPT users will receive summaries of news stories from Axel Springer\u2019s brands, including Politico, Business Insider, Bild and Welt, with attribution and links to the original sources of reporting, the companies said Wednesday. The agreement will allow OpenAI\u2019s models to take advantage of the publisher\u2019s higher quality and more current information in its chatbots\u2019 answers.\n\n\u201cWe are excited to have shaped this global partnership between Axel Springer and OpenAI \u2014 the first of its kind,\u201d Mathias D\u00f6pfner, chief executive of Axel Springer, said in a statement. \u201cWe want to explore the opportunities of AI empowered journalism \u2014 to bring quality, societal relevance and the business model of journalism to the next level.\u201d\n\nThe financial terms of the deal were not disclosed. The agreement was first reported by The Wall Street Journal.\n\n\u201cThis partnership with Axel Springer will help provide people with new ways to access quality, real-time news content through our AI tool,\u201d said Brad Lightcap, chief operating officer of OpenAI, in a statement. \u201cWe are deeply committed to working with publishers and creators around the world and ensuring they benefit from advanced AI technology and new revenue models.\u201d\n\nSince ChatGPT took the world by storm last year, becoming one of the fastest growing consumer applications in history with an estimated 100 million active users, the chatbot has raised alarms over its promotion of misinformation, at times disseminating fake and false assertions in its responses.\n\nA handful of online news outlets have also attempted to harness the nascent technology in their reporting, at times to disastrous results, resulting in embarrassing public corrections and apologies. Despite the early missteps, AI appears poised to upend the news and publishing industry.\n\nThe ChatGPT maker\u2019s agreement with Axel Springer comes one day after The New York Times announced the creation of a new newsroom leader position focused on artificial intelligence initiatives, signaling the key role the rapidly advancing technology will play in news production.\n\nEarlier this year, OpenAI also announced an agreement with The Associated Press to license the news collective\u2019s reporting archive."
    },
    {
        "metadata": {
            "title": "Sam Altman returns to OpenAI in a bizarre reversal of fortunes - CNN",
            "description": "Sam Altman returns to OpenAI in a bizarre reversal of fortunes  CNN",
            "published date": "Wed, 22 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiTWh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8xMS8yMi90ZWNoL29wZW5haS1hbHRtYW4tcmV0dXJucy1obmstaW50bC9pbmRleC5odG1s0gFGaHR0cHM6Ly9hbXAuY25uLmNvbS9jbm4vMjAyMy8xMS8yMi90ZWNoL29wZW5haS1hbHRtYW4tcmV0dXJucy1obmstaW50bA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnn.com",
                "title": "CNN"
            }
        },
        "article": "New York CNN \u2014\n\nSam Altman has agreed to return to lead OpenAI, the company said in a Tuesday post on X, just days after his surprise ouster as chief executive sparked an employee revolt that threatened to undermine what has been the leading company in the fledgling artificial intelligence industry.\n\n\u201cWe have reached an agreement in principle for Sam Altman to return to OpenAI as CEO with a new initial board,\u201d the company said, adding that the board will be chaired by Bret Taylor, a former co-CEO of Salesforce. Former Treasury Secretary Larry Summers will also join the board, alongside existing director, Quora CEO Adam D\u2019Angelo.\n\n\u201cWe are collaborating to figure out the details,\u201d it said.\n\nIn his own post on X, formerly Twitter, Altman wrote that he is \u201clooking forward\u201d to returning to OpenAI and building on the firm\u2019s \u201cstrong partnership\u201d with Microsoft, which is the ChatGPT maker\u2019s biggest financial backer.\n\nThe announcement appears to bring to an end days of chaos for the AI industry that included negotiations over who should lead OpenAI and how the firm should be run, as well as broader discussions about just how fast the arms race to develop AI technology should be moving.\n\nVideo Ad Feedback 'The only time I ever froze:' AI pioneer describes encounter with Steve Jobs (October 2023) 01:17 - Source: CNN\n\nThe details of Altman\u2019s firing and re-hiring remain murky. In its announcement Friday, OpenAI claimed that Altman had been insufficiently \u201ccandid\u201d with the board.\n\nThat ambiguous language sent the rumor mill flying. But a key factor in Altman\u2019s ouster was the presence of tensions between Altman, who favored pushing AI development more aggressively, and members of the original OpenAI board, who wanted to move more cautiously, according to CNN contributor Kara Swisher, who spoke to sources knowledgeable about the crisis.\n\nAs of Monday morning, Nadella had announced that Altman, along with fellow OpenAI co-founder Greg Brockman, would be joining Microsoft to lead a new AI research division. OpenAI said it had hired former Twitch chief Emmett Shear as interim CEO.\n\nBut then hundreds of OpenAI employees, nearly the company\u2019s entire staff, threatened to leave, potentially for Microsoft, if the company\u2019s board didn\u2019t resign and reinstate Altman as CEO.\n\nIt\u2019s unclear how Shear will be affected by Altman\u2019s return. Posting on X, Shear wrote: \u201cI am deeply pleased by this result, after (some) 72 very intense hours of work \u2026 I\u2019m glad to have been a part of the solution.\u201d\n\nBrockman is also returning to OpenAI, according to his post on X.\n\nUltimately, Microsoft and Altman appear to be the big winners from the dust-up: Altman will continue leading the firm he helped to found, now with a board that is, in theory, more supportive of his vision.\n\nAnd Microsoft has wrested more control over the company it invested billions in to help bolster its ambitions in developing AI, which many in Silicon Valley think will be the most important wave of technological advancement in the coming decades.\n\n\u201cWe are encouraged by the changes to the OpenAI board,\u201d Microsoft CEO Satya Nadella said on X. \u201cWe believe this is a first essential step on a path to more stable, well-informed, and effective governance.\u201d\n\nAltman\u2019s vision to quickly roll out and commercialize AI tools also appears to have won out.\n\nPublicly, Altman has long cautioned about risks posed by AI, and he has pledged to lawmakers and customers that he would move OpenAI forward responsibly.\n\n\u201cIs [AI] gonna be like the printing press that diffused knowledge, power and learning widely across the landscape that empowered ordinary, everyday individuals that led to greater flourishing, that led above all to greater liberty?\u201d he said in a May Senate subcommittee hearing pressing for regulation. \u201cOr is it gonna be more like the atom bomb \u2014 huge technological breakthrough, but the consequences (severe, terrible) continue to haunt us to this day?\u201d\n\nBut inside the company, Altman had been pushing to bring products to market more quickly and to sell them for a profit.\n\nAltman announced a few weeks ago at OpenAI\u2019s first-ever developer day that the company would make tools available so anyone could create their own version of ChatGPT. OpenAI has also worked with Microsoft to roll out ChatGPT-like technology across Microsoft\u2019s products.\n\nOpenAI and iPhone designer Jony Ive had also reportedly been in talks to raise $1 billion from Japanese conglomerate SoftBank for an AI device to replace the smartphone.\n\n\u2014 CNN\u2019s Juliana Liu and Diksha Madhok contributed reporting."
    },
    {
        "metadata": {
            "title": "OpenAI is adding new watermarks to DALL-E 3 - The Verge",
            "description": "OpenAI is adding new watermarks to DALL-E 3  The Verge",
            "published date": "Tue, 06 Feb 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiWmh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDI0LzIvNi8yNDA2Mzk1NC9haS13YXRlcm1hcmtzLWRhbGxlMy1vcGVuYWktY29udGVudC1jcmVkZW50aWFsc9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "OpenAI\u2019s image generator DALL-E 3 will add watermarks to image metadata as more companies roll out support for standards from the Coalition for Content Provenance and Authenticity (C2PA).\n\nThe company says watermarks from C2PA will appear in images generated on the ChatGPT website and the API for the DALL-E 3 model. Mobile users will get the watermarks by February 12th. They\u2019ll include both an invisible metadata component and a visible CR symbol, which will appear in the top left corner of each image.\n\nPeople can check the provenance \u2014 which AI tool was used to make the content \u2014 of any image generated by OpenAI\u2019s platforms through websites like Content Credentials Verify. So far, only still images, not videos or text, can carry the watermark.\n\nNew watermarks on DALL-E 3 images. Image: OpenAI\n\nOpenAI says adding the watermark metadata to images represents a \u201cnegligible effect on latency and will not affect the quality of the image generation.\u201d It will also increase image sizes slightly for some tasks.\n\nThe C2PA, a group consisting of companies like Adobe and Microsoft, has been pushing the use of the Content Credentials watermark to identify the provenance of content and show if it was made by humans or with AI. Adobe created a Content Credentials symbol, which OpenAI is adding to DALL-E 3 creations. Meta recently announced it will add tags to AI-generated content on its social media platforms.\n\nIdentifying AI-generated content is one of the flagship directives in the Biden administration\u2019s executive order on AI. But watermarking is not a surefire way to stop misinformation. OpenAI points out that C2PA\u2019s metadata can \u201ceasily be removed either accidentally or intentionally,\u201d especially as most social media platforms often remove metadata from uploaded content. Taking a screenshot omits the metadata."
    },
    {
        "metadata": {
            "title": "The New York Times blocks OpenAI's web crawler - The Verge",
            "description": "The New York Times blocks OpenAI's web crawler  The Verge",
            "published date": "Mon, 21 Aug 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiVGh0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDIzLzgvMjEvMjM4NDA3MDUvbmV3LXlvcmstdGltZXMtb3BlbmFpLXdlYi1jcmF3bGVyLWFpLWdwdNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "The New York Times has blocked OpenAI\u2019s web crawler, meaning that OpenAI can\u2019t use content from the publication to train its AI models. If you check the NYT\u2019s robots.txt page, you can see that the NYT disallows GPTBot, the crawler that OpenAI introduced earlier this month. Based on the Internet Archive\u2019s Wayback Machine, it appears NYT blocked the crawler as early as August 17th."
    },
    {
        "metadata": {
            "title": "How OpenAI so royally screwed up the Sam Altman firing - CNN",
            "description": "How OpenAI so royally screwed up the Sam Altman firing  CNN",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiTmh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8xMS8xOS90ZWNoL3NhbS1hbHRtYW4tb3Blbi1haS1maXJpbmctYm9hcmQvaW5kZXguaHRtbNIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnn.com",
                "title": "CNN"
            }
        },
        "article": "New York CNN \u2014\n\nOpenAI\u2019s overseers worried that the company was making the technological equivalent of a nuclear bomb, and its caretaker, Sam Altman, was moving so fast that he risked a global catastrophe.\n\nSo the board fired him. That may ultimately have been the logical solution.\n\nBut the manner in which Altman was fired \u2013 abruptly, opaquely and without warning to some of OpenAI\u2019s largest stakeholders and partners \u2013 defied logic. And it risked inflicting more damage than if the board took no such action at all.\n\nA company\u2019s board of directors has an obligation, first and foremost, to its shareholders. OpenAI\u2019s most important shareholder is Microsoft, the company that gave Altman & Co. $13 billion to help Bing, Office, Windows and Azure leapfrog Google and stay ahead of Amazon, IBM and other AI wannabes.\n\nYet Microsoft was not informed of Altman\u2019s firing until \u201cjust before\u201d the public announcement, according to CNN contributor Kara Swisher, who spoke to sources knowledgeable about the board\u2019s ousting of its CEO. Microsoft\u2019s stock sank after Altman was let go.\n\nEmployees weren\u2019t told the news ahead of time, either. Neither was Greg Brockman, the company\u2019s co-founder and former president, who said in a post on X that he found out about Altman\u2019s firing moments before it happened. Brockman, a key supporter of Altman and his strategic leadership of the company, resigned Friday. Other Altman loyalists also headed for the exits.\n\nSuddenly, OpenAI was in crisis. Reports that Altman and ex-OpenAI loyalists were about to start their own venture risked undoing everything that the company had worked so hard to achieve over the past several years.\n\nSo a day later, the board reportedly asked for a mulligan and tried to woo Altman back. It was a shocking turn of events and an embarrassing self-own by a company that its widely regarded as the most promising producer of the most exciting new technology.\n\nStrange board structure\n\nThe bizarre structure of OpenAI\u2019s board complicated matters.\n\nThe company is a nonprofit. But Altman, Brockman and Chief Scientist Ilya Sutskever in 2019 formed OpenAI LP, a for-profit entity that exists within the larger company\u2019s structure. That for-profit company took OpenAI from worthless to a valuation of $90 billion in just a few years \u2013 and Altman is largely credited as the mastermind of that plan and the key to the company\u2019s success.\n\nYet a company with big backers like Microsoft and venture capital firm Thrive Capital has an obligation to grow its business and make money. Investors want to ensure they\u2019re getting bang for their buck, and they\u2019re not known to be a patient bunch.\n\nThat probably led Altman to push the for-profit company to innovate faster and go to market with products. In the great \u201cmove fast and break things\u201d tradition of Silicon Valley, those products don\u2019t always work so well at first.\n\nThat\u2019s fine, perhaps, when it\u2019s a dating app or a social media platform. It\u2019s something entirely different when it\u2019s a technology so good at mimicking human speech and behavior that it can fool people into believing its fake conversations and images are real.\n\nAnd that\u2019s what reportedly scared the company\u2019s board, which remained majority controlled by the nonprofit wing of the company. Swisher reported that OpenAI\u2019s recent developer conference served as an inflection point: Altman announced that OpenAI would make tools available so anyone could create their own version of ChatGPT.\n\nFor Sutskever and the board, that was a step too far.\n\nA warning not without merit\n\nVideo Ad Feedback 'The only time I ever froze:' AI pioneer describes encounter with Steve Jobs (October 2023) 01:17 - Source: CNN\n\nBy Altman\u2019s own account, the company was playing with fire.\n\nWhen Altman set up OpenAI LP four years ago, the new company noted in its charter that it remained \u201cconcerned\u201d about AI\u2019s potential to \u201ccause rapid change\u201d for humanity. That could happen unintentionally, with the technology performing malicious tasks because of bad code \u2013 or intentionally by people subverting AI systems for evil purposes. So the company pledged to prioritize safety \u2013 even if that meant reducing profit for its stakeholders.\n\nAltman also urged regulators to set limits on AI to prevent people like him from inflicting serious damage on society.\n\nProponents of AI believe the technology has the potential to revolutionize every industry and better humanity in the process. It has the potential to improve education, finance, agriculture and health care.\n\nBut it also has the potential to take jobs away from people \u2013 14 million positions could disappear in the next five years, the World Economic Forum warned in April. AI is particularly adept at spreading harmful disinformation. And some, including former OpenAI board member Elon Musk, fear the technology will surpass humanity in intelligence and could wipe out life on the planet.\n\nNot how to handle a crisis\n\nWith those threats \u2013 real or perceived \u2013 it\u2019s no wonder the board was concerned that Altman was moving at too rapid of a pace. It may have felt obligated to get rid of him and replace him with someone who, in their view, would be more careful with the potentially dangerous technology.\n\nBut OpenAI isn\u2019t operating in a vacuum. It has stakeholders, some of them with billions poured into the company. And the so-called adults in the room were acting, as Swisher put it: like a \u201cclown car that crashed into a gold mine,\u201d quoting a famous Meta CEO Mark Zuckerberg line about Twitter.\n\nInvolving Microsoft in the decision, informing employees, working with Altman on a dignified exit plan\u2026all of those would have been solutions more typically employed by a board of a company OpenAI\u2019s size \u2013 and all with potentially better outcomes.\n\nMicrosoft, despite its massive stake, does not hold an OpenAI board seat, because of the company\u2019s strange structure. Now that could change, according to multiple news reports, including the Wall Street Journal and New York Times. One of the company\u2019s demands, including the return of Altman, is to have a seat at the table.\n\nWith OpenAI\u2019s ChatGPT-like capabilities embedded in Bing and other core products, Microsoft believed it had invested wisely in the promising new tech of the future. So it must have come as a shock to CEO Satya Nadella and his crew when they learned about Altman\u2019s firing along with the rest of the world on Friday evening.\n\nThe board angered a powerful ally and could be forever changed because of the way it handled Altman\u2019s ouster. It could end up with Altman back at the helm, a for-profit company on its nonprofit board \u2013 and a massive culture shift at OpenAI.\n\nAlternatively, it could become a competitor to Altman, who may ultimately decide to start a new company and drain talent from OpenAI.\n\nEither way, OpenAI is probably left off in a worse position now than it was in on Friday before it fired Altman. And it was a problem it could have avoided, ironically, by slowing down."
    },
    {
        "metadata": {
            "title": "OpenAI Is in Talks to Raise New Funding at Valuation of $100 Billion or More - Bloomberg",
            "description": "OpenAI Is in Talks to Raise New Funding at Valuation of $100 Billion or More  Bloomberg",
            "published date": "Fri, 22 Dec 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMicGh0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDIzLTEyLTIyL29wZW5haS1pbi10YWxrcy10by1yYWlzZS1uZXctZnVuZGluZy1hdC0xMDAtYmlsbGlvbi12YWx1YXRpb27SAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bloomberg.com",
                "title": "Bloomberg"
            }
        },
        "article": "OpenAI is in early discussions to raise a fresh round of funding at a valuation at or above $100 billion, people with knowledge of the matter said, a deal that would cement the ChatGPT maker as one of the world\u2019s most valuable startups.\n\nInvestors potentially involved in the fundraising round have been included in preliminary discussions, according to the people, who asked not to be identified to discuss private matters. Details like the terms, valuation and timing of the funding round haven\u2019t yet been finalized and could still change, the people said."
    },
    {
        "metadata": {
            "title": "OpenAI Is Working With US Military on Cybersecurity Tools - Bloomberg",
            "description": "OpenAI Is Working With US Military on Cybersecurity Tools  Bloomberg",
            "published date": "Tue, 16 Jan 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMidmh0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDI0LTAxLTE2L29wZW5haS13b3JraW5nLXdpdGgtdXMtbWlsaXRhcnktb24tY3liZXJzZWN1cml0eS10b29scy1mb3ItdmV0ZXJhbnPSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bloomberg.com",
                "title": "Bloomberg"
            }
        },
        "article": "OpenAI is working with the Pentagon on a number of projects including cybersecurity capabilities, a departure from the startup\u2019s earlier ban on providing its artificial intelligence to militaries.\n\nThe ChatGPT maker is developing tools with the US Defense Department on open-source cybersecurity software \u2014 collaborating with DARPA for its AI Cyber Challenge announced last year \u2014 and has had initial talks with the US government about methods to assist with preventing veteran suicide, Anna Makanju , the company\u2019s vice president of global affairs, said in an interview at Bloomberg House at the World Economic Forum in Davos on Tuesday."
    },
    {
        "metadata": {
            "title": "Sam Altman Returns as OpenAI CEO. Here's How It Happened | TIME - TIME",
            "description": "Sam Altman Returns as OpenAI CEO. Here's How It Happened | TIME  TIME",
            "published date": "Wed, 22 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiO2h0dHBzOi8vdGltZS5jb20vNjMzODc4OS9zYW0tYWx0bWFuLW9wZW5haS1yZXR1cm4tdGltZWxpbmUv0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://time.com",
                "title": "TIME"
            }
        },
        "article": "Over five chaotic days that transfixed Silicon Valley and beyond, the world\u2019s leading artificial intelligence company, OpenAI, appeared to be on the verge of imploding in a power struggle.\n\nThe maker of ChatGPT, the sensational chatbot, had a mission to safely develop smarter-than-human AI. But that mission looked in jeopardy on Friday when OpenAI\u2019s non-profit board of directors fired Altman, suggesting he had been dishonest in his communications with them. To many spectators, the future of not just AI but also humanity hung in the balance. Finally late on Tuesday night, after three CEO changes and a full-court press by Microsoft, the board gave Altman his old job back.\n\nAltman\u2019s return had been the subject of days of intense negotiations inside the company, outside pressure from OpenAI\u2019s biggest investor Microsoft, and a threat by nearly all of OpenAI\u2019s employees to quit if he were not reinstated.\n\nNeither Altman nor Greg Brockman, the company\u2019s president who was also removed from the board last week, will immediately retake their seats on the decision-making board. Ilya Sutskever, OpenAI\u2019s chief scientist who delivered Altman the news he was being fired, will reportedly step down from the board but remain at the company. The board\u2019s new chair is Bret Taylor, the former co-CEO of Salesforce. Also on the new board are Larry Summers, the former Treasury Secretary, and Quora CEO Adam D\u2019Angelo, the only remaining member of the board that fired Altman. It is unclear whether more members will be added subject to further negotiations. Satya Nadella, the CEO of Microsoft, called the changes to the board a \u201cfirst essential step on a path to more stable, well-informed, and effective governance.\u201d\n\nOpenAI did not immediately respond to a request for comment.\n\nMore From TIME\n\nAs part of the deal, OpenAI will reportedly retain its legal structure: a capped-profit company overseen by a non-profit board that has the legal freedom to take decisions that may not align with the interests of investors. The unusual set-up allowed the board to make its shock decision without informing Microsoft, which had invested more than $11 billion into the company. Altman had established this structure in 2019 to help OpenAI raise more capital while mitigating the perceived risks to humanity of corporate control over artificial intelligence.\n\nHere\u2019s a recap of OpenAI\u2019s rollercoaster week.\n\nThe crisis at OpenAI began on Friday last week, when Altman joined a video conference call with the board, minus Brockman. Sutskever informed Altman he was being fired and that the news would shortly be announced, according to a tweet written by Brockman. Sutskever informed Brockman shortly after that he was also being removed from the board, but invited him to remain at the company.\n\nOpenAI then published a blog post on its website announcing the firing. \u201cMr. Altman\u2019s departure follows a deliberative review process by the board, which concluded that he was not consistently candid in his communications with the board, hindering its ability to exercise its responsibilities,\u201d the post said. \u201cThe board no longer has confidence in his ability to continue leading OpenAI.\u201d The board said it had appointed chief technology officer Mira Murati as interim CEO.\n\nWhile speculation began to swirl about what the board meant by \u201cnot consistently candid in his communications,\u201d the board declined to share further information about how or why it had come to its decision. Brockman announced on Twitter that he was resigning from the company in protest at the Board\u2019s firing of Altman, and employee sentiment at OpenAI quickly turned against the board, along with a significant portion of the tech-focused chatter on X, formerly Twitter.\n\nInvestors, too, were angry. Microsoft was reportedly only informed of the board\u2019s decision to fire Altman one minute before the blog post was published. Nadella quickly began leading efforts to have the board reinstate Altman at the company, backed up by other OpenAI investors Thrive Capital, Khosla Ventures and Tiger Global Management, according to Bloomberg.\n\nBy Saturday, as dozens of OpenAI employees met for talks at Altman\u2019s San Francisco mansion, news had emerged that Altman and Brockman were already pitching a new AI company to investors.\n\nAs talks to bring back Altman and Brockman continued late into Sunday at OpenAI\u2019s offices in San Francisco\u2019s Mission District, they appeared to hit an impasse. Late Sunday night, the board named former Twitch chief executive Emmett Shear as OpenAI\u2019s new CEO, replacing Murati, an ally of Altman. Then, Nadella announced that Microsoft would hire Altman, Brockman and other OpenAI staff into a new independent AI unit inside Microsoft, with Altman as CEO.\n\nOpenAI employees, who over the weekend had been engaged in a coordinated show of support for Altman and Brockman on X, began to tweet the same phrase suggesting they might leave en-masse: \u201cOpenAI is nothing without its people.\u201d On Monday, more than 700 of the company\u2019s roughly 800 employees signed their name to an open letter calling on the board to resign, and threatening to jump ship to Microsoft. Shortly after that, Microsoft further increased the pressure, announcing it would offer any OpenAI employee that wanted them roles within this unit for similar compensation.\n\nAt the time he was fired, Altman was in talks with investors to sell employee shares in OpenAI at a valuation north of $80 billion, triple its value just six months ago. The deal\u2014the future of which is now uncertain\u2014could have made many of the company\u2019s employees and executives, who are routinely offered equity as part of their compensation packages, extremely wealthy.\n\nThe crisis continued into Monday\u2014the first day of a week that was meant to be a company-wide vacation for Thanksgiving, to recognize the amount of work OpenAI employees had put in over a blockbuster year. Instead, OpenAI\u2019s offices were the center of frantic activity.\n\nSutskever, OpenAI\u2019s chief scientist who fired Altman, made a surprise about-face. \u201cI deeply regret my participation in the board's actions,\u201d he tweeted. \u201cI never intended to harm OpenAI. I love everything we've built together and I will do everything I can to reunite the company.\u201d The change of heart reportedly came after a tearful meeting with Brockman\u2019s wife, Anna, and pressure from significant numbers of company staff.\n\nBy late Tuesday, the effort to reinstate Altman as OpenAI\u2019s CEO was complete. Part of the deal included Altman agreeing to an internal investigation into the alleged lack of candor with the board that led to his firing, the Information reported.\n\nEmployees celebrated the news with a party at OpenAI\u2019s offices on Tuesday night. Brockman, who returns as president, posted a selfie to X of himself smiling, accompanied by dozens of happy OpenAI employees. \u201cWe are so back,\u201d the caption said."
    },
    {
        "metadata": {
            "title": "Sarah Silverman Sues OpenAI and Meta Over Copyright Infringement - The New York Times",
            "description": "Sarah Silverman Sues OpenAI and Meta Over Copyright Infringement  The New York Times",
            "published date": "Mon, 10 Jul 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiUGh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjMvMDcvMTAvYXJ0cy9zYXJhaC1zaWx2ZXJtYW4tbGF3c3VpdC1vcGVuYWktbWV0YS5odG1s0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "The comedian Sarah Silverman has joined a class-action lawsuit against OpenAI and another against Meta accusing the companies of copyright infringement, saying they \u201ccopied and ingested\u201d her protected work in order to train their artificial intelligence programs, according to court papers.\n\nThe lawsuits, in which she joined the authors Christopher Golden and Richard Kadrey, were filed Friday in the San Francisco Division of the U.S. District Court of the Northern District of California. Each suit says that the company in question made copies of the authors\u2019 works, including Silverman\u2019s memoir, \u201cThe Bedwetter,\u201d without permission by scraping illegal online \u201cshadow libraries\u201d that contain the texts of thousands of books.\n\nThe lawsuit against Meta cites the company\u2019s own research paper about LLaMA, the large-language model it uses to train chatbots. According to the paper, made public in February, scientists included text from The Pile within their training dataset; the lawsuit says some of that text comes from shadow libraries.\n\n\u201cTheir copyrighted materials were copied and ingested as part of training,\u201d the lawsuit claims. \u201cMany of the plaintiffs\u2019 books appear in the dataset that Meta admitted to using.\u201d"
    },
    {
        "metadata": {
            "title": "OpenAI Courts Hollywood in Meetings With Film Studios, Directors - Bloomberg",
            "description": "OpenAI Courts Hollywood in Meetings With Film Studios, Directors  Bloomberg",
            "published date": "Fri, 22 Mar 2024 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMicmh0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDI0LTAzLTIyL29wZW5haS1jb3VydHMtaG9sbHl3b29kLWluLW1lZXRpbmdzLXdpdGgtZmlsbS1zdHVkaW9zLWRpcmVjdG9yc9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bloomberg.com",
                "title": "Bloomberg"
            }
        },
        "article": "OpenAI wants to break into the movie business.\n\nThe artificial intelligence startup has scheduled meetings in Los Angeles next week with Hollywood studios, media executives and talent agencies to form partnerships in the entertainment industry and encourage filmmakers to integrate its new AI video generator into their work, according to people familiar with the matter."
    },
    {
        "metadata": {
            "title": "OpenAI's Sora Turns AI Prompts Into Photorealistic Videos - WIRED",
            "description": "OpenAI's Sora Turns AI Prompts Into Photorealistic Videos  WIRED",
            "published date": "Thu, 15 Feb 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiPGh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9vcGVuYWktc29yYS1nZW5lcmF0aXZlLWFpLXZpZGVvL9IBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.wired.com",
                "title": "WIRED"
            }
        },
        "article": "We already know that OpenAI\u2019s chatbots can pass the bar exam without going to law school. Now, just in time for the Oscars, a new OpenAI app called Sora hopes to master cinema without going to film school. For now a research product, Sora is going out to a few select creators and a number of security experts who will red-team it for safety vulnerabilities. OpenAI plans to make it available to all wannabe auteurs at some unspecified date, but it decided to preview it in advance.\n\nOther companies, from giants like Google to startups like Runway, have already revealed text-to-video AI projects. But OpenAI says that Sora is distinguished by its striking photorealism\u2014something I haven\u2019t seen in its competitors\u2014and its ability to produce longer clips than the brief snippets other models typically do, up to one minute. The researchers I spoke to won\u2019t say how long it takes to render all that video, but when pressed, they described it as more in the \u201cgoing out for a burrito\u201d ballpark than \u201ctaking a few days off.\u201d If the hand-picked examples I saw are to be believed, the effort is worth it.\n\nOpenAI didn\u2019t let me enter my own prompts, but it shared four instances of Sora\u2019s power. (None approached the purported one-minute limit; the longest was 17 seconds.) The first came from a detailed prompt that sounded like an obsessive screenwriter\u2019s setup: \u201cBeautiful, snowy Tokyo city is bustling. The camera moves through the bustling city street, following several people enjoying the beautiful snowy weather and shopping at nearby stalls. Gorgeous sakura petals are flying through the wind along with snowflakes.\u201d\n\nAI-generated video made with OpenAI's Sora. Courtesy of OpenAI\n\nThe result is a convincing view of what is unmistakably Tokyo, in that magic moment when snowflakes and cherry blossoms coexist. The virtual camera, as if affixed to a drone, follows a couple as they slowly stroll through a streetscape. One of the passersby is wearing a mask. Cars rumble by on a riverside roadway to their left, and to the right shoppers flit in and out of a row of tiny shops.\n\nIt\u2019s not perfect. Only when you watch the clip a few times do you realize that the main characters\u2014a couple strolling down the snow-covered sidewalk\u2014would have faced a dilemma had the virtual camera kept running. The sidewalk they occupy seems to dead-end; they would have had to step over a small guardrail to a weird parallel walkway on their right. Despite this mild glitch, the Tokyo example is a mind-blowing exercise in world-building. Down the road, production designers will debate whether it\u2019s a powerful collaborator or a job killer. Also, the people in this video\u2014who are entirely generated by a digital neural network\u2014aren\u2019t shown in close-up, and they don\u2019t do any emoting. But the Sora team says that in other instances they\u2019ve had fake actors showing real emotions.\n\nThe other clips are also impressive, notably one asking for \u201can animated scene of a short fluffy monster kneeling beside a red candle,\u201d along with some detailed stage directions (\u201cwide eyes and open mouth\u201d) and a description of the desired vibe of the clip. Sora produces a Pixar-esque creature that seems to have DNA from a Furby, a Gremlin, and Sully in Monsters, Inc. I remember when that latter film came out, Pixar made a huge deal of how difficult it was to create the ultra-complex texture of a monster\u2019s fur as the creature moved around. It took all of Pixar\u2019s wizards months to get it right. OpenAI\u2019s new text-to-video machine \u2026 just did it.\n\n\u201cIt learns about 3D geometry and consistency,\u201d says Tim Brooks, a research scientist on the project, of that accomplishment. \u201cWe didn\u2019t bake that in\u2014it just entirely emerged from seeing a lot of data.\u201d\n\nAI-generated video made with the prompt, \u201canimated scene features a close-up of a short fluffy monster kneeling beside a melting red candle. the art style is 3d and realistic, with a focus on lighting and texture. the mood of the painting is one of wonder and curiosity, as the monster gazes at the flame with wide eyes and open mouth. its pose and expression convey a sense of innocence and playfulness, as if it is exploring the world around it for the first time. the use of warm colors and dramatic lighting further enhances the cozy atmosphere of the image.\u201d Courtesy of OpenAI\n\nWhile the scenes are certainly impressive, the most startling of Sora\u2019s capabilities are those that it has not been trained for. Powered by a version of the diffusion model used by OpenAI\u2019s Dalle-3 image generator as well as the transformer-based engine of GPT-4, Sora does not merely churn out videos that fulfill the demands of the prompts, but does so in a way that shows an emergent grasp of cinematic grammar.\n\nThat translates into a flair for storytelling. In another video that was created off of a prompt for \u201ca gorgeously rendered papercraft world of a coral reef, rife with colorful fish and sea creatures.\u201d Bill Peebles, another researcher on the project, notes that Sora created a narrative thrust by its camera angles and timing. \u201cThere's actually multiple shot changes\u2014these are not stitched together, but generated by the model in one go,\u201d he says. \u201cWe didn\u2019t tell it to do that, it just automatically did it.\u201d\n\nAI-generated video made with the prompt \u201ca gorgeously rendered papercraft world of a coral reef, rife with colorful fish and sea creatures.\u201d Courtesy of OpenAI\n\nIn another example I didn\u2019t view, Sora was prompted to give a tour of a zoo. \u201cIt started off with the name of the zoo on a big sign, gradually panned down, and then had a number of shot changes to show the different animals that live at the zoo,\u201d says Peebles, \u201cIt did it in a nice and cinematic way that it hadn't been explicitly instructed to do.\u201d\n\nOne feature in Sora that the OpenAI team didn\u2019t show, and may not release for quite a while, is the ability to generate videos from a single image or a sequence of frames. \u201cThis is going to be another really cool way to improve storytelling capabilities,\u201d says Brooks. \u201cYou can draw exactly what you have on your mind and then animate it to life.\u201d OpenAI is aware that this feature also has the potential to produce deepfakes and misinformation. \u201cWe\u2019re going to be very careful about all the safety implications for this,\u201d Peebles adds."
    },
    {
        "metadata": {
            "title": "Rogue superintelligence and merging with machines: Inside the mind of OpenAI\u2019s chief scientist - MIT Technology Review",
            "description": "Rogue superintelligence and merging with machines: Inside the mind of OpenAI\u2019s chief scientist  MIT Technology Review",
            "published date": "Thu, 26 Oct 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMikQFodHRwczovL3d3dy50ZWNobm9sb2d5cmV2aWV3LmNvbS8yMDIzLzEwLzI2LzEwODIzOTgvZXhjbHVzaXZlLWlseWEtc3V0c2tldmVyLW9wZW5haXMtY2hpZWYtc2NpZW50aXN0LW9uLWhpcy1ob3Blcy1hbmQtZmVhcnMtZm9yLXRoZS1mdXR1cmUtb2YtYWkv0gGVAWh0dHBzOi8vd3d3LnRlY2hub2xvZ3lyZXZpZXcuY29tLzIwMjMvMTAvMjYvMTA4MjM5OC9leGNsdXNpdmUtaWx5YS1zdXRza2V2ZXItb3BlbmFpcy1jaGllZi1zY2llbnRpc3Qtb24taGlzLWhvcGVzLWFuZC1mZWFycy1mb3ItdGhlLWZ1dHVyZS1vZi1haS9hbXAv?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.technologyreview.com",
                "title": "MIT Technology Review"
            }
        },
        "article": "Instead of building the next GPT or image maker DALL-E, Sutskever tells me his new priority is to figure out how to stop an artificial superintelligence (a hypothetical future technology he sees coming with the foresight of a true believer) from going rogue.\n\nSutskever tells me a lot of other things too. He thinks ChatGPT just might be conscious (if you squint). He thinks the world needs to wake up to the true power of the technology his company and others are racing to create. And he thinks some humans will one day choose to merge with machines.\n\nA lot of what Sutskever says is wild. But not nearly as wild as it would have sounded just one or two years ago. As he tells me himself, ChatGPT has already rewritten a lot of people\u2019s expectations about what\u2019s coming, turning \u201cwill never happen\u201d into \u201cwill happen faster than you think.\u201d\n\n\u201cIt\u2019s important to talk about where it\u2019s all headed,\u201d he says, before predicting the development of artificial general intelligence (by which he means machines as smart as humans) as if it were as sure a bet as another iPhone: \u201cAt some point we really will have AGI. Maybe OpenAI will build it. Maybe some other company will build it.\u201d\n\nSince the release of its sudden surprise hit, ChatGPT, last November, the buzz around OpenAI has been astonishing, even in an industry known for hype. No one can get enough of this nerdy $80 billion startup. World leaders seek (and get) private audiences. Its clunky product names pop up in casual conversation.\n\nOpenAI\u2019s CEO, Sam Altman, spent a good part of the summer on a weeks-long outreach tour, glad-handing politicians and speaking to packed auditoriums around the world. But Sutskever is much less of a public figure, and he doesn\u2019t give a lot of interviews.\n\nHe is deliberate and methodical when he talks. There are long pauses when he thinks about what he wants to say and how to say it, turning questions over like puzzles he needs to solve. He does not seem interested in talking about himself. \u201cI lead a very simple life,\u201d he says. \u201cI go to work; then I go home. I don\u2019t do much else. There are a lot of social activities one could engage in, lots of events one could go to. Which I don\u2019t.\u201d\n\nBut when we talk about AI, and the epochal risks and rewards he sees down the line, vistas open up: \u201cIt\u2019s going to be monumental, earth-shattering. There will be a before and an after.\u201d"
    },
    {
        "metadata": {
            "title": "Elon Musk sues OpenAI and CEO Sam Altman, claiming betrayal of its goal to benefit humanity - The Associated Press",
            "description": "Elon Musk sues OpenAI and CEO Sam Altman, claiming betrayal of its goal to benefit humanity  The Associated Press",
            "published date": "Fri, 01 Mar 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiX2h0dHBzOi8vYXBuZXdzLmNvbS9hcnRpY2xlL21pY3Jvc29mdC1zYW0tYWx0bWFuLW9wZW5haS1jaGF0Z3B0LTQyNTE4NmM3NjQwYWEzZDA5NTZlOTkzMTRhOTI0MGUy0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://apnews.com",
                "title": "The Associated Press"
            }
        },
        "article": "Elon Musk is suing OpenAI and its CEO Sam Altman over what he says is a betrayal of the ChatGPT maker\u2019s founding aims of benefiting humanity rather than pursuing profits.\n\nIn a lawsuit filed at San Francisco Superior Court, billionaire Musk said that when he bankrolled OpenAI\u2019s creation, he secured an agreement with Altman and Greg Brockman, the president, to keep the AI company as a nonprofit that would develop technology for the benefit of the public.\n\nUnder its founding agreement, OpenAI would also make its code open to the public instead of walling it off for any private company\u2019s gains, the lawsuit says.\n\nHowever, by embracing a close relationship with Microsoft, OpenAI and its top executives have set that pact \u201caflame\u201d and are \u201cperverting\u201d the company\u2019s mission, Musk alleges in the lawsuit.\n\nOpenAI declined to comment on the lawsuit Friday.\n\n\u201cOpenAI, Inc. has been transformed into a closed-source de facto subsidiary of the largest technology company in the world: Microsoft,\u201d the lawsuit filed Thursday says. \u201cUnder its new Board, it is not just developing but is actually refining an AGI to maximize profits for Microsoft, rather than for the benefit of humanity.\u201d\n\nAGI refers to artificial general intelligence, which are general purpose AI systems that can perform just as well as \u2014 or even better than \u2014 humans in a wide variety of tasks.\n\nMusk is suing over breach of contract, breach of fiduciary duty and unfair business practices. He also wants an injunction to prevent anyone, including Microsoft, from benefiting from OpenAI\u2019s technology.\n\nThose claims are unlikely to succeed in court but that might not be the point for Musk, who is getting his take and personal story on the record, said Anupam Chander, a law professor at Georgetown University.\n\n\u201cPartly there\u2019s an assertion of Elon\u2019s founding role in OpenAI and generative AI technology, in particular his claim he named OpenAI and he hired the key scientist and that he was the principal funder of its early years,\u201d Chander said. \u201cIn some sense it\u2019s a lawsuit that tries to establish his own place in the history of generative AI.\u201d\n\nMusk was an early investor in OpenAI when it was founded in 2015 and co-chaired its board alongside Altman. In the lawsuit, he said he invested \u201ctens of millions\u201d of dollars in the nonprofit research laboratory.\n\nMusk resigned from the board in early 2018 in a move that OpenAI said at the time would prevent conflicts of interest as the Tesla CEO was recruiting AI talent to build self-driving technology at the electric car maker. \u201cThis will eliminate a potential future conflict for Elon,\u201d OpenAI said in a February 2018 blog post. Musk has since said he also had disagreements with the startup\u2019s direction, but he continued to donate to the nonprofit.\n\nLater that year, OpenAI filed papers to incorporate a for-profit arm and began shifting most of its workforce to that business, but retained a nonprofit board of directors that governed the company. Microsoft made its first $1 billion investment in the company in 2019 and the next year, signed an agreement that gave the software giant exclusive rights to its AI models. That license is supposed to expire once OpenAI has achieved artificial general intelligence, the company has said.\n\nIts unveiling of ChatGPT in late 2022 bought worldwide fame to OpenAI and helped spark a race by tech companies to capitalize on the public\u2019s fascination with the technology.\n\nWhen the nonprofit board abruptly fired Altman as CEO late last year, for reasons that still haven\u2019t been fully disclosed, it was Microsoft that helped drive the push that brought Altman back as CEO and led most of the old board to resign. Musk\u2019s lawsuit alleged that those changes caused the checks and balances protecting the nonprofit mission to \u201ccollapse overnight.\u201d\n\nOne of Musk\u2019s claims is that the directors of the nonprofit have failed to uphold their obligations to follow its mission, but Dana Brakman Reiser, a professor at Brooklyn Law School, is skeptical that Musk had standing to bring that claim.\n\n\u201cIt would be very worrisome if every person who cared about or donated to a charity could suddenly sue their directors and officers to say, \u2018You\u2019re not doing what I think is the right thing to run this nonprofit,\u2019\u201d she said. In general, only other directors or an attorney general, for example, could bring that type of suit, she said.\n\nEven if Musk invested in the for-profit business, his complaint seems to be that the organization is making too much profit in contradiction to its mission, which includes making its technology publicly available.\n\n\u201cI care about nonprofits actually following the mission that they set out and not being captured for some kind for profit purpose. That is a real concern,\u201d Brakman Reiser said. \u201cWhether Elon Musk is the person to raise that claim, I\u2019m less sure.\u201d\n\nWhatever the legal merits of the claims, a brewing courtroom fight between Musk, who now has his own AI startup, and Altman could offer the public a peek into the internal debates and decision-making at OpenAI, though the company\u2019s lawyers will likely fight to keep some of those documents confidential.\n\n\u201cThe discovery will be epic,\u201d posted venture capitalist Chamath Palihapitiya on Musk\u2019s social media platform X on Friday. To which Musk replied in his only public commentary so far on the case: \u201cYes.\u201d\n\n\u2014\u2014-\n\nThe AP has signed a deal with OpenAI for it to access its news archive."
    },
    {
        "metadata": {
            "title": "OpenAI\u2019s hunger for data is coming back to bite it - MIT Technology Review",
            "description": "OpenAI\u2019s hunger for data is coming back to bite it  MIT Technology Review",
            "published date": "Wed, 19 Apr 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiZmh0dHBzOi8vd3d3LnRlY2hub2xvZ3lyZXZpZXcuY29tLzIwMjMvMDQvMTkvMTA3MTc4OS9vcGVuYWlzLWh1bmdlci1mb3ItZGF0YS1pcy1jb21pbmctYmFjay10by1iaXRlLWl0L9IBamh0dHBzOi8vd3d3LnRlY2hub2xvZ3lyZXZpZXcuY29tLzIwMjMvMDQvMTkvMTA3MTc4OS9vcGVuYWlzLWh1bmdlci1mb3ItZGF0YS1pcy1jb21pbmctYmFjay10by1iaXRlLWl0L2FtcC8?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.technologyreview.com",
                "title": "MIT Technology Review"
            }
        },
        "article": "In AI development, the dominant paradigm is that the more training data, the better. OpenAI\u2019s GPT-2 model had a data set consisting of 40 gigabytes of text. GPT-3, which ChatGPT is based on, was trained on 570 GB of data. OpenAI has not shared how big the data set for its latest model, GPT-4, is.\n\nBut that hunger for larger models is now coming back to bite the company. In the past few weeks, several Western data protection authorities have started investigations into how OpenAI collects and processes the data powering ChatGPT. They believe it has scraped people\u2019s personal data, such as names or email addresses, and used it without their consent.\n\nThe Italian authority has blocked the use of ChatGPT as a precautionary measure, and French, German, Irish, and Canadian data regulators are also investigating how the OpenAI system collects and uses data. The European Data Protection Board, the umbrella organization for data protection authorities, is also setting up an EU-wide task force to coordinate investigations and enforcement around ChatGPT.\n\nItaly has given OpenAI until April 30 to comply with the law. This would mean OpenAI would have to ask people for consent to have their data scraped, or prove that it has a \u201clegitimate interest\u201d in collecting it. OpenAI will also have to explain to people how ChatGPT uses their data and give them the power to correct any mistakes about them that the chatbot spits out, to have their data erased if they want, and to object to letting the computer program use it.\n\nIf OpenAI cannot convince the authorities its data use practices are legal, it could be banned in specific countries or even the entire European Union. It could also face hefty fines and might even be forced to delete models and the data used to train them, says Alexis Leautier, an AI expert at the French data protection agency CNIL.\n\nOpenAI\u2019s violations are so flagrant that it\u2019s likely that this case will end up in the Court of Justice of the European Union, the EU\u2019s highest court, says Lilian Edwards, an internet law professor at Newcastle University. It could take years before we see an answer to the questions posed by the Italian data regulator.\n\nHigh-stakes game\n\nThe stakes could not be higher for OpenAI. The EU\u2019s General Data Protection Regulation is the world\u2019s strictest data protection regime, and it has been copied widely around the world. Regulators everywhere from Brazil to California will be paying close attention to what happens next, and the outcome could fundamentally change the way AI companies go about collecting data.\n\nIn addition to being more transparent about its data practices, OpenAI will have to show it is using one of two possible legal ways to collect training data for its algorithms: consent or \u201clegitimate interest.\u201d"
    },
    {
        "metadata": {
            "title": "Elon Musk sues OpenAI over AI threat - Courthouse News Service",
            "description": "Elon Musk sues OpenAI over AI threat  Courthouse News Service",
            "published date": "Fri, 01 Mar 2024 03:57:11 GMT",
            "url": "https://news.google.com/rss/articles/CBMiRGh0dHBzOi8vd3d3LmNvdXJ0aG91c2VuZXdzLmNvbS9lbG9uLW11c2stc3Vlcy1vcGVuYWktb3Zlci1haS10aHJlYXQv0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.courthousenews.com",
                "title": "Courthouse News Service"
            }
        },
        "article": "OpenAI is not so open now, Musk claims, following the closed-source release of the company's artificial general intelligence technology under Microsoft.\n\n(CN) \u2014 Elon Musk says in a Thursday lawsuit that Sam Altman and OpenAI have betrayed an agreement from the artificial intelligence research company's founding to develop the technology for the benefit of humanity rather than profit.\n\nIn the suit filed Thursday night in San Francisco Superior Court, Musk claims OpenAI's recent relationship with tech giant Microsoft has compromised the company's original dedication to public, open-source artificial general intelligence.\n\n\"OpenAI, Inc. has been transformed into a closed-source de facto subsidiary of the largest technology company in the world: Microsoft. Under its new board, it is not just developing but is actually refining an AGI to maximize profits for Microsoft, rather than for the benefit of humanity,\" Musk says in the suit.\n\nMusk brings claims including breach of contract, breach of fiduciary duty and unfair business practices against OpenAI and asks for the company to revert back to open source. Musk also requests an injunction to prevent OpenAI, its president Gregory Brockman and its CEO Sam Altman \u2014 named as co-defendants in the case \u2014 as well as Microsoft, from profiting off of the company's artificial general intelligence technology.\n\nArtificial general intelligence, a type of AI developed to autonomously perform on the cognitive level of humans, has been OpenAI's main goal and is demonstrated, Musk says, in its GPT-4. The company released GPT-4 in March 2023, but according to Musk, it remains a closed model, in contrast to previous iterations \u2014 a move driven by commercial considerations rather than in the interest of humanity.\n\n\"The internal details of GPT-4 are known only to OpenAI and, on information and belief, to Microsoft. GPT-4 is hence the opposite of 'open AI,'\" Musk says in the suit. \"And it is closed for propriety commercial reasons: Microsoft stands to make a fortune selling GPT-4 to the public, which would not be possible if OpenAI\u2014as it is required to do\u2014makes the technology freely available to the public.\n\n\"Contrary to the founding agreement, defendants have chosen to use GPT-4 not for the benefit of humanity, but as proprietary technology to maximize profits for literally the largest company in the world,\" he adds.\n\nIn his suit, Musk also hones in on the 2023 firing and subsequent reinstatement of Altman as CEO. Altman's ouster, Musk says, prompted Microsoft to step in and force the resignation of the board members who attempted to remove him. The current board members, Musk claims, are no longer scientists and researchers who support and understand the technology.\n\n\"OpenAI, Inc.\u2019s once carefully crafted non-profit structure was replaced by a purely profit-driven CEO and a board with inferior technical expertise in AGI and AI public policy. The board now has an observer seat reserved solely for Microsoft,\" Musk claims.\n\nMusk, who was an original board member of OpenAI but departed in 2018, says the conflict between the board and Altman stemmed from the development of GPT-4 and the potential next iteration of the AGI technology, which Musk worries could compromise public safety.\n\nMicrosoft, which is not named as a defendant in the suit, gained exclusive licensing to OpenAI's GPT-3 language model in 2020. Microsoft continues to assert rights to GPT-4, which it claims has not reached the level of AGI, which would block its licensing privileges.\n\nMusk claims Microsoft's hold on Altman and the OpenAI board will keep them from declaring GPT-4 as a AGI in order to keep the technology private and profitable.\n\nNeither OpenAI nor Musk could immediately be reached for comment Thursday night. Musk is represented in the suit by Los Angeles law firm Irell & Manella."
    },
    {
        "metadata": {
            "title": "Who Controls OpenAI? - Bloomberg",
            "description": "Who Controls OpenAI?  Bloomberg",
            "published date": "Mon, 20 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiSWh0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vb3Bpbmlvbi9hcnRpY2xlcy8yMDIzLTExLTIwL3doby1jb250cm9scy1vcGVuYWnSAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bloomberg.com",
                "title": "Bloomberg"
            }
        },
        "article": "Connecting decision makers to a dynamic network of information, people and ideas, Bloomberg quickly and accurately delivers business and financial information, news and insight around the world"
    },
    {
        "metadata": {
            "title": "OpenAI drama continues: Sam Altman may be mulling a return to the company - CNN",
            "description": "OpenAI drama continues: Sam Altman may be mulling a return to the company  CNN",
            "published date": "Sat, 18 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiVmh0dHBzOi8vd3d3LmNubi5jb20vMjAyMy8xMS8xOC90ZWNoL29wZW5haS1zYW0tYWx0bWFuLXNoYWtldXAtd2hhdC1oYXBwZW5lZC9pbmRleC5odG1s0gFPaHR0cHM6Ly9hbXAuY25uLmNvbS9jbm4vMjAyMy8xMS8xOC90ZWNoL29wZW5haS1zYW0tYWx0bWFuLXNoYWtldXAtd2hhdC1oYXBwZW5lZA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnn.com",
                "title": "CNN"
            }
        },
        "article": "CNN \u2014\n\nIn a surprise twist after Friday\u2019s unexpected firing of OpenAI CEO Sam Altman, the artificial intelligence leader may be mulling a return.\n\nMultiple news reports, including the Wall Street Journal and New York Times, cited anonymous sources who said the board is having second thoughts about the firing and has asked Altman to return. Altman is considering the offer, those sources reportedly said.\n\nThat would mark a shocking reversal of one of the more bizarre chapters of Silicon Valley leadership changes \u2014 and a key decision that could affect control over the future of AI, one of the key technologies expected to pave the way for the decades to come.\n\nOpenAI did not respond to requests for comment.\n\nHow Altman was fired\n\nThe bombshell leadership change, which shook a giant of the artificial intelligence industry, took place extremely swiftly, said Greg Brockman, the company\u2019s co-founder and former president, in a post on X, formerly known as Twitter.\n\nAltman\u2019s firing unfolded on Friday as abruptly as it played out in public, according to one of the company\u2019s co-founders, who said he was also demoted and then quit in the aftermath.\n\nA key factor in Altman\u2019s ouster was the presence of tensions between Altman, who favored pushing AI development more aggressively, and members of the OpenAI board, who wanted to move more cautiously, according to CNN contributor Kara Swisher, who spoke to sources knowledgeable about the crisis.\n\nBrockman\u2019s post, which appeared to be a joint statement speaking for himself and Altman, said the two were \u201cstill trying to figure out exactly what happened,\u201d but summarized the sequence of events that led to Altman\u2019s firing.\n\nOn Thursday evening, Altman received a text message from Ilya Sutskever, another co-founder of OpenAI and its chief scientist, Brockman said. The text message asked Altman to attend a meeting the following day.\n\n\u201cSam joined a Google Meet and the whole board, except Greg, was there,\u201d Brockman said, referring to himself. \u201cIlya told Sam he was being fired and that the news was going out very soon.\u201d\n\n\u201cAt 12:19pm, Greg got a text from Ilya asking for a quick call,\u201d Brockman continued. \u201cAt 12:23pm, Ilya sent a Google Meet link. Greg was told that he was being removed from the board (but was vital to the company and would retain his role) and that Sam had been fired. Around the same time, OpenAI published a blog post.\u201d\n\nAccording to Swisher, Altman did not learn about the subject of the meeting until 30 minutes before.\n\nAfter receiving word of his own ouster as board chair, Brockman subsequently announced he was quitting the company.\n\nDriving the board\u2019s decision were Sutskever\u2019s concerns, which appear to have been exacerbated by OpenAI\u2019s recent developer conference and the announcement of a way for anyone to create their own versions of ChatGPT, said Swisher, citing her sources. Swisher added that it represented \u201can inflection moment of Altman pushing too far, too fast\u201d for Sutskever, who \u201cgot the board on his side.\u201d\n\nIn its announcement of Altman\u2019s firing, OpenAI claimed that Altman had been insufficiently \u201ccandid\u201d with the board and that it had hindered the board\u2019s ability to carry out its responsibilities.\n\nAn unfinished drama\n\nThe suddenness of the decision was reflected in how some of OpenAI\u2019s most important partners were left in the dark.\n\nMicrosoft, which has invested billions into OpenAI and integrated its technology into the Bing search engine, was not informed of Altman\u2019s firing until \u201cjust before\u201d the public announcement, Swisher said, while employees were not given any advance warning.\n\nOn Friday evening, Altman posted on X that he \u201cloved working with such talented people\u201d at OpenAI and that he \u201cwill have more to say about what\u2019s next later.\u201d\n\nHe added that \u201cif I start going off, the openai board should go after me for the full value of my shares.\u201d\n\nIn his post, Brockman hinted that he and Altman may already be forging ahead. \u201cPlease don\u2019t spend any time being concerned. We will be fine,\u201d Brockman said. \u201cGreater things coming soon.\u201d\n\nCNN has reached out to OpenAI for comment on Brockman and Swisher\u2019s accounts of how the events transpired.\n\nAn interim CEO\n\nIn announcing Altman\u2019s firing, OpenAI said chief technology officer Mira Murati will serve as interim CEO.\n\nIn a statement on its website, OpenAI said Murati is \u201cexceptionally qualified\u201d and that the company has \u201cthe utmost confidence in her ability to lead OpenAI during this transition period.\u201d\n\nMira Murati, Chief Technology Officer of OpenAI, speaks during The Wall Street Journal's WSJ Tech Live Conference in Laguna Beach, California on October 17, 2023. (Photo by Patrick T. Fallon / AFP) (Photo by PATRICK T. FALLON/AFP via Getty Images) Patrick T. Fallon/AFP/Getty Images\n\nMurati, 34, has been part of OpenAI\u2019s leadership team for five years, according to the company. The statement said she will step in as the board \u201cconducts a formal search for a permanent CEO.\u201d\n\nThe move immediately catapults Murati \u2013 already a significant figure in AI \u2013 as one of the most high-profile and recognizable women in tech.\n\nAnd it puts her atop the company as questions swirl about what Altman\u2019s ouster means, the direction of the board and even the purpose of the company and artificial intelligence itself.\n\nBut in some ways, this is familiar ground for Murati, a Dartmouth-educated engineer. In July, when OpenAI\u2019s head of trust and safety announced plans to step down, Murati took up the baton as interim manager of that team.\n\nMurati has previously spoken before about her high hopes for AI. In 2022, for example, she told CNN that AI \u201cis really an extension of the human mind, and I hope we figure out how to deploy it in ways that are robustly beneficial and effective.\u201d\n\nNo matter who is in charge, OpenAI faced a litany of challenges even before the upper-management shakeup. There are a growing number of competitors and startups in the AI space and increased regulations from governments may hinder the industry\u2019s growth."
    },
    {
        "metadata": {
            "title": "Elon Musk Sues OpenAI and Sam Altman for 'Flagrant Breaches' of Contract - WIRED",
            "description": "Elon Musk Sues OpenAI and Sam Altman for 'Flagrant Breaches' of Contract  WIRED",
            "published date": "Fri, 01 Mar 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiPWh0dHBzOi8vd3d3LndpcmVkLmNvbS9zdG9yeS9lbG9uLW11c2stc3Vlcy1zYW0tYWx0bWFuLW9wZW5haS_SAQA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.wired.com",
                "title": "WIRED"
            }
        },
        "article": "Elon Musk is suing OpenAI and Sam Altman for allegedly abandoning OpenAI\u2019s original mission to develop artificial intelligence to benefit humanity.\n\n\u201cOpenAI, Inc. has been transformed into a closed-source de facto subsidiary of the largest technology company in the world: Microsoft,\u201d Musk\u2019s lawyers wrote in the lawsuit, which was filed late on Thursday in San Francisco.\n\n\u201cUnder its new board, it is not just developing but is refining an AGI [Artificial General Intelligence] to maximize profits for Microsoft, rather than for the benefit of humanity,\u201d claims the filing. \u201cOn information and belief, GPT-4 is an AGI algorithm.\u201d\n\nMusk is one of the cofounders of OpenAI. The lawsuit claims he played a central role in establishing the company by contributing over $44 million between 2016 and 2020, paying for office space and convincing key team members, including chief scientist Ilya Sutskever, to join the business. Musk did all this, the lawsuit claims, because he wanted to support a nonprofit that would develop AI to benefit humanity. Instead his contributions were \u201ctwisted,\u201d the filing claims, for the benefit of both OpenAI and Microsoft.\n\n\u201cImagine donating to a nonprofit whose asserted mission is to protect the Amazon rainforest, but then the nonprofit creates a for-profit Amazonian logging company that uses the fruits of the donations to clear the rainforest,\u201d reads the lawsuit. \u201cThat is the story of OpenAI, Inc.\u201d\n\nOpenAI has a unique corporate structure. It is a nonprofit charged with safeguarding humanity against artificial general intelligence, or AGI, a hypothetical AI system that can surpass humans at most tasks. But in late 2019, after Musk left the company\u2019s board, it also established a for-profit arm with a less altruistic focus. (The profits of OpenAI LP are technically capped; investors can get back 100 times their investment, while any amount beyond that limit goes back to the nonprofit.) The explosive popularity of ChatGPT and demand for the underlying GPT-4 AI model has made that side of the company worth a reported $80 billion\u2014and drawn the ire of Musk.\n\nThe lawsuit describes how OpenAI\u2019s structure has become \u201cincreasingly complex\u201d in recent years. It also takes aim at OpenAI\u2019s relationship with Microsoft, which has invested around $13 billion into the AI company\u2019s for-profit business in an alliance that has attracted scrutiny from regulators in the US, the EU, and the UK. The UK regulator, the CMA, said in December that it was investigating to see if the partnership could potentially impact competition in the AI space. Neither OpenAI nor Microsoft immediately replied to WIRED\u2019s request for comment.\n\nThe lawsuit alleges that the internal design of GPT-4, the company\u2019s latest model, remains secret because Microsoft and OpenAI stand to make a fortune by selling access to the AI model to the public. \"GPT-4 is hence the opposite of 'Open AI',\" the filing reads.\n\nAI systems exist across a spectrum of openness, ranging from fully open source to fully closed, depending on how much their inner workings are shared with researchers and members of the public. Those in favor of open source argue the approach allows greater transparency and potential for innovation. Arguments against include warnings that it makes powerful AI models potentially available to criminals or geopolitical adversaries. Meta\u2019s Llama 2 model is free to download, modify, and deploy\u2014though it does have some limitations on use\u2014while GPT-4 is not."
    },
    {
        "metadata": {
            "title": "OpenAI still not training GPT-5, Sam Altman says - TechCrunch",
            "description": "OpenAI still not training GPT-5, Sam Altman says  TechCrunch",
            "published date": "Wed, 07 Jun 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiOWh0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wNi8wNy9vcGVuYWktZ3B0NS1zYW0tYWx0bWFuL9IBPWh0dHBzOi8vdGVjaGNydW5jaC5jb20vMjAyMy8wNi8wNy9vcGVuYWktZ3B0NS1zYW0tYWx0bWFuL2FtcC8?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://techcrunch.com",
                "title": "TechCrunch"
            }
        },
        "article": "OpenAI is still not training GPT-5, months after the Microsoft-backed startup pledged to not work on the successor to GPT-4 \u201cfor some time\u201d after many industry executives and academics expressed concerns about the fast-rate of advancements by Sam Altman\u2019s large language models.\n\n\u201cWe have a lot of work to do before we start that model,\u201d Altman, the chief executive of OpenAI, said at a conference hosted by Indian newspaper Economic Times. \u201cWe\u2019re working on the new ideas that we think we need for it, but we are certainly not close to it to start.\u201d\n\nIn late March, more than 1,100 signatories, including Elon Musk and Steve Wozniak, signed an open letter that calls on \u201call AI labs to immediately pause for at least 6 months the training of AI systems more powerful than GPT-4.\u201d\n\nWeeks later, Altman said that the letter was \u201cmissing most technical nuance about where we need the pause,\u201d but asserted that OpenAI had not started training GPT-5 \u2014 and didn\u2019t plan to do so for \u201csome time.\u201d\n\nAltman on Wednesday pushed back again on the concerns from some of the most vocal voices on AI, saying the startup was already evaluating potential dangers with more meaningful measures such as external audits and red-teaming and safety tests.\n\n\u201cWhen we finished GPT-4, it took us more than six months until we were ready to release it.\u201d\n\nEarlier in the interview, Altman also said that OpenAI was against regulating smaller AI startups. \u201cThe only regulation we have called for is on ourselves and people bigger,\u201d Altman said.\n\nAltman\u2019s trip to India is part of his attempt to aggressively meet with lawmakers and industry players globally and build confidence in OpenAI\u2019s willingness to work with regulators. In his meetings, Altman is proactively urging lawmakers to put serious thinking into the potential abuse and other downside of AI proliferation so that guardrails could be put in place to minimize any unintended accidents."
    },
    {
        "metadata": {
            "title": "OpenAI in 'Intense Discussions' to Quell Potential Staff Mutiny - Bloomberg",
            "description": "OpenAI in 'Intense Discussions' to Quell Potential Staff Mutiny  Bloomberg",
            "published date": "Tue, 21 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMia2h0dHBzOi8vd3d3LmJsb29tYmVyZy5jb20vbmV3cy9hcnRpY2xlcy8yMDIzLTExLTIxL29wZW5haS1pbi1pbnRlbnNlLWRpc2N1c3Npb25zLXRvLXVuaWZ5LWNvbXBhbnktbWVtby1zYXlz0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.bloomberg.com",
                "title": "Bloomberg"
            }
        },
        "article": "OpenAI said it\u2019s in \u201cintense discussions\u201d to unify the company after another tumultuous day that saw most employees threaten to quit if Sam Altman doesn\u2019t return as chief executive officer.\n\nVice President of Global Affairs Anna Makanju delivered the message in an internal memo reviewed by Bloomberg News, aiming to rally staff who\u2019ve grown anxious after days of disarray following Altman\u2019s ouster and the board\u2019s surprise appointment of former Twitch chief Emmett Shear as his interim replacement."
    },
    {
        "metadata": {
            "title": "OpenAI is pursuing a new way to fight A.I. 'hallucinations' - CNBC",
            "description": "OpenAI is pursuing a new way to fight A.I. 'hallucinations'  CNBC",
            "published date": "Wed, 31 May 2023 07:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiXGh0dHBzOi8vd3d3LmNuYmMuY29tLzIwMjMvMDUvMzEvb3BlbmFpLWlzLXB1cnN1aW5nLWEtbmV3LXdheS10by1maWdodC1haS1oYWxsdWNpbmF0aW9ucy5odG1s0gFgaHR0cHM6Ly93d3cuY25iYy5jb20vYW1wLzIwMjMvMDUvMzEvb3BlbmFpLWlzLXB1cnN1aW5nLWEtbmV3LXdheS10by1maWdodC1haS1oYWxsdWNpbmF0aW9ucy5odG1s?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.cnbc.com",
                "title": "CNBC"
            }
        },
        "article": "OpenAI is taking up the mantle against AI \"hallucinations,\" the company announced Wednesday, with a newer method for training artificial intelligence models.\n\nThe research comes at a time when misinformation stemming from AI systems is more hotly debated than ever, amid the generative AI boom and lead-up to the 2024 U.S. presidential election.\n\nOpenAI accelerated the generative AI boom last year when it released ChatGPT, its chatbot powered by GPT-3 and GPT-4, and surpassed 100 million monthly users in two months, reportedly setting a record for fastest-growing app. To date, Microsoft has invested more than $13 billion in OpenAI, and the startup's value has reached roughly $29 billion.\n\nAI hallucinations occur when models like OpenAI's ChatGPT or Google 's Bard fabricate information entirely, behaving as if they are spouting facts. One example: In Google's own February promotional video for Bard, the chatbot makes an untrue claim about the James Webb Space Telescope. More recently, ChatGPT cited \"bogus\" cases in a New York federal court filing, and the New York attorneys involved may face sanctions.\n\n\"Even state-of-the-art models are prone to producing falsehoods \u2014they exhibit a tendency to invent facts in moments of uncertainty,\" the OpenAI researchers wrote in the report. \"These hallucinations are particularly problematic in domains that require multi-step reasoning, since a single logical error is enough to derail a much larger solution.\"\n\nOpenAI's potential new strategy for fighting the fabrications: Train AI models to reward themselves for each individual, correct step of reasoning when they're arriving at an answer, instead of just rewarding a correct final conclusion. The approach is called \"process supervision,\" as opposed to \"outcome supervision,\" and could lead to better explainable AI, according to the researchers, since the strategy encourages models to follow more of a human-like chain of \"thought\" approach.\n\n\"Detecting and mitigating a model's logical mistakes, or hallucinations, is a critical step towards building aligned AGI [or artificial general intelligence],\" Karl Cobbe, mathgen researcher at OpenAI, told CNBC, noting that while OpenAI did not invent the process-supervision approach, the company is helping to push it forward. \"The motivation behind this research is to address hallucinations in order to make models more capable at solving challenging reasoning problems.\"\n\nOpenAI has released an accompanying dataset of 800,000 human labels it used to train the model mentioned in the research paper, Cobbe said.\n\nBen Winters, senior counsel at the Electronic Privacy Information Center and leader of its AI and human rights project, expressed skepticism, telling CNBC he would like to examine the full dataset and accompanying examples.\n\n\"I just don't think that this alone does any significant mitigation of concerns about misinformation and incorrect results \u2026 when it's actually being used in the wild,\" Winters said. He added, \"It definitely matters whether they plan on implementing whatever they have found through their research here [into their products], and if they're not, that does bring some fairly serious questions about what they are willing to release into the public.\"\n\nSince it's unclear that the OpenAI paper has been peer-reviewed or reviewed in another format, Suresh Venkatasubramanian, director of the center for technology responsibility at Brown University, told CNBC that he views the research as more of a preliminary observation than anything else.\n\n\"This will need to shake out in the research community before we can say anything certain about this,\" Venkatasubramanian said. \"In this world, there are a lot of results that come out very regularly, and because of the overall instability in how large language models work, what might work in one setting, model and context may not work in another setting, model and context.\"\n\nVenkatasubramanian added, \"Some of the hallucinatory stuff that people have been concerned about is [models] making up citations and references. There is no evidence in this paper that this would work for that. \u2026 It's not that I'm saying it won't work; I'm saying that this paper does not provide that evidence.\"\n\nCobbe said the company \"will likely submit [the paper] to a future conference for peer review.\" OpenAI did not respond to a request for comment on when, if ever, the company plans on implementing the new strategy into ChatGPT and its other products.\n\n\"It's certainly welcome to see companies trying to tinker with the development of their systems to try and reduce these kinds of errors \u2014 I think what's key is to interpret this as corporate research, in light of the many barriers that exist to deeper forms of accountability,\" Sarah Myers West, managing director of the AI Now Institute, told CNBC.\n\nWest added, \"[OpenAI is] releasing a small dataset of human-level feedback with this paper, but it hasn't provided basic details about the data used to train and test GPT-4. So, there's still a tremendous amount of opacity that is challenging any meaningful accountability efforts in the field of AI, even as these systems are directly affecting people already.\""
    },
    {
        "metadata": {
            "title": "OpenAI Seeks to Dismiss Parts of The New York Times's Lawsuit - The New York Times",
            "description": "OpenAI Seeks to Dismiss Parts of The New York Times's Lawsuit  The New York Times",
            "published date": "Tue, 27 Feb 2024 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiUGh0dHBzOi8vd3d3Lm55dGltZXMuY29tLzIwMjQvMDIvMjcvdGVjaG5vbG9neS9vcGVuYWktbmV3LXlvcmstdGltZXMtbGF3c3VpdC5odG1s0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.nytimes.com",
                "title": "The New York Times"
            }
        },
        "article": "OpenAI filed a motion in federal court on Monday that seeks to dismiss some key elements of a lawsuit brought by The New York Times Company.\n\nThe Times sued OpenAI and its partner Microsoft on Dec. 27, accusing them of infringing on its copyrights by using millions of its articles to train A.I. technologies like the online chatbot ChatGPT. Chatbots now compete with the news outlet as a source of reliable information, the lawsuit said.\n\nIn the motion, filed in U.S. District Court for the Southern District of New York, the defendants argue that ChatGPT \u201cis not in any way a substitute for a subscription to The New York Times.\u201d\n\n\u201cIn the real world, people do not use ChatGPT or any other OpenAI product for that purpose,\u201d the filing said. \u201cNor could they. In the ordinary course, one cannot use ChatGPT to serve up Times articles at will.\u201d"
    },
    {
        "metadata": {
            "title": "Come build with us: Microsoft and OpenAI partnership unveils new AI opportunities | The Microsoft Cloud Blog - Microsoft",
            "description": "Come build with us: Microsoft and OpenAI partnership unveils new AI opportunities | The Microsoft Cloud Blog  Microsoft",
            "published date": "Tue, 07 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMikQFodHRwczovL3d3dy5taWNyb3NvZnQuY29tL2VuLXVzL21pY3Jvc29mdC1jbG91ZC9ibG9nLzIwMjMvMTEvMDcvY29tZS1idWlsZC13aXRoLXVzLW1pY3Jvc29mdC1hbmQtb3BlbmFpLXBhcnRuZXJzaGlwLXVudmVpbHMtbmV3LWFpLW9wcG9ydHVuaXRpZXMv0gEA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.microsoft.com",
                "title": "Microsoft"
            }
        },
        "article": "At OpenAI\u2019s first DevDay Conference on November 6, 2023, Microsoft Chairman and CEO Satya Nadella made a surprise appearance during OpenAI CEO Sam Altman\u2019s keynote to deliver a powerful message: \u201cOur job number one is to build the best systems, so you can build the best models and deliver those to developers.\u201d This was a testament to the deep partnership between Microsoft and OpenAI. We\u2019re excited about the latest announcements from OpenAI\u2019s first DevDay event and want to highlight the opportunities it presents for all AI builders.\n\nNew models: GPT-4 Turbo on Azure OpenAI Service\n\nWe are very enthusiastic about all the new models introduced, including GPT-3.5 Turbo, and updates to models including DALL-E 3, and Whisper 3. Among them, the eagerly awaited GPT-4 Turbo offers lower pricing, extended prompt length, and structured JSON formatting with improved efficiency and control. We\u2019re looking forward to making these great Turbo models available on Azure OpenAI Service by the end of this year in keeping with our standard practice of bringing new model innovation from our partners at OpenAI to the Azure OpenAI Service.\n\nIncreasing access for all AI Builders\n\nOpenAI\u2019s announcement of lower pricing is significant. It will make the models more accessible and increase their utilization, allowing a broader range of applications to harness their power and ushering in a new era of generative AI. On Azure OpenAI Service, token pricing for the new models will be at parity with OpenAI\u2019s prices.\n\nAnd in an exciting development, Microsoft made GitHub Enterprise available to all DevDay conference in-person attendees to use for free for 90 days. GitHub Enterprise is a powerful tool for developers, assisting in code completion and development. Its integration with Microsoft\u2019s ecosystem aligns with the mission of helping developers easily bring ideas to life on Azure.\n\nGPTs: New ways to create and monetize\n\nGPTs are a new way for anyone to create a tailored version of ChatGPT to be more helpful in their daily life, at specific tasks, at work, or at home\u2014and then share that creation with others. No coding is required. You can make them for yourself, just for your company\u2019s internal use, or for everyone. Just like with plug-ins, we are looking forward to building deep ecosystem support for GPTs, which we\u2019ll share more on next week at our Microsoft Ignite conference.\n\nMicrosoft and OpenAI partnership\n\nOpenAI\u2019s introduction of a Custom Models program will be of particular interest to enterprises, and Microsoft will continue to offer the convenience of integrating OpenAI\u2019s services seamlessly within Microsoft\u2019s existing ecosystem and support infrastructure, providing a comprehensive solution for all enterprise needs.\n\nSam Altman, OpenAI\u2019s CEO, echoed the sentiment of a strong and productive partnership with Microsoft. \u201cI think we have the best partnership in tech,\u201d Altman told Nadella onstage.\n\nNadella went on to talk about the companies\u2019 alignment. \u201cOur mission is to empower every person and every organization on the planet to achieve more. And to me, ultimately, AI is only going to be useful if it truly does empower\u2026it\u2019s about being able to get the benefits of AI broadly disseminated to everyone,\u201d Nadella said.\n\nWith these announcements, developers and enterprises are now poised to explore new horizons, empowered by the combined strengths of Microsoft and OpenAI, and the limitless possibilities of generative AI.\n\nGet started with Azure OpenAI Service today"
    },
    {
        "metadata": {
            "title": "Sam Altman returns as CEO OpenAI - The Verge",
            "description": "Sam Altman returns as CEO OpenAI  The Verge",
            "published date": "Tue, 21 Nov 2023 08:00:00 GMT",
            "url": "https://news.google.com/rss/articles/CBMiS2h0dHBzOi8vd3d3LnRoZXZlcmdlLmNvbS8yMDIzLzExLzIyLzIzOTY3MjIzL3NhbS1hbHRtYW4tcmV0dXJucy1jZW8tb3Blbi1hadIBAA?oc=5&hl=en-US&gl=US&ceid=US:en",
            "publisher": {
                "href": "https://www.theverge.com",
                "title": "The Verge"
            }
        },
        "article": "Sam Altman will return as CEO of OpenAI, overcoming an attempted boardroom coup that sent the company into chaos over the past several days. Former president Greg Brockman, who quit in protest of Altman\u2019s firing, will return as well.\n\nThe company said in a statement late Tuesday that it has an \u201cagreement in principle\u201d for Altman to return alongside a new board composed of Bret Taylor, Larry Summers, and Adam D\u2019Angelo. D\u2019Angelo is a holdover from the previous board that initially fired Altman on Friday. He remains on this new board to give the previous board some representation, we\u2019re told.\n\nPeople familiar with the negotiations say that the main job of this small initial board is to vet and appoint an expanded board of up to nine people that will reset the governance of OpenAI. Microsoft, which has committed to investing billions in the company, wants to have a seat on that expanded board, as does Altman himself. During a press tour this week, CEO Satya Nadella said the company didn\u2019t want any more \u201csurprises.\u201d\n\nThe very human power struggle at the center of all this seems not yet completely over\n\nWe\u2019re told that both sides have agreed to an investigation into this whole saga. That investigation will presumably be done by an outside, independent law firm. Based on our conversations with people involved, the very human power struggle at the center of all this seems not yet completely over.\n\nAll key parties have posted about the deal for Altman to return, which we\u2019re told is a done deal minus some last-minute paperwork. On X (formerly Twitter), Altman said that \u201ceverything I\u2019ve done over the past few days has been in service of keeping this team and its mission together.\u201d\n\nOne of OpenAI\u2019s other big investors, Thrive Capital, called the return of Altman \u201cthe best outcome for the company, its employees, those who build on their technologies, and the world at large.\u201d\n\n\u201cOpenAI has the potential to be one of the most consequential companies in the history of computing,\u201d Thrive partner Kelly Sims said in a statement shared with The Verge. \u201cSam and Greg possess a profound commitment to the company\u2019s integrity, and an unmatched ability to inspire and lead. We couldn\u2019t be more excited for them to come back to the company they founded and helped build into what it is today.\u201d\n\nHelen Toner, who was a key board member in the initial move to oust Altman, simply said, \u201cAnd now, we all get some sleep.\u201d\n\nDo you know more about what\u2019s going on inside Google? I\u2019d love to chat. You can reach me securely on Signal, via email at alex.heath@theverge.com, or through the contact form on my website.\n\nAltman\u2019s return is even more shocking than his sudden exit on Friday. OpenAI\u2019s nonprofit board seemed resolute in its initial decision to remove Altman, shuffling through two CEOs in three days to avoid reinstating him. Meanwhile, the employees of OpenAI revolted, threatening to defect to Microsoft with Altman and co-founder Brockman if the board didn\u2019t resign.\n\nSince Altman was fired on Friday, the board members who opposed him have withheld the detailed reasoning for why they fired him, even under the threat of lawsuits from investors and employee walkouts. On Sunday, a key member of the board, Ilya Sutskever, flipped back to Altman\u2019s camp after being pleaded with by Brockman\u2019s wife. (Sutskever officiated their wedding at OpenAI headquarters \u2014 that\u2019s how deep this all goes.) The flipping of Sutskever, who is also OpenAI\u2019s chief scientist, left the remaining three board members more vulnerable."
    }
]