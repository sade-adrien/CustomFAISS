{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.documents.base import Document\n",
    "from langchain.vectorstores import FAISS\n",
    "from gnews import GNews\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "\n",
    "device = 'cuda:0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=device, cache_dir='/mnt/esperanto/et/huggingface/hub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_question = \"Why was Sam Altman fired from OpenAI in November 2023?\"\n",
    "\n",
    "inputs = tokenizer(f\"[INST]{user_question}[/INST]\", return_tensors='pt').to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n",
    "\n",
    "answer = tokenizer.decode(outputs[0,inputs['input_ids'].shape[1]:])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gathering news data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_news = GNews(language='en', country='US', period='365d', start_date=None, end_date=None, max_results=100)\n",
    "news_list = google_news.get_news('openai')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_news_list = []\n",
    "for news in tqdm(news_list):\n",
    "    try:\n",
    "        news_article = google_news.get_full_article(news['url']).text\n",
    "        full_news_list.append({\n",
    "                            'metadata': news,\n",
    "                            'article': news_article,\n",
    "        })\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"database.json\", \"w\") as write_file:\n",
    "    json.dump(full_news_list, write_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare database for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./database.json') as file:\n",
    "    data = json.load(file)\n",
    "    \n",
    "documents = []\n",
    "for item in data:\n",
    "    article = item['article']\n",
    "    metadata = item.get('metadata', {})\n",
    "    document = Document(page_content=article, metadata=metadata)\n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "documents = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "documents[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "                            model_name=embedding_model,\n",
    "                            model_kwargs={'device': device},\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.embed_query(\"Why was Sam Altman fired from OpenAI in November 2023?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_db = FAISS.from_documents(documents, embeddings)\n",
    "retriever = vector_db.as_retriever(search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = retriever.get_relevant_documents('Sam Altman')\n",
    "relevant_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bot_prompt = \"You are a helpful chatbot assistant. Your role is to answer questions of a curious user. To help you with this, you are provided below some context that may or may not be relevant. You should decide wether to base your answer on the provided context to be as helpful and accurate as possible. DO NOT MENTION that you base your answer on the context if you do, act as if it was your own knowledge. In any case, you should always try to use your knoweldge to provide a helpful and consistent answer regardless of the quality of the context.\"\n",
    "context_prompt = '\\n'.join([retriever.get_relevant_documents(user_question)[i].page_content for i in range(3)])\n",
    "full_prompt = f\"\"\"[INST]{bot_prompt}\\n[CONTEXT]{context_prompt}[/CONTEXT]\\nUSER: {user_question}[/INST]\"\"\"\n",
    "print(full_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(full_prompt, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, do_sample=False, max_new_tokens=500)\n",
    "\n",
    "answer = tokenizer.decode(outputs[0,inputs['input_ids'].shape[1]:])\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "retriever.get_relevant_documents(user_question)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_poc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
